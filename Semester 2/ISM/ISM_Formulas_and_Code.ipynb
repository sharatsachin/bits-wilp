{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Statistical Methods Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "- field that focuses on understanding and analyzing data, providing us with a toolkit of methods and techniques to collect, organize, summarize, and interpret data in a meaningful way\n",
    "- goal is to uncover patterns, relationships, and insights that can help us make informed decisions and draw reliable conclusions\n",
    "- descriptive statistics: methods that help us summarize and describe data in a concise and informative way\n",
    "- inferential statistics: methods that help us draw conclusions about a population based on a sample of data from that population\n",
    "\n",
    "## Types of variables\n",
    "1. Qualitative (categorical) variables\n",
    "    - variables that can be placed into distinct categories, according to some characteristic or attribute\n",
    "    - nominal variables: variables that have no natural ordering\n",
    "    - ordinal variables: variables that have a natural ordering\n",
    "2. Quantitative (numerical) variables\n",
    "    - variables that are measured on a numeric scale\n",
    "    - discrete variables: variables that can only take on a finite number of values\n",
    "    - continuous variables: variables that can take on an infinite number of values\n",
    "        1. interval variables: variables that have no natural zero point, ratios of values are not meaningful\n",
    "        2. ratio variables: variables that have a natural zero point, ratios of values are meaningful\n",
    "\n",
    "## Measures of central tendency\n",
    "They are measures that describe the center of a distribution of data\n",
    "- mean: the average of a set of values\n",
    "$${\\displaystyle {\\mu}={\\bar {x}}={\\frac {1}{n}}\\left(\\sum_{i=1}^{n}{x_{i}}\\right)={\\frac {x_{1}+x_{2}+\\cdots +x_{n}}{n}}}$$\n",
    "- median: the middle value of a set of values\n",
    "$$\\mathrm{median}(x) = \\begin{cases} \n",
    "    x_{\\frac{n+1}{2}} & \\text{if } n \\text{ is odd} \\\\\n",
    "    \\frac{x_{\\frac{n}{2}} + x_{\\frac{n + 1}{2}}}{2} & \\text{if } n \\text{ is even}\n",
    "\\end{cases}$$\n",
    "- mode: the most frequently occurring value in a set of values\n",
    "\n",
    "The mean, the median, and the mode each answer the question “Where is the center of the data set?” The nature of the data set, as indicated by a relative frequency histogram, determines which one gives the best answer.\n",
    "\n",
    "| Measure | When to Use |\n",
    "|---------|-------------|\n",
    "| Mean    | When data is normally distributed and there are no extreme outliers. |\n",
    "| Median  | When data has outliers or is skewed, providing a robust central value. |\n",
    "| Mode    | When identifying the most frequent or common value in categorical data. |\n",
    "\n",
    "Other results:\n",
    "$$mean − mode ≈ 3(mean − median)$$\n",
    "$$midrange = (min + max) / 2$$\n",
    "$$range = max− min$$\n",
    "\n",
    "### Skewness\n",
    "\n",
    "| Distribution | Characteristics | Mean vs Median vs Mode |\n",
    "|--|--|--|\n",
    "| Positive | Data skewed towards higher values<br>Majority of observations on the left side<br>Long tail on the right side | Mean > Median > Mode    |\n",
    "| Negative | Data skewed towards lower values<br>Majority of observations on the right side<br>Long tail on the left side | Mean < Median < Mode     |\n",
    "| Symmetric | Balanced distribution<br>Observations evenly spread on both sides | Mean ≈ Median ≈ Mode    |\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Relationship_between_mean_and_median_under_different_skewness.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "Bi-modal distributions have two peaks, while multi-modal distributions have more than two peaks.\n",
    "\n",
    "## Measures of variability\n",
    "They are measures that describe the spread of a distribution of data, to describe the distribution of data in a more complete way, and measure how well an individual value represents the entire distribution\n",
    "- range: the difference between the maximum and minimum values\n",
    "- variance: the average squared deviation from the mean\n",
    "$$\\sigma^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}$$\n",
    "- standard deviation: the square root of the variance\n",
    "$$\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n}}$$\n",
    "- coefficient of variation: the ratio of the standard deviation to the mean\n",
    "$$CV = \\frac{\\sigma}{\\bar{x}}$$\n",
    "\n",
    "The range, the standard deviation, and the variance each give a quantitative answer to the question “How variable are the data?”\n",
    "\n",
    "| Measure | When to use |\n",
    "|--|--|\n",
    "| Range | you need a quick measure of the spread or dispersion of data and want to know the difference between the highest and lowest values in the dataset.       |\n",
    "| Variance | you want to quantify the average squared deviation of data points from the mean, providing a measure of how much the data points vary from the mean value. |\n",
    "| Standard Deviation | you want a measure of the dispersion of data that is easy to interpret and represents the typical distance between each data point and the mean. |\n",
    "| Coefficient of Variation | you want to compare the relative variability between datasets with different units of measurement, allowing you to assess the variation relative to the mean. |\n",
    "\n",
    "Above formulas are for population, for sample, we use $n-1$ instead of $n$ in the denominator:\n",
    "- to provide an unbiased estimate of the population variance or standard deviation\n",
    "- adjustment accounts for the loss of one degree of freedom when estimating the sample mean and helps to avoid underestimating the true population variance or standard deviation\n",
    "- using $n−1$, we provide a more conservative estimate of the variability in the population, ensuring that our statistical inferences are more accurate and reliable\n",
    "\n",
    "## 5-point summary\n",
    "- minimum: the smallest value in the dataset\n",
    "- first quartile: the value such that 25% of the data falls below\n",
    "- median: the middle value in the dataset\n",
    "- third quartile: the value such that 75% of the data falls below\n",
    "- maximum: the largest value in the dataset\n",
    "\n",
    "## z-scores\n",
    "- z-score: the number of standard deviations a data point is from the mean\n",
    "- is the number $z$ given by the formula: $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "## Boxplots\n",
    "- boxplots are a graphical representation of the 5-point summary\n",
    "- outliers are observations that fall outside the upper and lower fences\n",
    "<img src=\"https://i.imgur.com/BgJweoR.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "## The Empirical Rule\n",
    "**If** a data set has an approximately bell-shaped relative frequency histogram, then:\n",
    "- Approximately 68% of the data lie within one standard deviation of the mean, that is, in the interval with endpoints $\\bar{x} \\pm s$ for samples and with endpoints $\\mu \\pm \\sigma$ for populations\n",
    "- Approximately 95% of the data lie within two standard deviations of the mean\n",
    "- Approximately 99.7% of the data lies within three standard deviations of the mean\n",
    "\n",
    "## Chebyshev’s Theorem\n",
    "For any data set, the proportion of observations that lie within $k$ standard deviations of the mean is at least $1 - \\frac{1}{k^2}$, where $k$ is any positive number larger than 1.\n",
    "- at least 75% of the data lie within two standard deviations, 89% within three standard deviations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "- A population is any specific collection of objects of interest. A sample is any subset or subcollection of the population, including the case that the sample consists of the whole population, in which case it is termed a census.\n",
    "- A measurement is a number or attribute computed for each member of a population or of a sample. The measurements of sample elements are collectively called the sample data.\n",
    "- A parameter is a number that summarizes some aspect of the population as a whole. A statistic is a number computed from the sample data.\n",
    "- Statistics computed from samples vary randomly from sample to sample. Conclusions made about population parameters are statements of probability.\n",
    "\n",
    "## Random experiments\n",
    "- random experiments are actions that occur by chance, and their outcomes are not predictable\n",
    "- sample space: the set of all possible outcomes of a random event\n",
    "    - discrete sample space: a sample space with a finite number of outcomes\n",
    "    - continuous sample space: a sample space with an infinite number of outcomes\n",
    "- event: a subset of the sample space of a random experiment\n",
    "- probability: a numerical measure of the likelihood that an event will occur\n",
    "$$ P(A) = \\frac{\\text{number of outcomes in A}}{\\text{number of outcomes in S}} = \\frac{\\text{number of favorable outcomes}}{\\text{number of possible outcomes}}$$\n",
    "- empirical probability: the relative frequency of an event occurring in a series of trials\n",
    "$$ P_{empirical}(A) = \\frac{\\text{number of times A occurs}}{\\text{number of observations}}$$\n",
    "- theoretical probability: the probability of an event occurring based on mathematical reasoning\n",
    "- law of large numbers: as the number of trials increases, the empirical probability of an event will converge to the theoretical probability of that event\n",
    "\n",
    "## Events\n",
    "- an event is a subset of the sample space of a random experiment\n",
    "- an event $A$ occurs on a particular trial of a random experiment if the outcome of that trial is in $A$\n",
    "- complement of an event: the set of all outcomes in the sample space that are not in the event\n",
    "  - the complement of an event $A$ is denoted by $A^c$\n",
    "    - $A^c = S - A, A \\cup A^c = S, A \\cap A^c = \\emptyset$\n",
    "    - $P(A^c) = 1 - P(A)$\n",
    "- union of two events: the set of all outcomes that are in either event\n",
    "  - the union of two events $A$ and $B$ is denoted by $A \\cup B$\n",
    "- intersection of two events: the set of all outcomes that are in both events\n",
    "  - the intersection of two events $A$ and $B$ is denoted by $A \\cap B$\n",
    "- mutually exclusive / disjoint events: events that have no outcomes in common\n",
    "  - if $A$ and $B$ are mutually exclusive, then $A \\cap B = \\emptyset$\n",
    "- independent events: events that have no effect on each other\n",
    "- addition rule: the probability of the union of two events is equal to the sum of the probabilities of the individual events minus the probability of their intersection $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$$\n",
    "  - if mutually exclusive, $P(A \\cap B) = 0$, then $P(A \\cup B) = P(A) + P(B)$\n",
    "\n",
    "## Probability\n",
    "- the probability of an outcome $e$ in a sample space $S$ is a number $P$ between $0$ and $1$ that measures the likelihood that $e$ will occur on a single trial of a random experiment. The probability of an event $E$ is the sum of the probabilities of the outcomes in $E$.\n",
    "- a number assigned to each member of the sample space of a random experiment that satisfies the following axioms:\n",
    "    1. $0 \\leq P(A) \\leq 1$\n",
    "    2. $P(S) = 1$\n",
    "    3. For two events $A$ and $B$, if $A$ and $B$ are mutually exclusive, then $P(A \\cup B) = P(A) + P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional probability\n",
    "\n",
    "The conditional probability of $A$ given $B$, denoted $P(A|B)$, is the probability that event $A$ has occurred in a trial of a random experiment for which it is known that event $B$ has definitely occurred.\n",
    "For any two events $A$ and $B$ with $P(B) > 0$, the conditional probability of $A$ given $B$ is defined as: $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "Here, $P(A \\cap B)$ is equal to both $P(A)P(B|A)$ and $P(B)P(A|B)$.\n",
    "\n",
    "Conditional probability relation for three events $A$, $B$, and $C$:\n",
    "$$ P(A \\cap B \\cap C) = P(A|B \\cap C)P(B \\cap C) = P(A|B \\cap C)P(B|C)P(C)$$\n",
    "\n",
    "This is called the multiplication rule, which can be extended to any number of events.\n",
    "\n",
    "## Independent events\n",
    "\n",
    "- We expect $P(A | B)$ to be different from $P(A)$, but it does not always happen. If $P(A | B) = P(A)$, then $A$ and $B$ are independent events and the occurrence of $B$ has no effect on the likelihood of $A$.\n",
    "  - $P(A|B) = P(A)$ if and only if $P(A \\cap B) = P(A)P(B)$, that is, the probability of $A$ and $B$ occurring together is equal to the product of their individual probabilities\n",
    "  - if A and B are not independent, then they are dependent and $P(A \\cap B) \\neq P(A)P(B)$\n",
    "  - independence intuitively means that the occurrence of one event does not affect the probability of the other event\n",
    "    - independence does not imply disjointness\n",
    "\n",
    "## Law of total probability\n",
    "\n",
    "If $A_1, A_2, ..., A_n$ are mutually exclusive and exhaustive events, then for any event $B$, $$ P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + ... + P(B|A_n)P(A_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem\n",
    "\n",
    "Let $P = {A_1, A_2, ..., A_n}$ be a partition of the sample space $S$ of a random experiment, and let $B$ be an event such that $P(B) > 0$. Then for any $i = 1, 2, ..., n$, $$ P(A_i|B) = \\frac{P(A_i \\cap B)}{P(B)} = \\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1) + P(A_2)P(B|A_2) + ... + P(A_n)P(B|A_n)}$$\n",
    "\n",
    "## Bayesian learning\n",
    "\n",
    "- Bayesian learning is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.\n",
    "- Naive Bayes classifier: a simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.\n",
    "    - assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "    - is a linear classifier, which means it assumes that the data is linearly separable.\n",
    "    - is a fast, simple classification algorithm that performs well on large datasets.   \n",
    "    - is a good choice when the dimensionality of the inputs is high.\n",
    "    - is often used for text classification, spam filtering, sentiment analysis, and recommender systems.\n",
    "\n",
    "## Bayes' theorem\n",
    "- in the context of machine learning, Bayes' theorem can be used to calculate the probability of a hypothesis given our prior knowledge. $$ P(h|D) = \\frac{P(D|h)P(h)}{P(D)}$$\n",
    "  - $P(h)$ is the prior probability of $h$ being true\n",
    "  - $P(D)$ is the prior probability of $D$ being true\n",
    "  - $P(h|D)$ is the posterior probability of $h$ being true given $D$\n",
    "  - $P(D|h)$ is the likelihood of $D$ given $h$\n",
    "- maximum a posteriori (MAP) estimation: a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the posterior probability. $$h_{MAP} = \\arg\\max\\limits_{h \\in H} P(h|D) = \\arg\\max\\limits_{h \\in H} \\frac{P(D|h)P(h)}{P(D)} = \\arg\\max\\limits_{h \\in H} P(D|h)P(h)$$\n",
    "  - $P(D)$ is a constant, so we can ignore it\n",
    "  - $h_{MAP}$ is called the maximum a posteriori hypothesis\n",
    "  - we find the parameter values that make the observed data most probable given our prior knowledge about the parameters\n",
    "- if we assume that all hypotheses are equally likely, then $P(h)$ is a constant, so we can ignore it. $$h_{ML} = \\arg\\max\\limits_{h \\in H} P(D|h)$$\n",
    "  - $h_{ML}$ is called the maximum likelihood hypothesis\n",
    "  - we find the parameter values that make the observed data most probable\n",
    "  - used when we have no prior knowledge about the parameters\n",
    "\n",
    "## Conditional independence\n",
    "Conditional independence, given C, is defined as independence under the probability law P(·|C). That is, A and B are conditionally independent given C if and only if $$ P(A \\cap B|C) = P(A|C)P(B|C)$$\n",
    "- independence does not imply conditional independence [(link)](https://www.youtube.com/watch?v%253DTAyA-rjmesQ%2526list%253DPLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6%2526index%253D35)\n",
    "- means that $$ P(A|B,C) = P(A|C)$$ and $$ P(A \\cap B|C) = P(A|C)P(B|C)$$\n",
    "<img src=\"https://i.imgur.com/O7op64y.jpg\" width=\"300\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "- Naive Bayes assumes that all features are conditionally independent given the class. $$ P(X_1, X_2 | Y) = P(X_1 | X_2, Y)P(X_2 | Y) = P(X_1 | Y)P(X_2 | Y)$$\n",
    "- General form : $$ P(X_1, X_2, ..., X_n | Y) = \\prod\\limits_{i=1}^n P(X_i | Y)$$\n",
    "  - how many parameters do we need to estimate? (??)\n",
    "  - how many parameters do we need to estimate if we assume that all features are conditionally independent given the class? (??)\n",
    "\n",
    "## Naive Bayes classifier\n",
    "\n",
    "- Naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.\n",
    "- We have a set of features $X = {X_1, X_2, ..., X_n}$ and a class variable $Y$.\n",
    "- We want to find the class $Y$ that maximizes the posterior probability $P(Y|X)$.\n",
    "- Then $$ P(Y = y_k|X_1, X_2, ..., X_n) = \\frac{P(Y = y_k)P(X_1, X_2, ..., X_n|Y = y_k)}{\\sum\\limits_{j} P(Y = y_j)P(X_1, X_2, ..., X_n|Y = y_j)} $$\n",
    "- Assuming conditional independence, we have $P(X_1, X_2, ..., X_n|Y = y_k) = \\prod\\limits_{i=1}^n P(X_i|Y = y_k)$. Therefore, $$P(Y = y_k|X_1, X_2, ..., X_n) = \\frac{P(Y = y_k)\\prod\\limits_{i=1}^n P(X_i|Y = y_k)}{\\sum\\limits_{j} P(Y = y_j)\\prod\\limits_{i=1}^n P(X_i|Y = y_j)}$$\n",
    "- Pick the most probable class: $$\\hat{y} = \\arg\\max\\limits_{y_k} P(Y = y_k)\\prod\\limits_{i} P(X_i|Y = y_k)$$\n",
    "\n",
    "Steps to apply Naive Bayes classifier, given a table like this:\n",
    "\n",
    "|Weather|Play|\n",
    "|---|---|\n",
    "|Sunny|No|\n",
    "|...|...|\n",
    "|Rainy|Yes|\n",
    "\n",
    "We convert it into a frequency table like this:\n",
    "\n",
    "|Weather|No|Yes|Total|Prob|\n",
    "|---|---|---|---|---|\n",
    "|Sunny|2|3|5| P(Sunny) = $\\frac{5}{14}$|\n",
    "|Overcast|0|4|4| P(Overcast) = $\\frac{4}{14}$|\n",
    "|Rainy|3|2|5| P(Rainy) = $\\frac{5}{14}$|\n",
    "|Total|5|9|14|1|\n",
    "|Prob|P(No) = $\\frac{5}{14}$|P(Yes) = $\\frac{9}{14}$|1|\n",
    "\n",
    "Then we can calculate the posterior probability of each class, given the evidence (weather), for example, $P(Yes|Sunny)$:\n",
    "$$ P(Yes|Sunny) = \\frac{P(Sunny|Yes)P(Yes)}{P(Sunny)} = \\frac{\\frac{3}{9}\\frac{9}{14}}{\\frac{5}{14}} = \\frac{3}{5}$$\n",
    "\n",
    "If there are multiple features, we can calculate the posterior probability of each class, given the evidence (weather and temperature), for example, $P(Yes|Sunny, Cool)$:\n",
    "$$ P(Yes|Sunny, Cool) = \\frac{P(Sunny, Cool|Yes)P(Yes)}{P(Sunny, Cool)} = \\frac{P(Sunny|Yes)P(Cool|Yes)P(Yes)}{P(Sunny)P(Cool)} $$\n",
    "\n",
    "## Naive Bayes for text classification\n",
    "You need a document $d$, a set of classes $C = {c_1, c_2, ..., c_n}$, and a set of $m$ hand-labelled documents $(d_1, c_1), (d_2, c_2), ..., (d_m, c_m)$. The for a document $d$, we want to find the class $c$ that maximizes the posterior probability $P(c|d)$.\n",
    "$$ P(c|d) = \\frac{P(c)P(d|c)}{P(d)} = \\frac{P(c)\\prod\\limits_{i=1}^n P(w_i|c)}{P(d)}$$\n",
    "Here, there are two assumptions : bag of words (position doesn't matter) and conditional independence.\n",
    "Then, we pick the most probable class: $$c_{MAP} = \\arg\\max\\limits_{c} P(c)\\prod\\limits_{i=1}^n P(w_i|c)$$\n",
    "Here, $$ P(c_j) = \\frac{docCount(C = c_j)}{N_{doc}} $$ and $$ P(w_i|c_j) = \\frac{wordCount(w_i, C = c_j)}{\\sum\\limits_{w \\in V} wordCount(w, C = c_j)} $$, where $V$ is the vocabulary.\n",
    "This has a problem of zero probability, so we use Laplace smoothing: $$ P(w_i|c_j) = \\frac{wordCount(w_i, C = c_j) + 1}{\\sum\\limits_{w \\in V} wordCount(w, C = c_j) + |V|} $$ where $|V|$ is the number of distinct words in the dataset.\n",
    "\n",
    "[Example](https://www.fi.muni.cz/~sojka/PV211/p13bayes.pdf):\n",
    "<img src=\"https://i.imgur.com/p3nZUNM.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "<img src=\"https://i.imgur.com/kcNsCro.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "Therefore, $$P(C|d_5) = \\frac{3}{4} {(\\frac{3}{7})}^3 \\frac{1}{14} \\frac{1}{14} \\frac{1}{P(d_5)}$$\n",
    "and $$P(\\bar{C} | d_5) = \\frac{1}{4} {(\\frac{2}{9})}^3 \\frac{2}{9} \\frac{2}{9} \\frac{1}{P(d_5)}$$\n",
    "\n",
    "$P(d_5)$ is the same for both classes, so we can ignore it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random variables\n",
    "- random variables are variables that take on numerical values based on the outcome of a random experiment\n",
    "- discrete random variables: random variables that can take on a finite number of values\n",
    "- continuous random variables: random variables that can take on an infinite number of values\n",
    "\n",
    "## Probability distributions of discrete random variables\n",
    "The probability distribution of a discrete random variable $X$ is a list of each possible value of $X$ together with the probability that $X$ takes that value in one trial of the experiment.\n",
    "The probabilities in the probability distribution of a discrete random variable $X$ must satisfy the following two conditions:\n",
    "1. $0 \\leq P(X = x) \\leq 1$ for each possible value $x$ of $X$\n",
    "2. $\\sum_{\\text{all } x} P(X = x) = 1$\n",
    "\n",
    "Example : probability distribution of $X$, the sum of the two dice, is given by:\n",
    "\n",
    "$$\\begin{array}{c|ccccccccccc} x &2 &3 &4 &5 &6 &7 &8 &9 &10 &11 &12 \\\\ \\hline P(x) &\\dfrac{1}{36} &\\dfrac{2}{36} &\\dfrac{3}{36} &\\dfrac{4}{36} &\\dfrac{5}{36} &\\dfrac{6}{36} &\\dfrac{5}{36} &\\dfrac{4}{36} &\\dfrac{3}{36} &\\dfrac{2}{36} &\\dfrac{1}{36} \\\\ \\end{array}$$\n",
    "\n",
    "- $P(X \\geq 9) = P(X = 9) + P(X = 10) + P(X = 11) + P(X = 12) = \\dfrac{10}{36} = \\dfrac{5}{18}$\n",
    "- $P(\\text{X is even}) = P(X = 2) + P(X = 4) + P(X = 6) + P(X = 8) + P(X = 10) + P(X = 12) = \\dfrac{18}{36} = \\dfrac{1}{2}$\n",
    "\n",
    "### Mean of a discrete random variable\n",
    "The mean (expected value / expectation) of a discrete random variable $X$ is the weighted average of the possible values of $X$, where the weights are the probabilities of the values of $X$.\n",
    "$$ \\mu = E(X) = \\sum_{\\text{all } x} xP(x)$$\n",
    "\n",
    "The mean of a discrete random variable is the long-run average value of the variable.\n",
    "\n",
    "Rules of expected value:\n",
    "1. $E(aX + b) = aE(X) + b$\n",
    "2. $E(X + Y) = E(X) + E(Y)$\n",
    "3. $E(XY) = E(X)E(Y)$ iff $X$ and $Y$ are independent\n",
    "\n",
    "### Variance and standard deviation of a discrete random variable\n",
    "The variance of a discrete random variable $X$ is the weighted average of the squared deviations of the possible values of $X$ from the mean of $X$, where the weights are the probabilities of the values of $X$.\n",
    "$$ \\sigma^2 = Var(X) = \\sum(x - \\mu)^2 P(x) = [\\sum x^2 P(x)] - \\mu^2 = E(X^2) - [E(X)]^2$$\n",
    "\n",
    "The standard deviation of a discrete random variable $X$ is the square root of the variance of $X$.\n",
    "$$ \\sigma = \\sqrt{Var(X)}$$\n",
    "\n",
    "Rules of variance:\n",
    "1. $Var(aX + b) = a^2Var(X)$\n",
    "2. $Var(X + Y) = Var(X) + Var(Y)$ if $X$ and $Y$ are independent\n",
    "\n",
    "## Probability distribution of a continuous random variable\n",
    "With continuous random variables one is concerned not with the event that the variable assumes a single particular value, but with the event that the random variable assumes a value in a particular interval.\n",
    "\n",
    "The probability distribution of a continuous random variable $X$ is an assignment of probabilities to intervals of decimal numbers using a function $f(x)$, called a density function, in the following way: the probability that $X$ assumes a value in the interval $[a, b]$ is equal to the area of the region that is bounded above by the graph of the equation $y=f(x)$, bounded below by the x-axis, and bounded on the left and right by the vertical lines through $a$ and $b$. The probability density function $f(x)$ must satisfy the following two conditions:\n",
    "1. $f(x) \\geq 0$ for all $x$\n",
    "2. $\\int_{-\\infty}^{\\infty} f(x) dx = 1$\n",
    "3. $P(a \\leq X \\leq b) = \\int_a^b f(x) dx$\n",
    "\n",
    "### Cumulative probability distribution function\n",
    "The cumulative probability distribution function of a continuous random variable $X$ is the function $F(x)$ defined by $F(x) = P(X \\leq x)$ for all $x$.\n",
    "$$ F(x) = \\int_{-\\infty}^x f(u) du$$\n",
    "\n",
    "To get the probability density function $f(x)$ from the cumulative probability distribution function $F(x)$, we differentiate $F(x)$ with respect to $x$.\n",
    "$$ f(x) = \\frac{d}{dx} F(x)$$\n",
    "\n",
    "### Mean of a continuous random variable\n",
    "The mean (expected value / expectation) of a continuous random variable $X$ is the weighted average of the possible values of $X$, where the weights are the probabilities of the values of $X$.\n",
    "$$ \\mu = E(X) = \\int_{-\\infty}^{\\infty} xf(x) dx$$\n",
    "\n",
    "### Variance and standard deviation of a continuous random variable\n",
    "The variance of a continuous random variable $X$ is the weighted average of the squared deviations of the possible values of $X$ from the mean of $X$, where the weights are the probabilities of the values of $X$.\n",
    "$$ \\sigma^2 = Var(X) = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) dx = \\int_{-\\infty}^{\\infty} x^2 f(x) dx - \\mu^2 = E(X^2) - [E(X)]^2$$\n",
    "\n",
    "The standard deviation of a continuous random variable $X$ is the square root of the variance of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint probability distributions\n",
    "\n",
    "## Joint probability distribution of two discrete random variables\n",
    "The joint probability distribution of two discrete random variables $X$ and $Y$ is a list of each possible pair of values of $X$ and $Y$ together with the probability that $X$ takes the first value and $Y$ takes the second value in one trial of the experiment. It's also called the joint probability mass function $f(x, y) = P(X = x, Y = y)$.\n",
    "The probabilities in the joint probability distribution of two discrete random variables $X$ and $Y$ must satisfy the following two conditions:\n",
    "1. $0 \\leq f(x,y) \\leq 1$ for each possible pair of values $(x, y)$ of $X$ and $Y$\n",
    "2. $\\sum_{\\text{all } x} \\sum_{\\text{all } y} f(x,y) = 1$\n",
    "\n",
    "## Marginal probability distribution of a discrete random variable\n",
    "The marginal probability distribution of a discrete random variable $X$ is the probability distribution of $X$ alone, regardless of the value of $Y$. It's also called the marginal probability mass function $f_X(x) = P(X = x)$.\n",
    "$$ f_X(x) = \\sum_{\\text{all } y} f(x,y)$$\n",
    "$$ f_Y(y) = \\sum_{\\text{all } x} f(x,y)$$\n",
    "\n",
    "## Joint probability distribution of two continuous random variables\n",
    "The joint probability distribution of two continuous random variables $X$ and $Y$ is an assignment of probabilities to regions of the $xy$-plane using a function $f(x,y)$, called a joint density function, in the following way: the probability that $(X, Y)$ assumes a value in the region $R$ is equal to the volume of the region that is bounded above by the graph of the equation $z=f(x,y)$, bounded below by the $xy$-plane, and bounded on the left and right by region $R$. \n",
    "The joint density function $f(x,y)$ must satisfy the following two conditions:\n",
    "1. $f(x,y) \\geq 0$ for all $(x,y)$\n",
    "2. $\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1$\n",
    "3. $P((X,Y) \\in R) = \\int \\int_R f(x,y) dx dy$\n",
    "\n",
    "## Marginal probability distribution of a continuous random variable\n",
    "The marginal probability distribution of a continuous random variable $X$ is the probability distribution of $X$ alone, regardless of the value of $Y$. It's also called the marginal probability density function $f_X(x)$.\n",
    "$$ f_X(x) = \\int_{-\\infty}^{\\infty} f(x,y) dy$$\n",
    "$$ f_Y(y) = \\int_{-\\infty}^{\\infty} f(x,y) dx$$\n",
    "\n",
    "Cumulative probability distribution tables, when available, facilitate computation of probabilities encountered in typical practical situations.\n",
    "- In place of $P(X = x)$, we can use $P(X \\leq x) = P(X = 0) + P(X = 1) + \\cdots + P(X = x)$\n",
    "- Here, $P(X \\geq x) = 1 - P(X < x) = 1 - P(X \\leq x - 1)$\n",
    "- and $P(x) = P(X \\leq x) - P(X \\leq x - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special probability distributions\n",
    "\n",
    "## Uniform distribution\n",
    "- Discrete uniform distribution: the discrete random variable $X$ that has a probability distribution given by the formula $$ P(X = x) = \\frac{1}{n}$$ for $x = 1, 2, ..., n$ is said to have the discrete uniform distribution with parameter $n$.\n",
    "- In the continuous case, the uniform distribution is a probability distribution wherein all intervals of the same length on the distribution's support are equally probable. The support is defined by the two parameters, $a$ and $b$, which are its minimum and maximum values. The distribution is often abbreviated $U(a, b)$.\n",
    "$$ f(x) = \\frac{1}{b - a} \\text{ for } a \\leq x \\leq b $$\n",
    "- The mean, variance of the uniform random variable $X$ with parameters $a$ and $b$ are given by the formulas:\n",
    "    $$ \\mu = E(X) = \\frac{a + b}{2}$$\n",
    "    $$ \\sigma^2 = Var(X) = \\frac{(b - a)^2}{12}$$\n",
    "\n",
    "\n",
    "## Bernoulli distribution\n",
    "$$ X \\sim Bern(p)$$\n",
    "- Models a single trial of a random experiment that has two possible outcomes, `success` or `failure`.\n",
    "- The probability of `success` is $p$ and the probability of `failure` is $1 - p$.\n",
    "- The discrete random variable $X$ that has a probability distribution given by the formula $$ P(X = x) = p^x (1 - p)^{1 - x} \\text{ for } x = 0, 1$$ is said to have the Bernoulli distribution with parameter $p$.\n",
    "- The mean, variance of the Bernoulli random variable $X$ with parameter $p$ are given by the formulas:\n",
    "    $$ \\mu = E(X) = p$$\n",
    "    $$ \\sigma^2 = Var(X) = p(1 - p)$$\n",
    "\n",
    "## Binomial distribution\n",
    "$$ X \\sim Bin(n, p)$$\n",
    "- The discrete random variable $X$ that counts the number of successes in $n$ identical, independent trials of a procedure that always results in either of two outcomes, `success` or `failure` and in which the probability of success on each trial is the same number $p$, is called the binomial random variable with parameters $n$ and $p$.\n",
    "- There is a formula for the probability that the binomial random variable with parameters $n$ and $p$ will take a particular value $x$.\n",
    "    $$ P(X = x) = \\binom{n}{x} p^x (1 - p)^{n - x} = \\frac{n!}{x!(n - x)!} p^x (1 - p)^{n - x} \\text{ for } x = 0, 1, ..., n$$\n",
    "- The mean, variance of the binomial random variable $X$ with parameters $n$ and $p$ are given by the formulas: (derivation in slides)\n",
    "    $$ \\mu = E(X) = np$$\n",
    "    $$ \\sigma^2 = Var(X) = np(1 - p)$$\n",
    "\n",
    "## Poisson distribution\n",
    "$$ X \\sim Poisson(\\lambda)$$\n",
    "- The discrete random variable $X$ that counts the number of occurrences of an event over a specified interval of time or space is said to have the Poisson distribution with parameter $\\lambda$.\n",
    "- The probability that the Poisson random variable $X$ with parameter $\\lambda$ will take a particular value $x$ is given by the formula: $$ P(X = x) = \\frac{e^{-\\lambda} \\lambda^x}{x!} \\text{ for } x = 0, 1, 2, ... \\infty $$ where $e \\approx 2.718$ and $\\lambda$ is the average number of occurrences per interval.\n",
    "- The mean, variance of the Poisson random variable $X$ with parameter $\\lambda$ are given by the formulas:\n",
    "    $$ \\mu = E(X) = \\lambda$$\n",
    "    $$ \\sigma^2 = Var(X) = \\lambda$$\n",
    "- The Poisson distribution is a limiting case of the binomial distribution when the number of trials $n$ is large and the probability of success $p$ is small.\n",
    "\n",
    "## Normal distribution\n",
    "$$ X \\sim N(\\mu, \\sigma)$$\n",
    "- The probability distribution corresponding to the density function for the bell curve with parameters $\\mu$ and $\\sigma$ is called the normal distribution with mean $\\mu$ and standard deviation $\\sigma$. A continuous random variable whose probabilities are described by the normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is called a normally distributed random variable, or a normal random variable for short, with mean $\\mu$ and standard deviation $\\sigma$.\n",
    "- The density curve for the normal distribution is symmetric about the mean $\\mu$.\n",
    "- The density curve for the normal distribution with mean $\\mu$ and standard deviation $\\sigma$ is given by the equation:\n",
    "    $$ y = f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}$$\n",
    "    where $\\pi \\approx 3.14159$ and $e \\approx 2.71828$ \n",
    "- Standard normal distribution is the normal distribution with mean $\\mu = 0$ and standard deviation $\\sigma = 1$, denoted by $Z = N(0, 1)$.\n",
    "    $$ y = f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}x^2}$$\n",
    "    - rules for help computing $P(Z)$:\n",
    "      - $P(Z \\leq z) = P(Z < z) + P(Z = z) = P(Z < z)$\n",
    "      - $P(Z \\geq z) = 1 - P(Z < z)$\n",
    "      - $P(z_1 \\leq Z \\leq z_2) = P(Z \\leq z_2) - P(Z < z_1)$\n",
    "- 68-95-99.7 rule: for any normal distribution, approximately 68% of the observations fall within one standard deviation of the mean, approximately 95% of the observations fall within two standard deviations of the mean, and approximately 99.7% of the observations fall within three standard deviations of the mean.\n",
    "- If $X$ is a normally distributed random variable with mean $\\mu$ and standard deviation $\\sigma$, then\n",
    "    $$P(X \\leq a) = P\\left(\\frac{X - \\mu}{\\sigma} \\leq \\frac{a - \\mu}{\\sigma}\\right) = P\\left(Z \\leq \\frac{a - \\mu}{\\sigma}\\right)$$\n",
    "    $$P(a < X < b) = P\\left( \\frac{a - \\mu}{\\sigma} < Z < \\frac{b - \\mu}{\\sigma} \\right)$$\n",
    "    where $Z$ is a standard normal random variable, and $a$ and $b$ are any two real numbers with $a < b$.\n",
    "    - The new endpoints $\\frac{a - \\mu}{\\sigma}$ and $\\frac{b - \\mu}{\\sigma}$ are called the standard score or z-score of the original endpoints $a$ and $b$.\n",
    "- normal approximation to the binomial distribution: if $X$ is a binomial random variable with parameters $n$ and $p$, then for large $n$, the distribution of $X$ is approximately normal with mean $\\mu = np$ and standard deviation $\\sigma = \\sqrt{np(1 - p)}$ (if $n > 30$, $np > 15$, and $n(1 - p) > 15$\n",
    "    - continuity correction: when approximating a discrete distribution with a continuous distribution, we can add or subtract $0.5$ to the endpoints of the interval to account for the fact that the continuous distribution is continuous and the discrete distribution is not.\n",
    "    - $P(X \\leq a) \\approx P\\left(Z \\leq \\frac{a + 0.5 - \\mu}{\\sigma}\\right)$\n",
    "    - $P(a < X < b) \\approx P\\left( \\frac{a - 0.5 - \\mu}{\\sigma} < Z < \\frac{b + 0.5 - \\mu}{\\sigma} \\right)$\n",
    "\n",
    "## t-distribution\n",
    "$$ X \\sim t(n)$$\n",
    "- The t-distribution has the probability density function:\n",
    "    $$ f(x) = \\frac{\\Gamma(\\frac{n + 1}{2})}{\\sqrt{n\\pi} \\Gamma(\\frac{n}{2})} \\left(1 + \\frac{x^2}{n}\\right)^{-\\frac{n + 1}{2}}$$\n",
    "    where $\\Gamma$ is the gamma function and $n$ is the degrees of freedom.\n",
    "- Used when working with small samples (less than 30) and when the population standard deviation is unknown.\n",
    "- Similar to the standard normal distribution, but with heavier tails, accounting for the extra variability in small samples.\n",
    "- Often used in hypothesis testing and confidence intervals for the mean of a population.\n",
    "- The t-distibution arises as the sampling distribution of the t-statistic.\n",
    "- Let $x_1, x_2, ..., x_n$ be a random sample from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. Then the random variable $$ t = \\frac{\\bar{x} - \\mu}{s / \\sqrt{n}}$$ has a t-distribution with $n - 1$ degrees of freedom, where $\\bar{x}$ is the sample mean and $s$ is the unbiased sample standard deviation.\n",
    "\n",
    "## Chi-square distribution\n",
    "$$ X \\sim \\chi^2(n)$$\n",
    "- The chi-square distribution with $n$ degrees of freedom is the distribution of the sum of the squares of $n$ independent standard normal random variables.\n",
    "- The chi-square distribution has the probability density function:\n",
    "    $$ f(x) = \\frac{1}{2^{\\frac{n}{2}} \\Gamma(\\frac{n}{2})} x^{\\frac{n}{2} - 1} e^{-\\frac{x}{2}}$$\n",
    "    where $\\Gamma$ is the gamma function and $n$ is the degrees of freedom.\n",
    "- Not symmetric, skewed to the right, varies from $0$ to $\\infty$.\n",
    "- Depends on the degrees of freedom $n$.\n",
    "- Used in hypothesis testing and confidence intervals for the variance of a population, measure of goodness of fit, and test of independence.\n",
    "\n",
    "## F-distribution\n",
    "$$ X \\sim F(n_1, n_2)$$\n",
    "- The F-distribution with $n_1$ and $n_2$ degrees of freedom is the distribution of the ratio of two independent chi-square random variables divided by their respective degrees of freedom.\n",
    "- The F-distribution has the probability density function:\n",
    "    $$ f(x) = \\frac{\\Gamma(\\frac{n_1 + n_2}{2})}{\\Gamma(\\frac{n_1}{2}) \\Gamma(\\frac{n_2}{2})} \\left(\\frac{n_1}{n_2}\\right)^{\\frac{n_1}{2}} x^{\\frac{n_1}{2} - 1} \\left(1 + \\frac{n_1}{n_2}x\\right)^{-\\frac{n_1 + n_2}{2}}$$\n",
    "    where $\\Gamma$ is the gamma function and $n_1$ and $n_2$ are the degrees of freedom.\n",
    "- Not symmetric, skewed to the right, varies from $0$ to $\\infty$.\n",
    "- Depends on the degrees of freedom $n_1$ and $n_2$, and also the order in which they are written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "- population: the entire group of individuals or instances about whom we hope to learn\n",
    "- sample: a subset of the population, examined in hope of learning about the population\n",
    "- two types of sampling:\n",
    "    1. random sampling: each individual is chosen randomly and entirely by chance\n",
    "        1. simple random sampling: each individual has the same chance of being chosen\n",
    "        2. stratified sampling: the population is divided into groups, called strata, and a random sample is taken from each stratum\n",
    "            - potential to match the overall population's demographics better than simple random sampling\n",
    "    2. non-random sampling: individuals are chosen by some non-random mechanism, and not by chance\n",
    "        1. convenience sampling: individuals are chosen based on the ease of access\n",
    "        2. snowball sampling: individuals are chosen based on referrals from other individuals\n",
    "        3. quota sampling: individuals are chosen based on pre-specified quotas regarding demographics, etc.\n",
    "        4. judgement sampling: individuals are chosen based on the judgement of the researcher\n",
    "- sampling variability: the value of a statistic varies in repeated random sampling, it decreases as the sample size increases\n",
    "\n",
    "# Sampling distributions\n",
    "\n",
    "- The sampling distribution of a statistic is the probability distribution of the statistic when the statistic is computed from samples of the same size from the same population.\n",
    "- There are formulas that relate the mean and standard deviation of the sample mean to the mean and standard deviation of the population from which the sample is drawn.\n",
    "- For example, consider random variable $\\bar{X}$, the sampling distribution of the sample mean, when the sample size is $n$. The mean of this r.v. is $\\mu_{\\bar{X}}$ and the standard deviation is $\\sigma_{\\bar{X}}$. Then:\n",
    "    $$ \\mu_{\\bar{X}} = \\mu$$\n",
    "    $$ \\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}$$ \n",
    "    where $\\mu$ and $\\sigma$ are the mean and standard deviation of the population.\n",
    "- The shape of the sampling distribution of $\\bar{X}$ is approximately normal if the sample size is large enough.\n",
    "- As $n$ increases, the shape of the sampling distribution of $\\bar{X}$ becomes more and more like the shape of the normal distribution. The probabilities on the lower and upper ends shrink, and the probabilities in the middle become larger in relation. \n",
    "<img src=\"https://i.imgur.com/9lItK6A.jpg\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "## Central limit theorem\n",
    "- In general, one may start with any distribution and the sampling distribution of the sample mean will increasingly resemble the bell-shaped normal curve as the sample size increases. This is the content of the Central Limit Theorem.\n",
    "- For sample sizes of 30 or more, the sampling distribution of the sample mean is approximately normal, regardless of the shape of the population distribution, with mean $\\mu_{\\bar{X}} = \\mu$ and standard deviation $\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}$. The larger the sample size, the better the approximation.\n",
    "\n",
    "<img src=\"https://i.imgur.com/0vzPLFF.jpg\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "- The importance of CLT is that it allows us to make probability statements about the sample mean, in relation to its value in comparison to the population mean. \n",
    "Realize there are two distributions involved: \n",
    "1. $X$, the population distribution, mean $\\mu$, standard deviation $\\sigma$\n",
    "2. $\\bar{X}$, the sampling distribution of the sample mean, mean $\\mu_{\\bar{X}} = \\mu$, standard deviation $\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "- For samples of any size drawn from a normal population, the sampling distribution of the sample mean is normal. For samples of any size drawn from a non-normal population, the sampling distribution of the sample mean is approximately normal if the sample size is 30 or more.\n",
    "- If the size of the population is finite, then we apply a correction factor to the standard deviation of the sample mean:\n",
    "    $$ \\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}} \\sqrt{\\frac{N - n}{N - 1}}$$\n",
    "    where $N$ is the population size and $n$ is the sample size.\n",
    "    - thumb rule : if sample size is less than 5% of the population size, then we can ignore the correction factor.\n",
    "\n",
    "## Sample proportion\n",
    "- There are formulas that relate the mean and standard deviation of the sample proportion to the mean and standard deviation of the population from which the sample is drawn.\n",
    "- sample proportion is the percentage of the sample that has a certain characteristic $\\hat{p}$, as opposed to the population proportion $p$.\n",
    "- viewed as a random variable, $\\hat{P}$ has a mean $\\mu_{\\hat{P}}$ and a standard deviation $\\sigma_{\\hat{P}}$.\n",
    "- if $np > 15$ and $n(1 - p) > 15$, then the relation to population proportion $p$:\n",
    "    $$ \\mu_{\\hat{P}} = p$$\n",
    "    $$ \\sigma_{\\hat{P}} = \\sqrt{\\frac{p(1 - p)}{n}}$$\n",
    "- CLT applies to sample proportion as well, but the condition is more complex:\n",
    "  - for large samples, sample proportion is normally distributed with mean $p$ and standard deviation $\\sqrt{\\frac{p(1 - p)}{n}}$.\n",
    "  - to check if sample size is large enough, we need to check if $\\left[ p - 3 \\sigma_{\\hat{P}}, p + 3 \\sigma_{\\hat{P}} \\right]$ lies wholly within the interval $[0, 1]$.\n",
    "    - since $p$ is unknown, we use $\\hat{p}$ instead.\n",
    "    - since $\\sigma_{\\hat{P}}$ is unknown, we use $\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "- The goal of estimation is to use sample data to estimate the value of an unknown population parameter.\n",
    "- Point estimation : use a single value to estimate a population parameter\n",
    "  - e.g. use sample mean $\\bar{x}$ to estimate population mean $\\mu$\n",
    "  - problem: we don't know how reliable the estimate is\n",
    "- Interval estimation : use an interval of values to estimate a population parameter\n",
    "  - we use the data to compute $E$, such that $[\\bar{x} - E, \\bar{x} + E]$ has a certain probability of containing the population parameter $\\mu$.\n",
    "  - we do this in such a way that, $95\\%$ of the all the intervals constructed from sample data will contain the population parameter $\\mu$. \n",
    "  - $E$ is called the margin of error, and the interval is called the $95\\%$ confidence interval for $\\mu$.\n",
    "\n",
    "The empirical rule states that you must go about 2 standard deviations in either direction from the mean to capture $95\\%$ of the values of $\\bar{X}$.\n",
    "\n",
    "The key idea is that, in sample after sample $95\\%$ of the values of $\\bar{X}$ lie in the interval $[\\mu - E, \\mu + E]$. So if we adjoin to each side of the point estimate $x$ a wing of length E, $95\\%$ of the time the wing will contain the population mean $\\mu$.\n",
    "- $95\\%$ confidence interval is thus $\\hat{x} \\pm 1.960 \\frac{\\sigma}{\\sqrt{n}}$\n",
    "  - for a different confidence level, use a different multiplier instead of $1.960$.\n",
    "  - Here, $1.960$ is the value of $z$ such that $P(-1.960 < Z < 1.960) = 0.95$, and is given by $z_{\\alpha/2} = z_{0.025} = 1.960$, where $\\alpha = 0.05$\n",
    "\n",
    "<img src=\"https://i.imgur.com/u3ME5Qj.png\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "In selecting the correct formula for construction of a confidence interval for a population mean ask two questions: is the population standard deviation $\\sigma$ known or unknown, and is the sample large or small?\n",
    "\n",
    "\n",
    "## Large sample $100(1 - \\alpha)\\%$ confidence interval for $\\mu$\n",
    "- if $\\sigma$ is known:\n",
    "    $$ \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "- if $\\sigma$ is unknown:\n",
    "    $$ \\bar{x} \\pm z_{\\alpha/2} \\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "A sample of size $n$ is large if $n \\geq 30$ or if the population from which the sample is drawn is normal or approximately normal.\n",
    "The number $E = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$ or $E = z_{\\alpha/2} \\frac{s}{\\sqrt{n}}$ is called the margin of error for the estimate $\\bar{x}$ of $\\mu$.\n",
    "\n",
    "## Small sample $100(1 - \\alpha)\\%$ confidence interval for $\\mu$\n",
    "\n",
    "We use the Student's $t$ distribution instead of the $z$ distribution. The $t$ distribution is similar to the $z$ distribution, but it is more spread out. The spread increases as the degrees of freedom decrease. The $t$ distribution is symmetric and bell-shaped, but it has more area in the tails than the $z$ distribution.\n",
    "\n",
    "- if $\\sigma$ is known:\n",
    "    $$ \\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "- if $\\sigma$ is unknown:\n",
    "    $$ \\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}$$ \n",
    "    with the degrees of freedom $df = n - 1$\n",
    "\n",
    "The population must be normal or approximately normal. \n",
    "\n",
    "## Large sample estimation for population proportion $p$\n",
    "\n",
    "- $100(1 - \\alpha)\\%$ confidence interval for $p$:\n",
    "    $$ \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}$$\n",
    "\n",
    "## Finding the minimum sample size\n",
    "\n",
    "We have a population with mean $\\mu$ and standard deviation $\\sigma$. We want to estimate the population mean $\\mu$ to within $E$ with $100(1 - \\alpha)\\%$ confidence. What is the minimum sample size $n$ required?\n",
    "$$ n = \\left( \\frac{z_{\\alpha/2} \\sigma}{E} \\right)^2 \\text{rounded up}$$\n",
    "\n",
    "For population proportion $p$, we have:\n",
    "$$ n = \\left( \\frac{z_{\\alpha/2} \\sqrt{p(1 - p)}}{E} \\right)^2 \\text{rounded up}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "- The null hypothesis $H_0$ is a statement about the value of a population parameter that is assumed to be true until there is convincing evidence to the contrary (status quo).\n",
    "- The alternative hypothesis $H_a$ is a statement that is accepted if the sample data provide sufficient evidence that the null hypothesis is false.\n",
    "- Hypothesis testing is a statistical procedure that uses sample data to decide between two competing claims (hypotheses) about a population parameter.\n",
    "- Two conclusions are possible:\n",
    "  - Reject the null hypothesis $H_0$ in favor of the alternative hypothesis $H_a$.\n",
    "  - Do not reject the null hypothesis $H_0$.\n",
    "\n",
    "### Errors in hypothesis testing\n",
    "\n",
    "|  | $H_0$ is true | $H_0$ is false |\n",
    "| --- | --- | --- |\n",
    "| Reject $H_0$ | Type I error | Correct decision |\n",
    "| Do not reject $H_0$ | Correct decision | Type II error |\n",
    "\n",
    "## Logic of hypothesis testing\n",
    "\n",
    "The test procedure is based on the initial assumption that $H_0$ is true.\n",
    "\n",
    "The criterion for judging between $H_0$ and $H_a$ based on the sample data is: if the value of $\\bar{X}$ would be highly unlikely to occur if $H_0$ were true, but favors the truth of $H_a$, then we reject $H_0$ in favor of $H_a$. Otherwise, we do not reject $H_0$.\n",
    "\n",
    "Supposing for now that $\\bar{X}$ follows a normal distribution, when the null hypothesis is true, the density function for the sample mean $\\bar{X}$ must be a bell curve centered at $\\mu_0$. Thus, if $H_0$ is true, then $\\bar{X}$ is likely to take a value near $\\mu_0$ and is unlikely to take values far away. Our decision procedure, therefore, reduces simply to:\n",
    "\n",
    "- If $H_a$ has the form $H_a: \\mu < \\mu_0$, then reject $H_0$ if $\\bar{x}$ is far to the left of $\\mu_0$ (rejection region is $[\\infty, C]$, left-tailed test)\n",
    "- If $H_a$ has the form $H_a: \\mu > \\mu_0$, then reject $H_0$ if $\\bar{x}$ is far to the right of $\\mu_0$ (rejection region is $[C, \\infty]$, right-tailed test)\n",
    "- If $H_a$ has the form $H_a: \\mu \\neq \\mu_0$, then reject $H_0$ if $\\bar{x}$ is far away from $\\mu_0$ in either direction (rejection region is $(-\\infty, C] \\cup [C', \\infty)$, two-tailed test)\n",
    "\n",
    "Rejection region is therefore the set of values of $\\bar{x}$ that are far away from $\\mu_0$ in the direction indicated by $H_a$. The critical value or critical values of a test of hypotheses are the number or numbers that determine the rejection region.\n",
    "\n",
    "Procedure for selecting $C$:\n",
    "- define a rare event : an event is rare if it has a probability of occurring that is less than or equal to $\\alpha$. (say $\\alpha = 0.01$)\n",
    "- then critical value $C$ is the value of $\\bar{x}$ that cuts off a tail of area $\\alpha$ in the appropriate direction.\n",
    "  - when the rejection region is in two tails, the critical values are the values of $\\bar{x}$ that cut off a tail of area $\\alpha/2$ in each direction.\n",
    "\n",
    "<img src=\"https://i.imgur.com/4xBkgrW.png\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "For example, $z_{0.005} = 2.58$ is the critical value for a test of $H_0: \\mu = 100$ against $H_a: \\mu \\neq 100$ at the $\\alpha = 0.01$ level of significance. The critical value will be $$ C = 100 \\pm 2.58 \\cdot \\sigma_{\\bar{x}} = 100 \\pm 2.58 \\cdot \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two populations\n",
    "\n",
    "Consider two populations:\n",
    "1. Population 1 with mean $\\mu_1$ and standard deviation $\\sigma_1$, sampling we get a sample of size $n_1$ with sample mean $\\bar{x}_1$ and sample standard deviation $s_1$\n",
    "2. Population 2 with mean $\\mu_2$ and standard deviation $\\sigma_2$, sampling we get a sample of size $n_2$ with sample mean $\\bar{x}_2$ and sample standard deviation $s_2$\n",
    "\n",
    "Our goal is to compare the two populations, by estimating the difference between the two population means $\\mu_1 - \\mu_2$, using the samples.\n",
    "\n",
    "Samples from two distinct populations are independent if each one is drawn without reference to the other, and has no connection with the other.\n",
    "\n",
    "### 100(1 - $\\alpha$)% confidence interval for $\\mu_1 - \\mu_2$ for large samples\n",
    "- A point estimate for the difference in two population means is simply the difference in the corresponding sample means.\n",
    "- A confidence interval for the difference in two population means is computed using a formula in the same fashion as was done for a single population mean.\n",
    "$$ (\\bar{x}_1 - \\bar{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$$\n",
    "\n",
    "### Hypothesis testing for $\\mu_1 - \\mu_2$ for large samples\n",
    "The same five-step procedure used to test hypotheses concerning a single population mean is used to test hypotheses concerning the difference between two population means. The only difference is in the formula for the standardized test statistic.\n",
    "\n",
    "$$ H_0: \\mu_1 - \\mu_2 = D_0$$\n",
    "$$ H_a: \\mu_1 - \\mu_2 < D_0 \\text{ or } \\mu_1 - \\mu_2 > D_0 \\text{ or } \\mu_1 - \\mu_2 \\neq D_0$$\n",
    "\n",
    "Standardized test statistic:\n",
    "$$ Z = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}$$\n",
    "The samples must be independent and the population distributions must be normal or the sample sizes must be large.\n",
    "\n",
    "### 100(1 - $\\alpha$)% confidence interval for $\\mu_1 - \\mu_2$ for small samples\n",
    "$$ (\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2} \\sqrt{s_p^2 \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}$$\n",
    "where $s_p^2$ is the pooled sample variance, defined as\n",
    "$$ s_p^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}$$\n",
    "and the number of degrees of freedom is $n_1 + n_2 - 2$.\n",
    "\n",
    "### Hypothesis testing for $\\mu_1 - \\mu_2$ for small samples\n",
    "$$ T = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{s_p^2 \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$$\n",
    "The test statistic has Student's t distribution with $n_1 + n_2 - 2$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and correlation\n",
    "Two variables $x$ and $y$ have a deterministic linear relationship if points plotted from $(x, y)$ pairs lie exactly along a single straight line. In practice it is common for two variables to exhibit a relationship that is close to linear but which contains an element, possibly large, of randomness.\n",
    "\n",
    "### Covariance\n",
    "The covariance of two variables $x$ and $y$ is a measure of the linear association between the two variables. The covariance is denoted by $S_{xy}$ and is defined by the following formula:\n",
    "$$ S_{xy} = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{n - 1}$$\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of the $x$ and $y$ values, respectively.\n",
    "\n",
    "If $X_1, X_2, ..., X_n$ are $n$ variables, then the covariance matrix is defined as:\n",
    "   $$ S = \\begin{bmatrix}\n",
    "   S_{11} & S_{12} & \\cdots & S_{1n} \\\\\n",
    "   S_{21} & S_{22} & \\cdots & S_{2n} \\\\\n",
    "   \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "   S_{n1} & S_{n2} & \\cdots & S_{nn}\n",
    "   \\end{bmatrix}$$ \n",
    "where $S_{ij} = S_{ji}$ and $S_{ii} = Var(X_i)$ .\n",
    "\n",
    "$S_{xy} = E[(X - \\mu_x)(Y - \\mu_y)] = E[XY] - \\mu_x \\mu_y$\n",
    "\n",
    "\n",
    "### Linear correlation coefficient\n",
    "The linear correlation coefficient is a number computed directly from the data that measures the strength of the linear relationship between the two variables $x$ and $y$. The linear correlation coefficient is denoted by $r$ and is defined by the following formula:\n",
    "$$ r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2} \\sum{(y_i - \\bar{y})^2}}} = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}}$$\n",
    "where $S_{xy}$, $S_{xx}$, and $S_{yy}$ are the sums of squares defined by\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the sample means of the $x$ and $y$ values, respectively.\n",
    "\n",
    "Properties:\n",
    "1. value of $r$ is always between -1 and 1, inclusive\n",
    "2. sign of $r$ indicates the direction of the linear relationship between $x$ and $y$\n",
    "3. size of $|r|$ indicates the strength of the linear relationship between $x$ and $y$\n",
    "   - $|r|$ close to 1 indicates a strong linear relationship, $|r|$ close to 0 indicates a weak linear relationship\n",
    "\n",
    "Numerical:\n",
    "\n",
    "|$X$|$Y$|$X - \\bar{X}$|$Y - \\bar{Y}$|$(X - \\bar{X})(Y - \\bar{Y})$|\n",
    "|---|---|---|---|---|\n",
    "|...|...|...|...|...|\n",
    "|$\\sum X$|$\\sum Y$| $\\sum (X - \\bar{X})$ | $\\sum (Y - \\bar{Y})$ | $\\sum (X - \\bar{X})(Y - \\bar{Y})$ |\n",
    "\n",
    "### Regression line\n",
    "\n",
    "The regression line is the line that best fits the data. The equation of the regression line is\n",
    "$$ y = a + bx$$\n",
    "where $a$ is the $y$-intercept and $b$ is the slope of the line.\n",
    "\n",
    "For a given set of points, we can find the regression line by minimizing the sum of the squared errors. The sum of the squared errors is defined as\n",
    "$$ SSE = \\sum{(y_i - \\hat{y}_i)^2}$$\n",
    "where $y_i$ is the observed value of $y$ and $\\hat{y}_i$ is the predicted value of $y$.\n",
    "\n",
    "The regression line is the line that minimizes the sum of the squared errors. The slope and $y$-intercept of the regression line are given by the following formulas:\n",
    "$$ b = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum{(x_i - \\bar{x})^2}}$$\n",
    "$$ a = \\bar{y} - b\\bar{x}$$\n",
    "\n",
    "### Coefficient of determination\n",
    "The coefficient of determination is a number computed directly from the data that measures the proportion of the total variation in the $y$ values that is explained by the regression line. The coefficient of determination is denoted by $r^2$ and is defined by the following formula:\n",
    "$$ r^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum{(y_i - \\hat{y}_i)^2}}{\\sum{(y_i - \\bar{y})^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\chi^2$ tests and F-tests\n",
    "\n",
    "## $\\chi^2$ test for independence\n",
    "All the $\\chi^2$ distributions form a family, each specified by a parameter called the number of degrees of freedom. The number of degrees of freedom for a $\\chi^2$ distribution is equal to the number of independent standard normal random variables that are squared and summed to obtain the $\\chi^2$ random variable.\n",
    "\n",
    "<img src=\"https://i.imgur.com/T4Ow1S0.jpg\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "The value of the $\\chi^2$ random variable with $df = k$ that cuts off a right tail with an area of $c$ is denoted by $\\chi^2_c$, and is called a critical value.\n",
    "\n",
    "The $\\chi^2$ test for independence is used to test the null hypothesis that two categorical variables are independent. For example:\n",
    "- $H_0$: Baby gender and baby heart rate (high/low) are independent (two factors are independent)\n",
    "- $H_1$: Baby gender and baby heart rate are not independent (two factors are not independent)\n",
    "\n",
    "Steps : \n",
    "1. Create the contingency table (rows denoting Factor 1, columns denoting Factor 2) and compute the row and column totals\n",
    "2. Compute the expected counts for each cell, $$E_{ij} = \\frac{row\\ total \\times column\\ total}{grand\\ total}$$\n",
    "3. Compute the $\\chi^2$ test statistic, $$\\chi^2 = \\sum{\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}}$$, where $O_{ij}$ is the observed count for cell $(i, j)$ and $E_{ij}$ is the expected count for cell $(i, j)$\n",
    "4. Based on decided $\\alpha$ level and degrees of freedom ($df = (n_{rows} - 1) \\times (n_{columns} - 1)$, find the critical value $\\chi^2_\\alpha$\n",
    "5. Compare the test statistic and critical value, if $\\chi^2 > \\chi^2_\\alpha$, reject $H_0$\n",
    "    - If $H_0$ is rejected, conclude that the two categorical variables are not independent\n",
    "\n",
    "## $\\chi^2$ goodness-of-fit test\n",
    "\n",
    "The $\\chi^2$ goodness-of-fit test is used to test the null hypothesis that a population distribution follows a specified distribution. For example:\n",
    "- $H_0$: The 6-sided die is fair\n",
    "- $H_1$: The 6-sided die is not fair\n",
    "\n",
    "We wish to determine that every face of the die has the same probability of appearing. We roll the die 60 times and record the number of times each face appears. We then compare the observed counts with the expected counts.\n",
    "\n",
    "Steps:\n",
    "1. Create the contingency table (rows denoting each face of the die, columns denoting observed counts and expected counts)\n",
    "2. Compute the expected counts for each face, $$E_{i} = \\frac{\\text{total number of rolls}}{\\text{number of faces}}$$\n",
    "3. Compute the $\\chi^2$ test statistic, $$\\chi^2 = \\sum{\\frac{(O_{i} - E_{i})^2}{E_{i}}}$$, where $O_{i}$ is the observed count for face $i$ and $E_{i}$ is the expected count for face $i$\n",
    "4. Based on decided $\\alpha$ level and degrees of freedom ($df = n_{faces} - 1$, find the critical value $\\chi^2_\\alpha$\n",
    "5. Compare the test statistic and critical value, if $\\chi^2 > \\chi^2_\\alpha$, reject $H_0$\n",
    "    - If $H_0$ is rejected, conclude that the die is not fair\n",
    "\n",
    "## F-test for equality of two variances\n",
    "- Family of F-distributions, each specified by two parameters called the degrees of freedom ($df_1$ and $df_2$), also called the numerator and denominator degrees of freedom. They are not interchangeable.\n",
    "\n",
    "<img src=\"https://i.imgur.com/pfMjAvT.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "The value of the F random variable with $df_1$ and $df_2$ that cuts off a right tail with an area of $c$ is denoted by $F_{df_1, df_2, \\alpha}$, and is called a critical value.\n",
    "Also, $$F_{df_1, df_2, \\alpha} = \\frac{1}{F_{df_2, df_1, 1 - \\alpha}}$$\n",
    "\n",
    "The F-test for equality of two variances is used to test the null hypothesis that two populations have equal variances. For example:\n",
    "- $H_0$: $\\sigma_1^2 = \\sigma_2^2$\n",
    "- Three forms of $H_1$:\n",
    "    - $\\sigma_1^2 \\neq \\sigma_2^2$ (two-tailed test)\n",
    "    - $\\sigma_1^2 > \\sigma_2^2$ (right-tailed test)\n",
    "    - $\\sigma_1^2 < \\sigma_2^2$ (left-tailed test)\n",
    "\n",
    "Steps:\n",
    "1. We take two samples from the two populations of size $n_1$ and $n_2$ and compute the sample variances $s_1^2$ and $s_2^2$\n",
    "    - the samples are independent, and the populations are normally distributed\n",
    "2. Compute the F test statistic, $$F = \\frac{s_1^2}{s_2^2}$$\n",
    "3. Based on decided $\\alpha$ level and degrees of freedom ($df_1 = n_1 - 1$ and $df_2 = n_2 - 1$), find the critical value $F_{df_1, df_2, \\alpha}$\n",
    "4. There are different rejection regions for the three forms of $H_1$:\n",
    "    - $\\sigma_1^2 \\neq \\sigma_2^2$: reject $H_0$ if $F \\le F_{df_1, df_2, 1 - \\alpha/2}$ or $F \\ge F_{df_1, df_2, \\alpha/2}$\n",
    "    - $\\sigma_1^2 > \\sigma_2^2$: reject $H_0$ if $F \\ge F_{df_1, df_2, \\alpha}$\n",
    "    - $\\sigma_1^2 < \\sigma_2^2$: reject $H_0$ if $F \\le F_{df_1, df_2, 1 - \\alpha}$\n",
    "\n",
    "## F-test for equality of means of several populations\n",
    "\n",
    "Given population 1 with sample size $n_1$, sample mean $\\bar{x}_1$ and sample variance $s_1^2$, population 2 with sample size $n_2$, sample mean $\\bar{x}_2$ and sample variance $s_2^2$, and so on, the F-test for equality of means of several populations is used to test the null hypothesis that the means of all populations are equal. \n",
    "We calculate the following quantities:\n",
    "- $n = n_1 + n_2 + \\dots + n_k$\n",
    "- $\\bar{x} = \\frac{n_1\\bar{x}_1 + n_2\\bar{x}_2 + \\dots + n_k\\bar{x}_k}{n}$\n",
    "- Mean square treatment (MST), $MST = \\frac{n_1(\\bar{x}_1 - \\bar{x})^2 + n_2(\\bar{x}_2 - \\bar{x})^2 + \\dots + n_k(\\bar{x}_k - \\bar{x})^2}{k - 1}$\n",
    "- Mean square error (MSE), $MSE = \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2 + \\dots + (n_k - 1)s_k^2}{n - k}$\n",
    "\n",
    "Finally we have the F-test statistic, $$F = \\frac{MST}{MSE}$$\n",
    "If the $k$ populations are normally distributed, then $F$ has an F-distribution with $k - 1$ and $n - k$ degrees of freedom.\n",
    "The test is right tailed, so we reject $H_0$ if $F \\ge F_{k - 1, n - k, \\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA test\n",
    "\n",
    "### One-way ANOVA\n",
    "- ANOVA stands for analysis of variance, and is a generalization of the F-test for equality of means of several populations.\n",
    "- ANOVA is used to test the null hypothesis that the means of $k$ populations are equal.\n",
    "- $k$ is the number of populations, $n$ is the total number of observations, $n_i$ is the number of observations in population $i$, $\\bar{x}_i$ is the sample mean of population $i$, $s_i^2$ is the sample variance of population $i$.\n",
    "- $df_{treatment} = k - 1$, $df_{error} = n - k$, $df_{total} = n - 1$\n",
    "\n",
    "Steps:\n",
    "1. We take $k$ samples from the $k$ populations of size $n_1, n_2, \\dots, n_k$ and compute the sample means $\\bar{x}_1, \\bar{x}_2, \\dots, \\bar{x}_k$\n",
    "    - the samples are independent, and the populations are normally distributed\n",
    "2. Calculate the following quantities:\n",
    "    - $n = n_1 + n_2 + \\dots + n_k$\n",
    "    - $$\\bar{x} = \\frac{n_1\\bar{x}_1 + n_2\\bar{x}_2 + \\dots + n_k\\bar{x}_k}{n}$$\n",
    "3. Calculate the following quantities:\n",
    "    - Sum of Squares Regression (SSR), $$ SSR = n_1(\\bar{x}_1 - \\bar{x})^2 + n_2(\\bar{x}_2 - \\bar{x})^2 + \\dots + n_k(\\bar{x}_k - \\bar{x})^2 $$\n",
    "    - Sum of Squares Error (SSE), $$ SSE = \\sum_{i = 1}^{k} SSE_i $$ where $$ SSE_i = \\sum_{j = 1}^{n_i} (x_{ij} - \\bar{x}_i)^2 $$\n",
    "    - Then we have the Sum of Squares Total (SST), $$ SST = SSR + SSE $$\n",
    "4. Fill in the ANOVA table:\n",
    "\n",
    "| Source of variation | Sum of squares | Degrees of freedom | Mean square | F |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Treatment | $SSR$ | $k - 1$ | $MST = \\frac{SSR}{k - 1}$ | $F = \\frac{MST}{MSE}$ |\n",
    "| Error | $SSE$ | $n - k$ | $MSE = \\frac{SSE}{n - k}$ | |\n",
    "| Total | $SST$ | $n - 1$ | | |\n",
    "\n",
    "5. If the $k$ populations are normally distributed, then $F$ has an F-distribution with $k - 1$ and $n - k$ degrees of freedom. The critical value is $F_{k - 1, n - k, \\alpha}$, and if $F \\ge F_{k - 1, n - k, \\alpha}$, we reject $H_0$.\n",
    "\n",
    "### Two-way ANOVA\n",
    "\n",
    "| | 1 | 2 | ... | $m$ | Total $x_{i.}$ | Total $x_{i.}^2$ | \n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | $x_{11}$ | $x_{12}$ | ... | $x_{1m}$ | $x_{1.}$ | $x_{1.}^2$ |\n",
    "| 2 | $x_{21}$ | $x_{22}$ | ... | $x_{2m}$ | $x_{2.}$ | $x_{2.}^2$ |\n",
    "| ... | ... | ... | ... | ... | ... | ... |\n",
    "| $k$ | $x_{k1}$ | $x_{k2}$ | ... | $x_{km}$ | $x_{k.}$ | $x_{k.}^2$ |\n",
    "| Total $x_{.j}$ | $x_{.1}$ | $x_{.2}$ | ... | $x_{.m}$ | G | |\n",
    "| Total $x_{.j}^2$ | $x_{.1}^2$ | $x_{.2}^2$ | ... | $x_{.m}^2$ | | |\n",
    "\n",
    "- $k$ is the number of treatments, $m$ is the number of blocks, $n$ is the total number of observations.\n",
    "- ANOVA is used to test the null hypothesis that the means of $k$ populations are equal.\n",
    "- Two way ANOVA is used to test the null hypothesis that the means of $k$ populations are equal, but the populations are divided into groups based on a factor.\n",
    "- Two pairs of hypotheses:\n",
    "    - $H_{01}$: There is no significant difference in the means of different groups\n",
    "    - $H_{02}$: There is no significant difference in the means of different blocks\n",
    "    - $H_{11}$: Atleast one pair of means of different groups is significantly different\n",
    "    - $H_{12}$: Atleast one pair of means of different blocks is significantly different\n",
    "\n",
    "Steps:\n",
    "1. Find the correction factor $$ CF = \\frac{(\\sum_{j = 1}^{m} \\sum_{i = 1}^{n} x_{ij})^2}{n} $$\n",
    "2. Find the total sum of squares $$ TSS = \\sum_{j = 1}^{k} \\sum_{i = 1}^{m} x_{ij}^2 - CF $$\n",
    "3. Find the sum of squares for treatments $$ SST = \\sum_{j = 1}^{k} \\frac{x_{j.}^2}{m} - CF $$\n",
    "4. Find the sum of squares for blocks $$ SSB = \\sum_{i = 1}^{m} \\frac{x_{.i}^2}{k} - CF $$\n",
    "5. Find the sum of squares for error $$ SSE = TSS - SST - SSB $$\n",
    "6. Anova table (two-way):\n",
    "\n",
    "| Source of variation | Sum of squares | Degrees of freedom | Mean square | F |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Treatments | $SST$ | $k - 1$ | $MST = \\frac{SST}{k - 1}$ | $F_t = \\frac{MST}{MSE}$ |\n",
    "| Blocks | $SSB$ | $m - 1$ | $MSB = \\frac{SSB}{m - 1}$ | $F_b = \\frac{MSB}{MSE}$ |\n",
    "| Error | $SSE$ | $(k - 1)(m - 1)$ | $MSE = \\frac{SSE}{(k - 1)(m - 1)}$ | |\n",
    "| Total | $TSS$ | $km - 1$ | | |\n",
    "\n",
    "7. The critical value for $F_t$ is $F_{k - 1, (k - 1)(m - 1), \\alpha}$, and the critical value for $F_b$ is $F_{m - 1, (k - 1)(m - 1), \\alpha}$. If $F_t \\ge F_{k - 1, (k - 1)(m - 1), \\alpha}$ or $F_b \\ge F_{m - 1, (k - 1)(m - 1), \\alpha}$, we reject $H_{01}$ or $H_{02}$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "\n",
    "## Time Series Data\n",
    "- a set of observations on a variable measured at successive points in time ($y_0$, $y_1$, $y_2$, ..., $y_n$)\n",
    "- the measurement of the variables may be made continuously or at discrete points in time\n",
    "- often a variable continuous in time is measured at discrete points in time\n",
    "- discrete time series data may be generated from an accumulation of data over a period of time\n",
    "    - monthly sales, daily rainfall, annual production\n",
    "\n",
    "## Time Series Decomposition\n",
    "- a time series may be decomposed into four components\n",
    "    - trend (long term progression of the series, secular variation)\n",
    "        - exists when there is a persistent, long term increase or decrease in the data\n",
    "        - may be linear or nonlinear\n",
    "    - seasonal\n",
    "        - exists when a series is influenced by seasonal factors\n",
    "        - seasonal factors are cyclical and repeat over a fixed period\n",
    "        - seasonal factors are usually multiplicative\n",
    "    - cyclical\n",
    "        - exists when data exhibit rises and falls that are not of fixed period\n",
    "        - cyclical variation is usually due to economic conditions\n",
    "        - usually of at least 2 years duration (longer and more erratic than seasonal)\n",
    "    - irregular\n",
    "        - exists when data are influenced by factors not considered in the analysis\n",
    "        - may be due to unusual events, one time occurrences, or other sources of variation\n",
    "        - also called residual or error\n",
    "- sometimes trend and cyclical are combined into a single component called trend-cycle component\n",
    "- these components are additive or multiplicative\n",
    "    - additive: $y_t = T_t + S_t + C_t + I_t$\n",
    "        - the magnitude of the seasonal variation does not depend on the magnitude of the time series\n",
    "    - multiplicative: $y_t = T_t \\times S_t \\times C_t \\times I_t$\n",
    "        - the magnitude of the seasonal variation depends on the magnitude of the time series\n",
    "    - or a combination of the two\n",
    "\n",
    "## Forecasting\n",
    "- the prediction of future events or a quantity depends on several factors including:\n",
    "    1. how well we understand the factors that contribute to the quantity\n",
    "    2. how much data is available\n",
    "    3. whether the forecasts can affect the thing we are trying to forecast\n",
    "- basic steps in forecasting:\n",
    "    1. problem definition\n",
    "    2. gather information\n",
    "    3. preliminary (exploratory) analysis\n",
    "    4. choose and fit models\n",
    "    5. use models for prediction and evaluate them\n",
    "\n",
    "### Forecasting Time Frames\n",
    "- short term: up to 6 months, or more frequently\n",
    "    - needed for the scheduling of production, inventory, and personnel\n",
    "    - forecasts of demand for individual products are needed for production scheduling\n",
    "- medium term: 6 months to 2 years\n",
    "    - needed for sales and production planning, budgeting, and cost control\n",
    "    - to determine future resource requirements, in order to purchase raw materials and hire personnel, buy machinery and equipment\n",
    "    - forecasts of total demand are needed for sales planning\n",
    "- long term: more than 3 years\n",
    "\n",
    "### Forecasting Methods\n",
    "- Qualitative methods:\n",
    "    1. personal opinion or judgement\n",
    "        - used when there is little or no data available, usually relies on the opinion of experts\n",
    "    2. panel consensus\n",
    "        - a group of experts meet and discuss the forecast and arrive at a consensus\n",
    "    3. Delphi method\n",
    "        - a panel of experts is selected and each is asked to independently provide a forecast and their justification\n",
    "        - the justifications are then shared with the group and each expert is asked to revise their opinion\n",
    "        - the process is repeated until a consensus is reached\n",
    "        - final forecast is got by aggregating the individual expert forecasts\n",
    "    4. market research\n",
    "        - collect forecast beased on well designed objectives and opinions about the future variables\n",
    "        - questionnaires used to gather data and prepare summary reports\n",
    "- Quantitative methods:\n",
    "    1. time series analysis\n",
    "        - smoothing methods\n",
    "        - exponential smoothing\n",
    "        - trend projection methods\n",
    "    2. causal models\n",
    "        - regression analysis\n",
    "        - econometric models\n",
    "- Time series - Trend?\n",
    "    - (Trend = Yes) -> Trend models\n",
    "        - Linear, quadratic, exponential, autoregressive\n",
    "        - explicitly calculate the components of the time series as a basis for forecasting\n",
    "    - (Trend = No) -> Smoothing models\n",
    "        - Moving average, exponential smoothing\n",
    "        - do not explicitly calculate the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Models\n",
    "\n",
    "#### Linear Trend Model\n",
    "\n",
    "- $y_t = a + b t + e_t$\n",
    "- $a$ and $b$ are the intercept and slope of the trend line, $e_t$ is the error term, $t$ is the time period (1, 2, 3, ..., $n$)\n",
    "- the trend line is a straight line that best fits the data\n",
    "- First calculate $x$ based on $t$ so that it is centered around $0$:\n",
    "    - say for $5$ data points [2016, 2017, 2018, 2019, 2020], $x = [-2, -1, 0, 1, 2]$\n",
    "    - for $6$ data points [2015, 2016, 2017, 2018, 2019, 2020], $x = [-5, -3, -1, 1, 3, 5]$\n",
    "- create table with:\n",
    "    - $t$ (time period), $y_t$ (data), $x_t$ (centered time period), $x_t^2$, $x_t y_t$\n",
    "- then calculate $$b = \\frac{\\sum x_t y_t}{\\sum {x_t}^2}$$ $$a = \\frac{\\sum y_t}{n}$$\n",
    "- then forcasted value is $$\\hat{y}_{n+1} = a + b (x_{n+1})$$\n",
    "\n",
    "#### Quadratic Trend Model\n",
    "- $y_t = a + b t + c t^2 + e_t$\n",
    "- We can create 3 equations with 3 unknowns ($a$, $b$, $c$) and solve them to get the values of $a$, $b$, $c$:\n",
    "    - $\\sum y_t = a n + b \\sum x + c \\sum x^2$\n",
    "    - $\\sum x_t y_t = a \\sum x + b \\sum x^2 + c \\sum x^3$\n",
    "    - $\\sum x_t^2 y_t = a \\sum x^2 + b \\sum x^3 + c \\sum x^4$\n",
    "- $x$ is centered around $0$ as in the linear trend model\n",
    "- create table with:\n",
    "    - $t$ (time period), $y_t$ (data), $x_t$ (centered time period), $x_t^2$, $x_t^3$, $x_t^4$, $x_t y_t$, $x_t^2 y_t$\n",
    "- then forcasted value is $$\\hat{y}_{n+1} = a + b (x_{n+1}) + c (x_{n+1})^2$$\n",
    "\n",
    "### Smoothing Models\n",
    "\n",
    "#### Moving Average\n",
    "- appropriate for data with horizontal pattern (stationary data)\n",
    "- $y_t = \\frac{1}{k} \\sum_{i=1}^k y_{t-i}$\n",
    "\n",
    "#### Centered Moving Average\n",
    "- appropriate for data with trend pattern\n",
    "- by default, moving average values are placed at the period in which they are calculated\n",
    "- when you center the moving averages, they are placed at the center of the range rather than the end of it\n",
    "- if $k$ is odd:\n",
    "    - say $k = 3$, then the first numeric moving average value is placed at period $2$, the next at period $3$, and so on\n",
    "    - in this case, the moving average value for the first and last periods is missing\n",
    "- if $k$ is even:\n",
    "    - say $k = 4$, then because you cannot place a moving average value at period $2.5$, calculate the average of the first four values and name it $ma_1$\n",
    "    - then calculate the average of the next four values and name it $ma_2$\n",
    "    - the average of those two values is the number and place at period $3$\n",
    "\n",
    "#### Exponential Smoothing\n",
    "- calculates exponentially smoothed time series $f_t$ from the original time series $y_t$ as follows:\n",
    "    - $f_1 = y_1$\n",
    "    - $f_{t+1} = \\alpha y_t + (1 - \\alpha) f_t$ where $0 < \\alpha < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures\n",
    "1. Mean Absolute Deviation (MAD)\n",
    "    - gives less weight to large errors\n",
    "    $$MAD = \\frac{\\sum_{t=1}^n |y_t - \\hat{y}_t|}{n}$$\n",
    "2. Mean Squared Error (MSE)\n",
    "    - gives more weight to large errors\n",
    "    $$MSE = \\frac{\\sum_{t=1}^n (y_t - \\hat{y}_t)^2}{n}$$\n",
    "3. Mean Absolute Percentage Error (MAPE)\n",
    "    - gives less overall weight to large errors if the time series values are large\n",
    "    $$MAPE = \\frac{\\sum_{t=1}^n \\frac{|y_t - \\hat{y}_t|}{y_t}}{n} \\times 100$$\n",
    "4. Largest Absolute Deviation (LAD)\n",
    "    - tells us that all deviations fall below a certain threshold value\n",
    "    $$LAD = \\max_{1 \\leq t \\leq n} |y_t - \\hat{y}_t|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt Winters Method\n",
    "\n",
    "#### Components form of exponential smoothing\n",
    "1. Forecast equation\n",
    "    - $\\hat{y}_{t+h} = l_t$\n",
    "2. Smoothing equation\n",
    "    - $l_t = \\alpha y_t + (1 - \\alpha) l_{t-1}$ where $l_t$ is the smoothed value of $y_t$ and $h$ is the forecast horizon\n",
    "\n",
    "#### Holt's linear trend method (double exponential smoothing)\n",
    "1. Forecast equation\n",
    "    - $\\hat{y}_{t+h} = l_t + h b_t$\n",
    "2. Level equation\n",
    "    - $l_t = \\alpha y_t + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "3. Trend equation\n",
    "    - $b_t = \\beta^* (l_t - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "    \n",
    "Where $l_t$ denotes an estimate of the level of the series at time $t$, $b_t$ denotes an estimate of the trend (slope) of the series at time $t$, $\\alpha$ and $\\beta^*$ are smoothing parameters, and $0 \\leq \\alpha \\leq 1$ and $0 \\leq \\beta^* \\leq 1$\n",
    "- $\\alpha$ is the level smoothing parameter\n",
    "- $\\beta^*$ is the trend smoothing parameter\n",
    "\n",
    "With yearly data:\n",
    "\n",
    "|Year|#Sold|Level|Trend|Forecast|Error|\n",
    "|---|---|---|---|---|---|\n",
    "|1|$y_1$|$l_1$|$b_1$|$\\hat{f}_1$|$y_1 - \\hat{f}_1$|\n",
    "|2|$y_2$|$l_2$|$b_2$|$\\hat{f}_2$|$y_2 - \\hat{f}_2$|\n",
    "|...|...|...|...|...|...|\n",
    "|10|$y_{10}$|$l_{10}$|$b_{10}$|$\\hat{f}_{10}$|$y_{10} - \\hat{f}_{10}$|\n",
    "\n",
    "First calculate $l_1$ by setting $l_1 = y_1$ and $b_1 = 0$\n",
    "\n",
    "Then calculate $l_2$, $b_2$, $\\hat{f}_{2}$ onwards using the following formula:\n",
    "- $l_t = \\alpha y_t + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "- $b_t = \\beta^* (l_t - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "- $\\hat{f}_{t+1} = l_t + b_t$\n",
    "\n",
    "Finally, to predict into the future, use the following formula:\n",
    "- $\\hat{f}_{t+k} = l_t + k b_t$\n",
    "\n",
    "\n",
    "#### Holt-Winters seasonal method (triple exponential smoothing) [Link](https://youtu.be/4_ciGzvrQl8)\n",
    "1. Forecast equation\n",
    "    - $\\hat{y}_{t+h} = l_t + h b_t + s_{t+h-m(k+1)}$\n",
    "2. Level equation\n",
    "    - $l_t = \\alpha (y_t - s_{t-m}) + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "3. Trend equation\n",
    "    - $b_t = \\beta^* (l_t - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "4. Seasonal equation\n",
    "    - $s_t = \\gamma (y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma) s_{t-m}$\n",
    "\n",
    "where $k$ is the integer part of $\\frac{h-1}{m}$, $m$ is the number of seasons in a year and $\\gamma$ is the seasonal smoothing parameter\n",
    "\n",
    "With monthly data, $m = 12$ : \n",
    "\n",
    "|Index|Month|#Sold|Level|Trend|Seasonal|Forecast|Error|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|1|Jan|$y_1$|$l_1$|$b_1$|$s_1$|$\\hat{f}_1$|$y_1 - \\hat{f}_1$|\n",
    "|2|Feb|$y_2$|$l_2$|$b_2$|$s_2$|$\\hat{f}_2$|$y_2 - \\hat{f}_2$|\n",
    "|...|...|...|...|...|...|...|...|\n",
    "|12|Dec|$y_{12}$|$l_{12}$|$b_{12}$|$s_{12}$|$\\hat{f}_{12}$|$y_{12} - \\hat{f}_{12}$|\n",
    "|13|Jan|$y_{13}$|$l_{13}$|$b_{13}$|$s_{13}$|$\\hat{f}_{13}$|$y_{13} - \\hat{f}_{13}$|\n",
    "|14|Feb|$y_{14}$|$l_{14}$|$b_{14}$|$s_{14}$|$\\hat{f}_{14}$|$y_{14} - \\hat{f}_{14}$|\n",
    "\n",
    "First calculate $s_1$ to $s_{12}$ using the following formula:\n",
    "- $s_t = \\frac{1}{12} \\frac{y_t}{\\sum_{k=1}^{12} y_k}$\n",
    "\n",
    "Then calculate $l_{13}$, $b_{13}$ using the following formula:\n",
    "- $l_{13} = \\frac{y_{13}}{s_1}$\n",
    "- $b_{13} = \\frac{y_{13}}{s_1} - \\frac{y_{12}}{s_12}$\n",
    "\n",
    "Then calculate $s_{13}$ using the following formula:\n",
    "- $s_{13} = \\gamma \\frac{y_{13}}{l_{13}} + (1 - \\gamma) s_{(13 - 12)}$\n",
    "\n",
    "Then calculate $l_{14}$, $b_{14}$, $s_{14}$, $\\hat{f}_{14}$ onwards using the following formula: (forecast within the data)\n",
    "- $l_{t} = \\alpha (y_t - s_{t-m}) + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "- $b_{t} = \\beta^* (l_{t} - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "- $s_{t} = \\gamma \\frac{y_{t}}{l_{t}} + (1 - \\gamma) s_{(t-m)}$\n",
    "- $\\hat{f}_{t+1} = (l_t + b_t) s_{t-m+1}$\n",
    "\n",
    "Finally, to predict into the future, use the following formula: (forecast beyond the last data point)\n",
    "- $\\hat{f}_{t+k} = (l_t + k b_t) s_{t-m+k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary Stochastic Process in Time Series\n",
    "\n",
    "A stochastic process is a collection of random variables indexed by time. A time series is a realization of a stochastic process. A time series is said to be stationary if its statistical properties do not change over time. In particular, its mean, variance, autocorrelation remain constant over time. A stationary series has no trend, its variations around its mean have a constant amplitude, and it wiggles in a consistent fashion, i.e., its short-term random time patterns always look the same in a statistical sense.\n",
    "\n",
    "#### Autocoavariance Function (ACVF)\n",
    "- the autocovariance function (ACVF) of a stationary time series $y_t$ is defined as\n",
    "    - $\\gamma(h) = Cov(y_t, y_{t+h}) = E[(y_t - \\bar{y})(y_{t+h} - \\bar{y})] = \\frac{1}{n} \\sum_{t=1}^{n-h} (y_t - \\bar{y})(y_{t+h} - \\bar{y})$\n",
    "\n",
    "#### Autocorrelation Function (ACF)\n",
    "- the autocorrelation function (ACF) of a stationary time series $y_t$ is defined as\n",
    "    - $\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\frac{Cov(y_t, y_{t+h})}{Var(y_t)}$\n",
    "\n",
    "Auto correlation is the correlation of a time series with the same time series lagged by $k$ time units. It is a measure of how well the present value of a time series is related with its past values. If auto correlation is high, it means that the present value is well correlated with the immediate past value. The value of auto correlation coefficient can range from $-1$ to $1$.\n",
    "$$ r_h = \\rho(h) = \\frac{\\sum_{t=1}^{n-h} (y_t - \\bar{y})(y_{t+h} - \\bar{y})}{\\sum_{t=1}^{n} (y_t - \\bar{y})^2}$$\n",
    "- $r_1$ measures the correlation between $y_t$ and $y_{t-1}$\n",
    "- $r_2$ measures the correlation between $y_t$ and $y_{t-2}$ and so on\n",
    "- autocorrelation for small lags tends to be large and positive because observations nearby in time are also nearby in size\n",
    "- autocorrelation will be larger for smaller seasonal lags\n",
    "- time series that show no autocorrelation are called white noise (i.i.d. random variables with zero mean and constant variance)\n",
    "    - for white noise, we expect $95\\%$ of the sample autocorrelations to lie in the interval $(-2/\\sqrt{n}, 2/\\sqrt{n})$ where $n$ is the sample size\n",
    "\n",
    "#### Partial Autocorrelation Function (PACF)\n",
    "- the partial autocorrelation function (PACF) of a stationary time series $y_t$ is defined as $$\\phi_{hh} = \\rho(h) = \\frac{Cov(y_t, y_{t+h} | y_{t+1}, y_{t+2}, ..., y_{t+h-1})}{\\sqrt{Var(y_t | y_{t+1}, y_{t+2}, ..., y_{t+h-1}) Var(y_{t+h} | y_{t+1}, y_{t+2}, ..., y_{t+h-1})}}$$\n",
    "\n",
    "#### ACF (Auto-Correlation Function) Plot:\n",
    "- shows the correlation between a series and its lagged values\n",
    "- helps in identifying the presence of autocorrelation in the data, which is a measure of how each data point in the series is related to its previous values\n",
    "- significant autocorrelation at a particular lag indicates that past values of the series are useful in predicting future values\n",
    "\n",
    "#### PACF (Partial Auto-Correlation Function) Plot:\n",
    "- shows the correlation between a series and its lagged values after removing the contributions of the intermediate lags.\n",
    "- helps in identifying the direct and isolated relationships between the current value and its past values, excluding the influence of other lags.\n",
    "- helps in determining the order of an autoregressive (AR) model. If there is a significant spike at a specific lag, it suggests that this lag is a potential candidate for inclusion in the AR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Regressive (AR) Model\n",
    "- an autoregressive (AR) model is when the value of a variable in one period is related to its values in previous periods\n",
    "- an AR model of order $p$ is denoted by $AR(p)$\n",
    "- $AR(1)$ model: $y_t = c + \\phi_1 y_{t-1} + e_t$\n",
    "- $AR(p)$ model: $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p$ are parameters of the model\n",
    "- this is like a multiple regression model with lagged values of $y_t$ as predictors\n",
    "- each partial correlation coefficient can be estimated as the last coefficient in an AR model\n",
    "    - specifically, $\\alpha_{k}$ the partial autocorrelation coefficient at lag $k$ is the estimate of $\\phi_k$ in an $AR(k)$ model\n",
    "\n",
    "#### Moving Average (MA) Model\n",
    "- a moving average (MA) model is when the value of a variable in one period is related to the error term in the previous period\n",
    "- an MA model of order $q$ is denoted by $MA(q)$\n",
    "- $MA(1)$ model: $y_t = c + e_t + \\theta_1 e_{t-1}$\n",
    "- $MA(q)$ model: $y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q}$ where $e_t$ is white noise and $\\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model\n",
    "\n",
    "#### ARMA Model\n",
    "- an autoregressive moving average (ARMA) model is a combination of autoregressive and moving average models\n",
    "- an ARMA model of order $(p, q)$ is denoted by $ARMA(p, q)$\n",
    "- $ARMA(1, 1)$ model: $y_t = c + \\phi_1 y_{t-1} + e_t + \\theta_1 e_{t-1}$\n",
    "- $ARMA(p, q)$ model: $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q}$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model\n",
    "\n",
    "#### ARIMA Model \n",
    "- an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model that includes an additional integrated component\n",
    "- the integrated component of an ARIMA model is the differencing of raw observations to allow for the time series to become stationary\n",
    "- an ARIMA model is characterized by 3 terms: $p$, $d$, $q$ where\n",
    "    - $p$ is the order of the autoregressive model (AR)\n",
    "    - $d$ is the degree of differencing (the number of times the data have had past values subtracted)\n",
    "    - $q$ is the order of the moving average model (MA)\n",
    "- $ARIMA(p, d, q)$ model: $$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q} $$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model and $y'_t$ is the differenced series\n",
    "\n",
    "#### Seasonal ARIMA (SARIMA) Model\n",
    "- a seasonal ARIMA (SARIMA) model is an extension of the ARIMA model that explicitly supports univariate time series data with a seasonal component\n",
    "- it has additional hyperparameters to specify the autoregression (AR), differencing (I), and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality\n",
    "- $\\text{SARIMA}(p, d, q)(P, D, Q)_m$ model: $$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q} + \\phi_1 y'_{t-m} + \\phi_2 y'_{t-2m} + ... + \\phi_P y'_{t-Pm} + e_t + \\theta_1 e_{t-m} + \\theta_2 e_{t-2m} + ... + \\theta_Q e_{t-Qm} $$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model and $y'_t$ is the differenced series\n",
    "\n",
    "#### Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) Model\n",
    "- a seasonal autoregressive integrated moving-average with exogenous regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables\n",
    "- it adds to the SARIMA model a linear regression model that is used to model the exogenous variables\n",
    "- $\\text{SARIMAX}(p, d, q)(P, D, Q)_m$ model: $$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q} + \\phi_1 y'_{t-m} + \\phi_2 y'_{t-2m} + ... + \\phi_P y'_{t-Pm} + e_t + \\theta_1 e_{t-m} + \\theta_2 e_{t-2m} + ... + \\theta_Q e_{t-Qm} + \\beta_1 x_{1t} + \\beta_2 x_{2t} + ... + \\beta_k x_{kt} $$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model and $y'_t$ is the differenced series and $x_{1t}, x_{2t}, ..., x_{kt}$ are the exogenous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Estimation of the parameters of a model is often done by maximum likelihood estimation (MLE). The likelihood function is defined as the probability of the observed data as a function of the parameters of the model. The maximum likelihood estimate of the parameters is the value of the parameters that maximize the likelihood function.\n",
    "Likelikood function for a time series model is the joint probability distribution of the observed data. The likelihood function is maximized with respect to the parameters of the model to obtain the maximum likelihood estimates of the parameters.\n",
    "\n",
    "Suppose we have random variables $X_1, X_2, ..., X_n$ that are independent and identically distributed (i.i.d.) with probability density function $f(x; \\theta)$ where $\\theta$ is the parameter of the distribution. The likelihood function is defined as $$L(\\theta) = \\prod_{i=1}^n f(x_i; \\theta)$$ The maximum likelihood estimate of $\\theta$ is the value of $\\theta$ that maximizes the likelihood function $L(\\theta)$.\n",
    "\n",
    "The maximum likelihood estimate of $\\theta$, $$\\hat{\\theta} = \\arg \\max_{\\theta} L(\\theta) = \\arg \\max_{\\theta} \\log L(\\theta)$$\n",
    "For maximization, we have $\\frac{\\partial \\log L(\\theta)}{\\partial \\theta} = 0$ and $\\frac{\\partial^2 \\log L(\\theta)}{\\partial \\theta^2} < 0$\n",
    "\n",
    "#### For binomial distribution\n",
    "- the likelihood function is $$L(p) = \\prod_{i=1}^N {}^n C_{x_i} p^{x_i} (1 - p)^{n - x_i}$$\n",
    "- the maximum likelihood estimate of $p$ is $$\\hat{p} = \\frac{\\sum_{i=1}^N x_i}{n N}$$ where $n$ is the number of trials and $N$ is the number of experiments\n",
    "\n",
    "#### For poisson distribution\n",
    "- the likelihood function is $$L(\\lambda) = \\prod_{i=1}^N \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!}$$\n",
    "- the maximum likelihood estimate of $\\lambda$ is $$\\hat{\\lambda} = \\frac{\\sum_{i=1}^N x_i}{N}$$\n",
    "\n",
    "#### For Normal distribution\n",
    "- the likelihood function is $$L(\\mu, \\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i - \\mu)^2}{2 \\sigma^2}}$$\n",
    "- the maximum likelihood estimate is $$\\hat{\\mu} = \\frac{\\sum_{i=1}^N x_i}{N}, \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^N (x_i - \\hat{\\mu})^2}{N}$$\n",
    "\n",
    "#### For Uniform distribution\n",
    "- the likelihood function is $$L(a, b) = \\prod_{i=1}^N \\frac{1}{b - a}$$\n",
    "- the maximum likelihood estimate is $$\\hat{a} = \\min_{i=1}^N x_i, \\hat{b} = \\max_{i=1}^N x_i$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
