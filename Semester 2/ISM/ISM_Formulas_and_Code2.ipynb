{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Statistical Methods Formulas (Part 2, Time Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series\n",
    "\n",
    "## Time Series Data\n",
    "- a set of observations on a variable measured at successive points in time ($y_0$, $y_1$, $y_2$, ..., $y_n$)\n",
    "- the measurement of the variables may be made continuously or at discrete points in time\n",
    "- often a variable continuous in time is measured at discrete points in time\n",
    "- discrete time series data may be generated from an accumulation of data over a period of time\n",
    "    - monthly sales, daily rainfall, annual production\n",
    "\n",
    "## Time Series Decomposition\n",
    "- a time series may be decomposed into four components\n",
    "    - trend (long term progression of the series, secular variation)\n",
    "        - exists when there is a persistent, long term increase or decrease in the data\n",
    "        - may be linear or nonlinear\n",
    "    - seasonal\n",
    "        - exists when a series is influenced by seasonal factors\n",
    "        - seasonal factors are cyclical and repeat over a fixed period\n",
    "        - seasonal factors are usually multiplicative\n",
    "    - cyclical\n",
    "        - exists when data exhibit rises and falls that are not of fixed period\n",
    "        - cyclical variation is usually due to economic conditions\n",
    "        - usually of at least 2 years duration (longer and more erratic than seasonal)\n",
    "    - irregular\n",
    "        - exists when data are influenced by factors not considered in the analysis\n",
    "        - may be due to unusual events, one time occurrences, or other sources of variation\n",
    "        - also called residual or error\n",
    "- sometimes trend and cyclical are combined into a single component called trend-cycle component\n",
    "- these components are additive or multiplicative\n",
    "    - additive: $y_t = T_t + S_t + C_t + I_t$\n",
    "        - the magnitude of the seasonal variation does not depend on the magnitude of the time series\n",
    "    - multiplicative: $y_t = T_t \\times S_t \\times C_t \\times I_t$\n",
    "        - the magnitude of the seasonal variation depends on the magnitude of the time series\n",
    "    - or a combination of the two\n",
    "\n",
    "## Forecasting\n",
    "- the prediction of future events or a quantity depends on several factors including:\n",
    "    1. how well we understand the factors that contribute to the quantity\n",
    "    2. how much data is available\n",
    "    3. whether the forecasts can affect the thing we are trying to forecast\n",
    "- basic steps in forecasting:\n",
    "    1. problem definition\n",
    "    2. gather information\n",
    "    3. preliminary (exploratory) analysis\n",
    "    4. choose and fit models\n",
    "    5. use models for prediction and evaluate them\n",
    "\n",
    "### Forecasting Time Frames\n",
    "- short term: up to 6 months, or more frequently\n",
    "    - needed for the scheduling of production, inventory, and personnel\n",
    "    - forecasts of demand for individual products are needed for production scheduling\n",
    "- medium term: 6 months to 2 years\n",
    "    - needed for sales and production planning, budgeting, and cost control\n",
    "    - to determine future resource requirements, in order to purchase raw materials and hire personnel, buy machinery and equipment\n",
    "    - forecasts of total demand are needed for sales planning\n",
    "- long term: more than 3 years\n",
    "\n",
    "### Forecasting Methods\n",
    "- Qualitative methods:\n",
    "    1. personal opinion or judgement\n",
    "        - used when there is little or no data available, usually relies on the opinion of experts\n",
    "    2. panel consensus\n",
    "        - a group of experts meet and discuss the forecast and arrive at a consensus\n",
    "    3. Delphi method\n",
    "        - a panel of experts is selected and each is asked to independently provide a forecast and their justification\n",
    "        - the justifications are then shared with the group and each expert is asked to revise their opinion\n",
    "        - the process is repeated until a consensus is reached\n",
    "        - final forecast is got by aggregating the individual expert forecasts\n",
    "    4. market research\n",
    "        - collect forecast beased on well designed objectives and opinions about the future variables\n",
    "        - questionnaires used to gather data and prepare summary reports\n",
    "- Quantitative methods:\n",
    "    1. time series analysis\n",
    "        - smoothing methods\n",
    "        - exponential smoothing\n",
    "        - trend projection methods\n",
    "    2. causal models\n",
    "        - regression analysis\n",
    "        - econometric models\n",
    "- Time series - Trend?\n",
    "    - (Trend = Yes) -> Trend models\n",
    "        - Linear, quadratic, exponential, autoregressive\n",
    "        - explicitly calculate the components of the time series as a basis for forecasting\n",
    "    - (Trend = No) -> Smoothing models\n",
    "        - Moving average, exponential smoothing\n",
    "        - do not explicitly calculate the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend Models\n",
    "\n",
    "#### Linear Trend Model\n",
    "\n",
    "- $y_t = a + b t + e_t$\n",
    "- $a$ and $b$ are the intercept and slope of the trend line, $e_t$ is the error term, $t$ is the time period (1, 2, 3, ..., $n$)\n",
    "- the trend line is a straight line that best fits the data\n",
    "- First calculate $x$ based on $t$ so that it is centered around $0$:\n",
    "    - say for $5$ data points [2016, 2017, 2018, 2019, 2020], $x = [-2, -1, 0, 1, 2]$\n",
    "    - for $6$ data points [2015, 2016, 2017, 2018, 2019, 2020], $x = [-5, -3, -1, 1, 3, 5]$\n",
    "- create table with:\n",
    "    - $t$ (time period), $y_t$ (data), $x_t$ (centered time period), $x_t^2$, $x_t y_t$\n",
    "- then calculate $$b = \\frac{\\sum x_t y_t}{\\sum {x_t}^2}$$ $$a = \\frac{\\sum y_t}{n}$$\n",
    "- then forcasted value is $$\\hat{y}_{n+1} = a + b (x_{n+1})$$\n",
    "\n",
    "#### Quadratic Trend Model\n",
    "- $y_t = a + b t + c t^2 + e_t$\n",
    "- We can create 3 equations with 3 unknowns ($a$, $b$, $c$) and solve them to get the values of $a$, $b$, $c$:\n",
    "    - $\\sum y_t = a n + b \\sum x + c \\sum x^2$\n",
    "    - $\\sum x_t y_t = a \\sum x + b \\sum x^2 + c \\sum x^3$\n",
    "    - $\\sum x_t^2 y_t = a \\sum x^2 + b \\sum x^3 + c \\sum x^4$\n",
    "- $x$ is centered around $0$ as in the linear trend model\n",
    "- create table with:\n",
    "    - $t$ (time period), $y_t$ (data), $x_t$ (centered time period), $x_t^2$, $x_t^3$, $x_t^4$, $x_t y_t$, $x_t^2 y_t$\n",
    "- then forcasted value is $$\\hat{y}_{n+1} = a + b (x_{n+1}) + c (x_{n+1})^2$$\n",
    "\n",
    "### Smoothing Models\n",
    "\n",
    "#### Moving Average\n",
    "- appropriate for data with horizontal pattern (stationary data)\n",
    "- $y_t = \\frac{1}{k} \\sum_{i=1}^k y_{t-i}$\n",
    "\n",
    "#### Centered Moving Average\n",
    "- appropriate for data with trend pattern\n",
    "- by default, moving average values are placed at the period in which they are calculated\n",
    "- when you center the moving averages, they are placed at the center of the range rather than the end of it\n",
    "- if $k$ is odd:\n",
    "    - say $k = 3$, then the first numeric moving average value is placed at period $2$, the next at period $3$, and so on\n",
    "    - in this case, the moving average value for the first and last periods is missing\n",
    "- if $k$ is even:\n",
    "    - say $k = 4$, then because you cannot place a moving average value at period $2.5$, calculate the average of the first four values and name it $ma_1$\n",
    "    - then calculate the average of the next four values and name it $ma_2$\n",
    "    - the average of those two values is the number and place at period $3$\n",
    "\n",
    "#### Exponential Smoothing\n",
    "- calculates exponentially smoothed time series $f_t$ from the original time series $y_t$ as follows:\n",
    "    - $f_1 = y_1$\n",
    "    - $f_{t+1} = \\alpha y_t + (1 - \\alpha) f_t$ where $0 < \\alpha < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures\n",
    "1. Mean Absolute Deviation (MAD)\n",
    "    - gives less weight to large errors\n",
    "    $$MAD = \\frac{\\sum_{t=1}^n |y_t - \\hat{y}_t|}{n}$$\n",
    "2. Mean Squared Error (MSE)\n",
    "    - gives more weight to large errors\n",
    "    $$MSE = \\frac{\\sum_{t=1}^n (y_t - \\hat{y}_t)^2}{n}$$\n",
    "3. Mean Absolute Percentage Error (MAPE)\n",
    "    - gives less overall weight to large errors if the time series values are large\n",
    "    $$MAPE = \\frac{\\sum_{t=1}^n \\frac{|y_t - \\hat{y}_t|}{y_t}}{n} \\times 100$$\n",
    "4. Largest Absolute Deviation (LAD)\n",
    "    - tells us that all deviations fall below a certain threshold value\n",
    "    $$LAD = \\max_{1 \\leq t \\leq n} |y_t - \\hat{y}_t|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt Winters Method\n",
    "\n",
    "#### Components form of exponential smoothing\n",
    "1. Forecast equation\n",
    "    - $\\hat{y}_{t+h} = l_t$\n",
    "2. Smoothing equation\n",
    "    - $l_t = \\alpha y_t + (1 - \\alpha) l_{t-1}$ where $l_t$ is the smoothed value of $y_t$ and $h$ is the forecast horizon\n",
    "\n",
    "#### Holt's linear trend method (double exponential smoothing)\n",
    "1. Forecast equation\n",
    "    - $\\hat{y}_{t+h} = l_t + h b_t$\n",
    "2. Level equation\n",
    "    - $l_t = \\alpha y_t + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "3. Trend equation\n",
    "    - $b_t = \\beta^* (l_t - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "    \n",
    "Where $l_t$ denotes an estimate of the level of the series at time $t$, $b_t$ denotes an estimate of the trend (slope) of the series at time $t$, $\\alpha$ and $\\beta^*$ are smoothing parameters, and $0 \\leq \\alpha \\leq 1$ and $0 \\leq \\beta^* \\leq 1$\n",
    "- $\\alpha$ is the level smoothing parameter\n",
    "- $\\beta^*$ is the trend smoothing parameter\n",
    "\n",
    "With yearly data:\n",
    "\n",
    "|Year|#Sold|Level|Trend|Forecast|Error|\n",
    "|---|---|---|---|---|---|\n",
    "|1|$y_1$|$l_1$|$b_1$|$\\hat{f}_1$|$y_1 - \\hat{f}_1$|\n",
    "|2|$y_2$|$l_2$|$b_2$|$\\hat{f}_2$|$y_2 - \\hat{f}_2$|\n",
    "|...|...|...|...|...|...|\n",
    "|10|$y_{10}$|$l_{10}$|$b_{10}$|$\\hat{f}_{10}$|$y_{10} - \\hat{f}_{10}$|\n",
    "\n",
    "First calculate $l_1$ by setting $l_1 = y_1$ and $b_1 = 0$\n",
    "\n",
    "Then calculate $l_2$, $b_2$, $\\hat{f}_{2}$ onwards using the following formula:\n",
    "- $l_t = \\alpha y_t + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "- $b_t = \\beta^* (l_t - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "- $\\hat{f}_{t+1} = l_t + b_t$\n",
    "\n",
    "Finally, to predict into the future, use the following formula:\n",
    "- $\\hat{f}_{t+k} = l_t + k b_t$\n",
    "\n",
    "\n",
    "#### Holt-Winters seasonal method (triple exponential smoothing) [Link](https://youtu.be/4_ciGzvrQl8)\n",
    "1. Forecast equation\n",
    "    - $\\hat{y}_{t+h} = l_t + h b_t + s_{t+h-m(k+1)}$\n",
    "2. Level equation\n",
    "    - $l_t = \\alpha (y_t - s_{t-m}) + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "3. Trend equation\n",
    "    - $b_t = \\beta^* (l_t - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "4. Seasonal equation\n",
    "    - $s_t = \\gamma (y_t - l_{t-1} - b_{t-1}) + (1 - \\gamma) s_{t-m}$\n",
    "\n",
    "where $k$ is the integer part of $\\frac{h-1}{m}$, $m$ is the number of seasons in a year and $\\gamma$ is the seasonal smoothing parameter\n",
    "\n",
    "With monthly data, $m = 12$ : \n",
    "\n",
    "|Index|Month|#Sold|Level|Trend|Seasonal|Forecast|Error|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|1|Jan|$y_1$|$l_1$|$b_1$|$s_1$|$\\hat{f}_1$|$y_1 - \\hat{f}_1$|\n",
    "|2|Feb|$y_2$|$l_2$|$b_2$|$s_2$|$\\hat{f}_2$|$y_2 - \\hat{f}_2$|\n",
    "|...|...|...|...|...|...|...|...|\n",
    "|12|Dec|$y_{12}$|$l_{12}$|$b_{12}$|$s_{12}$|$\\hat{f}_{12}$|$y_{12} - \\hat{f}_{12}$|\n",
    "|13|Jan|$y_{13}$|$l_{13}$|$b_{13}$|$s_{13}$|$\\hat{f}_{13}$|$y_{13} - \\hat{f}_{13}$|\n",
    "|14|Feb|$y_{14}$|$l_{14}$|$b_{14}$|$s_{14}$|$\\hat{f}_{14}$|$y_{14} - \\hat{f}_{14}$|\n",
    "\n",
    "First calculate $s_1$ to $s_{12}$ using the following formula:\n",
    "- $s_t = \\frac{1}{12} \\frac{y_t}{\\sum_{k=1}^{12} y_k}$\n",
    "\n",
    "Then calculate $l_{13}$, $b_{13}$ using the following formula:\n",
    "- $l_{13} = \\frac{y_{13}}{s_1}$\n",
    "- $b_{13} = \\frac{y_{13}}{s_1} - \\frac{y_{12}}{s_12}$\n",
    "\n",
    "Then calculate $s_{13}$ using the following formula:\n",
    "- $s_{13} = \\gamma \\frac{y_{13}}{l_{13}} + (1 - \\gamma) s_{(13 - 12)}$\n",
    "\n",
    "Then calculate $l_{14}$, $b_{14}$, $s_{14}$, $\\hat{f}_{14}$ onwards using the following formula: (forecast within the data)\n",
    "- $l_{t} = \\alpha (y_t - s_{t-m}) + (1 - \\alpha) (l_{t-1} + b_{t-1})$\n",
    "- $b_{t} = \\beta^* (l_{t} - l_{t-1}) + (1 - \\beta^*) b_{t-1}$\n",
    "- $s_{t} = \\gamma \\frac{y_{t}}{l_{t}} + (1 - \\gamma) s_{(t-m)}$\n",
    "- $\\hat{f}_{t+1} = (l_t + b_t) s_{t-m+1}$\n",
    "\n",
    "Finally, to predict into the future, use the following formula: (forecast beyond the last data point)\n",
    "- $\\hat{f}_{t+k} = (l_t + k b_t) s_{t-m+k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary Stochastic Process in Time Series\n",
    "\n",
    "A stochastic process is a collection of random variables indexed by time. A time series is a realization of a stochastic process. A time series is said to be stationary if its statistical properties do not change over time. In particular, its mean, variance, autocorrelation remain constant over time. A stationary series has no trend, its variations around its mean have a constant amplitude, and it wiggles in a consistent fashion, i.e., its short-term random time patterns always look the same in a statistical sense.\n",
    "\n",
    "#### Autocoavariance Function (ACVF)\n",
    "- the autocovariance function (ACVF) of a stationary time series $y_t$ is defined as\n",
    "    - $\\gamma(h) = Cov(y_t, y_{t+h}) = E[(y_t - \\bar{y})(y_{t+h} - \\bar{y})] = \\frac{1}{n} \\sum_{t=1}^{n-h} (y_t - \\bar{y})(y_{t+h} - \\bar{y})$\n",
    "\n",
    "#### Autocorrelation Function (ACF)\n",
    "- the autocorrelation function (ACF) of a stationary time series $y_t$ is defined as\n",
    "    - $\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\frac{Cov(y_t, y_{t+h})}{Var(y_t)}$\n",
    "\n",
    "Auto correlation is the correlation of a time series with the same time series lagged by $k$ time units. It is a measure of how well the present value of a time series is related with its past values. If auto correlation is high, it means that the present value is well correlated with the immediate past value. The value of auto correlation coefficient can range from $-1$ to $1$.\n",
    "$$ r_h = \\rho(h) = \\frac{\\sum_{t=1}^{n-h} (y_t - \\bar{y})(y_{t+h} - \\bar{y})}{\\sum_{t=1}^{n} (y_t - \\bar{y})^2}$$\n",
    "- $r_1$ measures the correlation between $y_t$ and $y_{t-1}$\n",
    "- $r_2$ measures the correlation between $y_t$ and $y_{t-2}$ and so on\n",
    "- autocorrelation for small lags tends to be large and positive because observations nearby in time are also nearby in size\n",
    "- autocorrelation will be larger for smaller seasonal lags\n",
    "- time series that show no autocorrelation are called white noise (i.i.d. random variables with zero mean and constant variance)\n",
    "    - for white noise, we expect $95\\%$ of the sample autocorrelations to lie in the interval $(-2/\\sqrt{n}, 2/\\sqrt{n})$ where $n$ is the sample size\n",
    "\n",
    "#### Partial Autocorrelation Function (PACF)\n",
    "- the partial autocorrelation function (PACF) of a stationary time series $y_t$ is defined as $$\\phi_{hh} = \\rho(h) = \\frac{Cov(y_t, y_{t+h} | y_{t+1}, y_{t+2}, ..., y_{t+h-1})}{\\sqrt{Var(y_t | y_{t+1}, y_{t+2}, ..., y_{t+h-1}) Var(y_{t+h} | y_{t+1}, y_{t+2}, ..., y_{t+h-1})}}$$\n",
    "\n",
    "#### ACF (Auto-Correlation Function) Plot:\n",
    "- shows the correlation between a series and its lagged values\n",
    "- helps in identifying the presence of autocorrelation in the data, which is a measure of how each data point in the series is related to its previous values\n",
    "- significant autocorrelation at a particular lag indicates that past values of the series are useful in predicting future values\n",
    "\n",
    "#### PACF (Partial Auto-Correlation Function) Plot:\n",
    "- shows the correlation between a series and its lagged values after removing the contributions of the intermediate lags.\n",
    "- helps in identifying the direct and isolated relationships between the current value and its past values, excluding the influence of other lags.\n",
    "- helps in determining the order of an autoregressive (AR) model. If there is a significant spike at a specific lag, it suggests that this lag is a potential candidate for inclusion in the AR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto Regressive (AR) Model\n",
    "- an autoregressive (AR) model is when the value of a variable in one period is related to its values in previous periods\n",
    "- an AR model of order $p$ is denoted by $AR(p)$\n",
    "- $AR(1)$ model: $y_t = c + \\phi_1 y_{t-1} + e_t$\n",
    "- $AR(p)$ model: $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p$ are parameters of the model\n",
    "- this is like a multiple regression model with lagged values of $y_t$ as predictors\n",
    "- each partial correlation coefficient can be estimated as the last coefficient in an AR model\n",
    "    - specifically, $\\alpha_{k}$ the partial autocorrelation coefficient at lag $k$ is the estimate of $\\phi_k$ in an $AR(k)$ model\n",
    "\n",
    "#### Moving Average (MA) Model\n",
    "- a moving average (MA) model is when the value of a variable in one period is related to the error term in the previous period\n",
    "- an MA model of order $q$ is denoted by $MA(q)$\n",
    "- $MA(1)$ model: $y_t = c + e_t + \\theta_1 e_{t-1}$\n",
    "- $MA(q)$ model: $y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q}$ where $e_t$ is white noise and $\\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model\n",
    "\n",
    "#### ARMA Model\n",
    "- an autoregressive moving average (ARMA) model is a combination of autoregressive and moving average models\n",
    "- an ARMA model of order $(p, q)$ is denoted by $ARMA(p, q)$\n",
    "- $ARMA(1, 1)$ model: $y_t = c + \\phi_1 y_{t-1} + e_t + \\theta_1 e_{t-1}$\n",
    "- $ARMA(p, q)$ model: $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q}$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model\n",
    "\n",
    "#### ARIMA Model \n",
    "- an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model that includes an additional integrated component\n",
    "- the integrated component of an ARIMA model is the differencing of raw observations to allow for the time series to become stationary\n",
    "- an ARIMA model is characterized by 3 terms: $p$, $d$, $q$ where\n",
    "    - $p$ is the order of the autoregressive model (AR)\n",
    "    - $d$ is the degree of differencing (the number of times the data have had past values subtracted)\n",
    "    - $q$ is the order of the moving average model (MA)\n",
    "- $ARIMA(p, d, q)$ model: $$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q} $$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model and $y'_t$ is the differenced series\n",
    "\n",
    "#### Seasonal ARIMA (SARIMA) Model\n",
    "- a seasonal ARIMA (SARIMA) model is an extension of the ARIMA model that explicitly supports univariate time series data with a seasonal component\n",
    "- it has additional hyperparameters to specify the autoregression (AR), differencing (I), and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality\n",
    "- $\\text{SARIMA}(p, d, q)(P, D, Q)_m$ model: $$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q} + \\phi_1 y'_{t-m} + \\phi_2 y'_{t-2m} + ... + \\phi_P y'_{t-Pm} + e_t + \\theta_1 e_{t-m} + \\theta_2 e_{t-2m} + ... + \\theta_Q e_{t-Qm} $$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model and $y'_t$ is the differenced series\n",
    "\n",
    "#### Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) Model\n",
    "- a seasonal autoregressive integrated moving-average with exogenous regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables\n",
    "- it adds to the SARIMA model a linear regression model that is used to model the exogenous variables\n",
    "- $\\text{SARIMAX}(p, d, q)(P, D, Q)_m$ model: $$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + ... + \\theta_q e_{t-q} + \\phi_1 y'_{t-m} + \\phi_2 y'_{t-2m} + ... + \\phi_P y'_{t-Pm} + e_t + \\theta_1 e_{t-m} + \\theta_2 e_{t-2m} + ... + \\theta_Q e_{t-Qm} + \\beta_1 x_{1t} + \\beta_2 x_{2t} + ... + \\beta_k x_{kt} $$ where $e_t$ is white noise and $\\phi_1, \\phi_2, ..., \\phi_p, \\theta_1, \\theta_2, ..., \\theta_q$ are parameters of the model and $y'_t$ is the differenced series and $x_{1t}, x_{2t}, ..., x_{kt}$ are the exogenous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Estimation of the parameters of a model is often done by maximum likelihood estimation (MLE). The likelihood function is defined as the probability of the observed data as a function of the parameters of the model. The maximum likelihood estimate of the parameters is the value of the parameters that maximize the likelihood function.\n",
    "Likelikood function for a time series model is the joint probability distribution of the observed data. The likelihood function is maximized with respect to the parameters of the model to obtain the maximum likelihood estimates of the parameters.\n",
    "\n",
    "Suppose we have random variables $X_1, X_2, ..., X_n$ that are independent and identically distributed (i.i.d.) with probability density function $f(x; \\theta)$ where $\\theta$ is the parameter of the distribution. The likelihood function is defined as $$L(\\theta) = \\prod_{i=1}^n f(x_i; \\theta)$$ The maximum likelihood estimate of $\\theta$ is the value of $\\theta$ that maximizes the likelihood function $L(\\theta)$.\n",
    "\n",
    "The maximum likelihood estimate of $\\theta$, $$\\hat{\\theta} = \\arg \\max_{\\theta} L(\\theta) = \\arg \\max_{\\theta} \\log L(\\theta)$$\n",
    "For maximization, we have $\\frac{\\partial \\log L(\\theta)}{\\partial \\theta} = 0$ and $\\frac{\\partial^2 \\log L(\\theta)}{\\partial \\theta^2} < 0$\n",
    "\n",
    "#### For binomial distribution\n",
    "- the likelihood function is $$L(p) = \\prod_{i=1}^N {}^n C_{x_i} p^{x_i} (1 - p)^{n - x_i}$$\n",
    "- the maximum likelihood estimate of $p$ is $$\\hat{p} = \\frac{\\sum_{i=1}^N x_i}{n N}$$ where $n$ is the number of trials and $N$ is the number of experiments\n",
    "\n",
    "#### For poisson distribution\n",
    "- the likelihood function is $$L(\\lambda) = \\prod_{i=1}^N \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!}$$\n",
    "- the maximum likelihood estimate of $\\lambda$ is $$\\hat{\\lambda} = \\frac{\\sum_{i=1}^N x_i}{N}$$\n",
    "\n",
    "#### For Normal distribution\n",
    "- the likelihood function is $$L(\\mu, \\sigma^2) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x_i - \\mu)^2}{2 \\sigma^2}}$$\n",
    "- the maximum likelihood estimate is $$\\hat{\\mu} = \\frac{\\sum_{i=1}^N x_i}{N}, \\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^N (x_i - \\hat{\\mu})^2}{N}$$\n",
    "\n",
    "#### For Uniform distribution\n",
    "- the likelihood function is $$L(a, b) = \\prod_{i=1}^N \\frac{1}{b - a}$$\n",
    "- the maximum likelihood estimate is $$\\hat{a} = \\min_{i=1}^N x_i, \\hat{b} = \\max_{i=1}^N x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
