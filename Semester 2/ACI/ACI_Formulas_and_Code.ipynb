{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial and Computational Intelligence Formulas and Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and act like humans. AI can be classified into two categories:\n",
    "- **Narrow or Weak AI**: AI that is designed to perform a specific task, such as playing chess or recognizing speech.\n",
    "- **General or Strong AI**: AI that has the ability to perform any intellectual task that a human can.\n",
    "\n",
    "#### Definition\n",
    "- the simulation of human intelligence processes by machines, especially computer systems\n",
    "- processes include learning, reasoning, problem-solving, perception, and language understanding\n",
    "- aspects :\n",
    "1. Acting Humanly: The Turing Test Approach\n",
    "    - proposed by Alan Turing (1950), is designed to provide a satisfactory operational definition of intelligence\n",
    "    - computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer\n",
    "    - skills required : NLP, knowledge representation, automated reasoning, machine learning, computer vision, robotics\n",
    "2. Thinking Humanly: The Cognitive Modeling Approach\n",
    "    - involves having machines understand, mimic, and replicate human thought processes\n",
    "    - cognitive science brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind\n",
    "    - skills required : cognitive science, neuroscience, philosophy\n",
    "3. Thinking Rationally: The \"Laws of Thought\" Approach\n",
    "    - programming computers to mimic the cognitive processes humans go through when they're thinking rationally\n",
    "    - could involve logical reasoning or decision-making processes\n",
    "    - skills required : logic, algorithms, probability\n",
    "4. Acting Rationally: The Rational Agent Approach\n",
    "    - rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome\n",
    "    - skills required : game theory, economics, decision theory\n",
    "    - `(percept sequence) -> (agent function) -> (action sequence)`\n",
    "\n",
    "#### Applications of AI\n",
    "\n",
    "AI has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Healthcare**: AI can be used to diagnose diseases, develop personalized treatment plans, and monitor patient health.\n",
    "- **Finance**: AI can be used for fraud detection, risk assessment, and investment analysis.\n",
    "- **Transportation**: AI can be used for autonomous vehicles, traffic management, and logistics optimization.\n",
    "- **Education**: AI can be used for personalized learning, student assessment, and educational research.\n",
    "\n",
    "### Computational Intelligence\n",
    "\n",
    "Computational Intelligence (CI) is a subfield of AI that focuses on the development of intelligent algorithms that can learn from data and adapt to new situations. CI can be classified into three categories:\n",
    "\n",
    "- **Neural Networks**: CI that is inspired by the structure and function of the human brain.\n",
    "- **Fuzzy Systems**: CI that is based on the theory of fuzzy sets, which allows for reasoning with uncertain or imprecise information.\n",
    "- **Evolutionary Computation**: CI that is based on the principles of natural selection and genetic algorithms.\n",
    "\n",
    "#### Applications of CI\n",
    "\n",
    "CI has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Data Mining**: CI can be used to extract useful information from large datasets.\n",
    "- **Robotics**: CI can be used to develop intelligent robots that can perform complex tasks.\n",
    "- **Image Processing**: CI can be used to enhance and analyze digital images and videos.\n",
    "- **Natural Language Processing**: CI can be used to understand and generate human language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent Agents\n",
    "\n",
    "- program that perceives its environment through sensors and acts upon that environment through actuators\n",
    "- PEAS description of task environment\n",
    "    - Performance measure\n",
    "    - Environment\n",
    "    - Actuators\n",
    "    - Sensors\n",
    "- table with PEAS environment for Medical diagnosis system, satellite image analysis system, interactive English tutor\n",
    "\n",
    "| Agent Type              | Performance Measure     | Environment               | Actuators                          | Sensors                      |\n",
    "|-------------------------|-------------------------|---------------------------|------------------------------------|------------------------------|\n",
    "| Medical diagnosis system| Healthy patient, reduced costs | Patient, hospital, staff | Display of questions, tests, diagnoses, treatments, referrals | Keyboard entry of symptoms, findings, patient’s answers |\n",
    "| Satellite image analysis system | Correct image categorization | Downlink from orbiting satellite | Display of scene categorization | Color pixel arrays |\n",
    "| Part-picking robot      | Percentage of parts in correct bins | Conveyor belt with parts; bins | Jointed arm and hand | Camera, joint angle sensors |\n",
    "| Refinery controller     | Purity, yield, safety | Refinery, operators | Valves, pumps, heaters, displays | Temperature, pressure, chemical sensors |\n",
    "| Interactive English tutor | Student’s score on test | Set of students, testing agency | Display of exercises, suggestions, corrections | Keyboard entry |\n",
    "\n",
    "#### Task Environment Dimensions\n",
    "1. sensing\n",
    "    - fully observable : sensors give access to the complete state of the environment at each point in time (eg. chess)\n",
    "    - partially observable : sensors give access to only partial state of the environment at each point in time (eg. poker)\n",
    "2. state determinism\n",
    "    - deterministic : next state of environment is completely determined by current state and action executed by agent\n",
    "    - stochastic : next state of environment is not completely determined by current state and action executed by agent\n",
    "3. action dependance\n",
    "    - episodic : agent's experience is divided into atomic episodes, agent's next action does not depend on actions taken earlier\n",
    "    - sequential : agent's next action depends on actions taken earlier\n",
    "4. state continuity\n",
    "    - static : environment is unchanged while agent is deliberating\n",
    "    - dynamic : environment can change while agent is deliberating\n",
    "    - semi-dynamic : environment does not change with passage of time, but the agent's performance score does\n",
    "5. number of agents\n",
    "    - single agent : agent is operating by itself in the environment\n",
    "    - multiagent : agent is operating with other agents\n",
    "6. number of states / actions\n",
    "    - discrete : finite number of states / actions, represented by integers\n",
    "    - continuous : infinite number of states / actions, represented by real numbers\n",
    "\n",
    "- table with task environment dimensions for medical diagnosis system, satellite image analysis system, interactive English tutor\n",
    "\n",
    "| Task Environment | Fully / Partially Observable | Single / Multi Agent | Deterministic / Stochastic | Episodic / Sequential | Static / Dynamic / Semi-Dynamic | Discrete / Continuous |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Medical diagnosis system | partially | single | stochastic | sequential | dynamic | continuous |\n",
    "| Satellite image analysis system | fully | single | deterministic | episodic | static | continuous |\n",
    "| Interactive English tutor | partially | multiagent | stochastic | sequential | dynamic | discrete |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Architectures\n",
    "\n",
    "| Agent Type | Description | Example |\n",
    "| -- | -- | -- |\n",
    "| Reflex Agents    | - Makes decision based on current percept only, without considering history of previous percepts or actions<br>- Uses set of predefined rules or condition-action pairs to determine its actions<br>- Selects action based on current state of environment and corresponding rule that matches percept<br>- Does not have internal representation or model of world<br>- Typically simple and efficient but lack ability to plan or reason about future states                                  | A simple reflex agent for vacuum cleaning robot may have rule that says \"if current location is dirty, then vacuum the location; otherwise, move to next location\" |\n",
    "| Model-Based Agents| - Maintains internal model or representation of world<br>- Uses this model to keep track of state of environment, as well as possible actions and their outcomes<br>- Uses model to infer how environment will evolve in response to its actions                                                                                    | A chess-playing agent may have model that represents current state of board and possible moves that can be made by each player, can use this model to predict how board will look after each possible move and select move that leads to best outcome |\n",
    "| Goal-Based Agents | - Operates based on set of predefined goals or objectives<br>- Has knowledge about current state of environment and uses this information to determine actions required to achieve goals<br>- Continually assesses current state, compares it to desired goal state, and selects actions that bring it closer to achieving objectives<br>- Often employs search algorithms or planning techniques to find sequence of actions that will lead to goals<br>- Considers current state, available actions, and transition model of environment to make decisions  | A delivery robot that aims to deliver packages to specific locations would have goal-based agent, evaluates current state such as location of packages and obstacles, and plans actions to navigate environment efficiently and complete deliveries |\n",
    "| Utility-Based Agents | - Operates based on utility function that maps state of environment to real number representing degree of satisfaction or desirability of that state<br>- Selects actions that maximize expected utility<br>- Considers current state, available actions, and transition model of environment to make decisions<br>- Typically used in domains where goals are not well-defined or known                                                     | An autonomous car navigating in traffic could be considered utility-based agent, evaluates factors such as safety, efficiency, and passenger comfort when selecting actions, aiming to maximize overall utility of journey |\n",
    "| Learning Agents  | - Intelligent agents that can acquire knowledge and improve their performance through experience<br>- Learn from interactions with environment, including feedback and rewards, to update internal representations and adjust behavior over time<br>- Can adapt and improve decision-making abilities without explicit programming<br>- Types of learning agents:<br>&emsp;&emsp;1. Supervised learning agents<br>&emsp;&emsp;2. Reinforcement learning agents<br>&emsp;&emsp;3. Unsupervised learning agents    | A recommendation system that learns user preferences and provides personalized recommendations based on previous interactions would be considered learning agent, learns from user feedback and adjusts recommendations to improve accuracy and user satisfaction |\n",
    "\n",
    "```\n",
    "function MODEL-BASED-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: \n",
    "        state, some description of the current world state\n",
    "        transition_model, a description of how the next state depends on current state and action\n",
    "        sensor_model, a description of how current state is determined by percept\n",
    "        rules, a set of condition-action rules\n",
    "        action, the most recent action, initially none\n",
    "\n",
    "    state <- UPDATE-STATE(state, action, percept, transition_model, sensor_model)\n",
    "    rule_match <- RULE-MATCH(state, rules)\n",
    "    action <- rule_match.ACTION\n",
    "    return action\n",
    "```\n",
    "\n",
    "To summarize, the model-based reflex agent maintains an internal representation of the world state and uses a transition model and a sensor model to update its understanding of the environment based on received percepts. It then matches the updated state against a set of rules to determine the appropriate action to take in the given state. Finally, it returns the selected action. This agent combines knowledge of the world with its current perception to make intelligent decisions.\n",
    "\n",
    "#### Learning agent\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/IiPoo.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "The four components are:\n",
    "- learning element: makes improvements to the performance element (an example would be Q-learning)\n",
    "- performance element: chooses the actions to take in the environment (this is analogous to a model, e.g. a neural network, that contains the knowledge or rules to act in the environment)\n",
    "- critic: provides feedback (based on some performance metric) to the learning element in order for it to improve the performance element (so this is how you evaluate the potential improvements)\n",
    "- problem generator: suggests actions that will lead to new informative experiences (this would be a behavior policy in reinforcement learning)\n",
    "    \n",
    "```\n",
    "function LEARNING-AGENT-WITH-EXPLORATION(percept) returns an action\n",
    "    persistent:\n",
    "        Q, a table of action values indexed by state and action, initially zero\n",
    "        N, a table of frequencies for state-action pairs, initially zero\n",
    "        s, a, the previous state and action, initially null\n",
    "    if TERMINAL?(s) then Q[s, a] <- R\n",
    "    if s is not null then\n",
    "        increment N[s, a]\n",
    "        Q[s, a] <- Q[s, a] + α(N[s, a])(R + γmaxa'Q[s', a'] - Q[s, a])\n",
    "    if TERMINAL?(s) then s <- null\n",
    "    else s <- s'\n",
    "    if EXPLORE(s) then a <- random action\n",
    "    else a <- argmaxa'Q[s, a']\n",
    "    return a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem solving agents\n",
    "\n",
    "An agent that tries to come up with a sequence of actions that will bring the environment into a desired state.\n",
    "\n",
    "| Phase                 | Description                                                                                                                                             |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Goal Formulation | The agent determines the objectives based on the initial state and percepts from the environment.<br>A goal can be simple or complex, consisting of multiple sub-goals.          |\n",
    "| Problem Formulation | The agent defines the problem in a structured way.<br>1. Initial State: The situation or state where the agent starts.<br>2. Successor Function: Description of possible actions and their resulting states. <br>3. Transition Model: Description of how the state evolves based on actions and current state. <br>4. Goal Test: Method used to determine if a state is a goal state.<br>5. Path Cost: Numerical cost assigned to each path, typically aiming to minimize this cost. |\n",
    "| Search Phase | The agent explores the state space using different search algorithms to find a solution.<br>The search strategy depends on the nature of the problem, agent's knowledge, and available resources. <br>1. Uninformed Search Strategies: Algorithms like Breadth-First Search, Depth-First Search that have no additional information about the problem beyond its definition.<br>2. Informed Search Strategies: Algorithms like A* Search, Greedy Best-First Search that use additional knowledge about the problem.<br>3. Search Tree/Graph: Representation of the problem as a tree or graph where nodes are states and edges are actions.<br>4. Exploration Strategy: The agent's approach to exploring the state space, broadly or deeply or a combination of both. |\n",
    "| Execution Phase | The agent implements the sequence of actions found in the search phase.<br>Actions are executed until the goal state is reached.<br>In a dynamic environment, the agent may need to react to changes and potentially redo some of the previous steps. |\n",
    "\n",
    "Breakdown of problem for different use cases:\n",
    "\n",
    "| Problem              | Initial State                                | Possible Actions                                        | Transition Model (How actions affect the state) | Goal Test (What defines success)    | Path Cost (Measure of the cost of a path) |\n",
    "|----------------------|----------------------------------------------|---------------------------------------------------------|--------------------------------------------------|-------------------------------------|---------------------------------------------|\n",
    "| Vacuum World         | Any state (room could be clean or dirty)     | Move Left, Move Right, Suck (clean the room)            | If at position A and Move Left (ML), then end up at position B which could be dirty or clean.    | All rooms are clean.                 | Number of steps taken in the cleaning process |\n",
    "| 8 – Queen Problem    | Chessboard with no queens placed             | Add a queen to any empty square on the chessboard       | If a queen is added at position A1, it might make position B2 unsafe (FAIL) or B3 safe (SAFE). | All queens are placed safely (no two queens threaten each other). | Number of moves made, including backtracking |\n",
    "| Travelling Problem   | Starting location, defined by the problem     | Take a flight, train, or go shopping (context-specific actions) | If at position A and decide to move to position S, then you end up at position S. | You have arrived at your destination (position B). | Sum of costs, time spent, and quality of travel |\n",
    "\n",
    "#### Different search strategies\n",
    "\n",
    "Search strategies can be generally categorized into Uninformed Search Strategies and Informed Search Strategies.\n",
    "\n",
    "1. Uninformed Search Strategies \n",
    "These are algorithms that have no additional information about the problem beyond its definition. They include:\n",
    "    - Breadth-First Search (BFS): This strategy explores all nodes at a given depth before moving to the next depth level.\n",
    "        - implemented using a queue\n",
    "        - time complexity: $O(b^d)$, where $b$ is the branching factor and $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(b^d)$\n",
    "    - Depth-First Search (DFS): This strategy explores all nodes along a path as far as possible before backtracking.\n",
    "        - implemented using a stack / recursion\n",
    "        - time complexity: $O(b^m)$, where $m$ is the maximum depth of the search tree\n",
    "        - space complexity: $O(bm)$\n",
    "    - Uniform-Cost Search: This strategy explores the node with the lowest path cost.\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^{(1 + C*/e)})$, where $C*$ is the cost of the optimal solution and $e$ is the minimum edge cost\n",
    "    - Depth-Limited Search: This is a depth-first search strategy but with a limit on the depth of the search.\n",
    "        - implemented using a stack / recursion, but completion is not guaranteed if the depth limit $l$ is reached before the goal $d$ is found $(l < d)$.\n",
    "        - time complexity: $O(b^l)$, where $l$ is the depth limit\n",
    "        - space complexity: $O(bl)$\n",
    "    - Iterative Deepening Search: This is a depth-limited search strategy but with an increasing depth limit.\n",
    "        - implemented using a stack / recursion\n",
    "        - time complexity: $O(b^d)$, where $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(bd)$\n",
    "2. Informed Search Strategies\n",
    "These are algorithms that use additional knowledge about the problem. They include:\n",
    "    - Greedy Best-First Search: This strategy expands the node that is closest to the goal, as determined by a heuristic function.\n",
    "        - \n",
    "    - A* Search: This strategy expands the node that minimizes the cost function $f(n) = g(n) + h(n)$, where $g(n)$ is the cost of the path from the initial state to node $n$ and $h(n)$ is the heuristic estimate of the cost to reach the goal from node $n$.\n",
    "    - AO* Search: This is an A* Search strategy but with a limit on the cost of the path.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
