{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial and Computational Intelligence Formulas and Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and act like humans. AI can be classified into two categories:\n",
    "- **Narrow or Weak AI**: AI that is designed to perform a specific task, such as playing chess or recognizing speech.\n",
    "- **General or Strong AI**: AI that has the ability to perform any intellectual task that a human can.\n",
    "\n",
    "#### Definition\n",
    "- the simulation of human intelligence processes by machines, especially computer systems\n",
    "- processes include learning, reasoning, problem-solving, perception, and language understanding\n",
    "- aspects :\n",
    "1. Acting Humanly: The Turing Test Approach\n",
    "    - proposed by Alan Turing (1950), is designed to provide a satisfactory operational definition of intelligence\n",
    "    - computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer\n",
    "    - skills required : NLP, knowledge representation, automated reasoning, machine learning, computer vision, robotics\n",
    "2. Thinking Humanly: The Cognitive Modeling Approach\n",
    "    - involves having machines understand, mimic, and replicate human thought processes\n",
    "    - cognitive science brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind\n",
    "    - skills required : cognitive science, neuroscience, philosophy\n",
    "3. Thinking Rationally: The \"Laws of Thought\" Approach\n",
    "    - programming computers to mimic the cognitive processes humans go through when they're thinking rationally\n",
    "    - could involve logical reasoning or decision-making processes\n",
    "    - skills required : logic, algorithms, probability\n",
    "4. Acting Rationally: The Rational Agent Approach\n",
    "    - rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome\n",
    "    - skills required : game theory, economics, decision theory\n",
    "    - `(percept sequence) -> (agent function) -> (action sequence)`\n",
    "\n",
    "#### Applications of AI\n",
    "\n",
    "AI has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Healthcare**: AI can be used to diagnose diseases, develop personalized treatment plans, and monitor patient health.\n",
    "- **Finance**: AI can be used for fraud detection, risk assessment, and investment analysis.\n",
    "- **Transportation**: AI can be used for autonomous vehicles, traffic management, and logistics optimization.\n",
    "- **Education**: AI can be used for personalized learning, student assessment, and educational research.\n",
    "\n",
    "### Computational Intelligence\n",
    "\n",
    "Computational Intelligence (CI) is a subfield of AI that focuses on the development of intelligent algorithms that can learn from data and adapt to new situations. CI can be classified into three categories:\n",
    "\n",
    "- **Neural Networks**: CI that is inspired by the structure and function of the human brain.\n",
    "- **Fuzzy Systems**: CI that is based on the theory of fuzzy sets, which allows for reasoning with uncertain or imprecise information.\n",
    "- **Evolutionary Computation**: CI that is based on the principles of natural selection and genetic algorithms.\n",
    "\n",
    "#### Applications of CI\n",
    "\n",
    "CI has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Data Mining**: CI can be used to extract useful information from large datasets.\n",
    "- **Robotics**: CI can be used to develop intelligent robots that can perform complex tasks.\n",
    "- **Image Processing**: CI can be used to enhance and analyze digital images and videos.\n",
    "- **Natural Language Processing**: CI can be used to understand and generate human language."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent Agents\n",
    "\n",
    "- program that perceives its environment through sensors and acts upon that environment through actuators\n",
    "- PEAS description of task environment\n",
    "    - Performance measure\n",
    "    - Environment\n",
    "    - Actuators\n",
    "    - Sensors\n",
    "- table with PEAS environment for Medical diagnosis system, satellite image analysis system, interactive English tutor\n",
    "\n",
    "| Agent Type              | Performance Measure     | Environment               | Actuators                          | Sensors                      |\n",
    "|-------------------------|-------------------------|---------------------------|------------------------------------|------------------------------|\n",
    "| Medical diagnosis system| Healthy patient, reduced costs | Patient, hospital, staff | Display of questions, tests, diagnoses, treatments, referrals | Keyboard entry of symptoms, findings, patient’s answers |\n",
    "| Satellite image analysis system | Correct image categorization | Downlink from orbiting satellite | Display of scene categorization | Color pixel arrays |\n",
    "| Part-picking robot      | Percentage of parts in correct bins | Conveyor belt with parts; bins | Jointed arm and hand | Camera, joint angle sensors |\n",
    "| Refinery controller     | Purity, yield, safety | Refinery, operators | Valves, pumps, heaters, displays | Temperature, pressure, chemical sensors |\n",
    "| Interactive English tutor | Student’s score on test | Set of students, testing agency | Display of exercises, suggestions, corrections | Keyboard entry |\n",
    "\n",
    "#### Task Environment Dimensions\n",
    "1. sensing\n",
    "    - fully observable : sensors give access to the complete state of the environment at each point in time (eg. chess)\n",
    "    - partially observable : sensors give access to only partial state of the environment at each point in time (eg. poker)\n",
    "2. state determinism\n",
    "    - deterministic : next state of environment is completely determined by current state and action executed by agent\n",
    "    - stochastic : next state of environment is not completely determined by current state and action executed by agent\n",
    "3. action dependance\n",
    "    - episodic : agent's experience is divided into atomic episodes, agent's next action does not depend on actions taken earlier\n",
    "    - sequential : agent's next action depends on actions taken earlier\n",
    "4. state continuity\n",
    "    - static : environment is unchanged while agent is deliberating\n",
    "    - dynamic : environment can change while agent is deliberating\n",
    "    - semi-dynamic : environment does not change with passage of time, but the agent's performance score does\n",
    "5. number of agents\n",
    "    - single agent : agent is operating by itself in the environment\n",
    "    - multiagent : agent is operating with other agents\n",
    "6. number of states / actions\n",
    "    - discrete : finite number of states / actions, represented by integers\n",
    "    - continuous : infinite number of states / actions, represented by real numbers\n",
    "\n",
    "- table with task environment dimensions for medical diagnosis system, satellite image analysis system, interactive English tutor\n",
    "\n",
    "| Task Environment | Fully / Partially Observable | Single / Multi Agent | Deterministic / Stochastic | Episodic / Sequential | Static / Dynamic / Semi-Dynamic | Discrete / Continuous |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Medical diagnosis system | partially | single | stochastic | sequential | dynamic | continuous |\n",
    "| Satellite image analysis system | fully | single | deterministic | episodic | static | continuous |\n",
    "| Interactive English tutor | partially | multiagent | stochastic | sequential | dynamic | discrete |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Architectures\n",
    "\n",
    "| Agent Type | Description | Example |\n",
    "| -- | -- | -- |\n",
    "| Reflex Agents    | - Makes decision based on current percept only, without considering history of previous percepts or actions<br>- Uses set of predefined rules or condition-action pairs to determine its actions<br>- Selects action based on current state of environment and corresponding rule that matches percept<br>- Does not have internal representation or model of world<br>- Typically simple and efficient but lack ability to plan or reason about future states                                  | A simple reflex agent for vacuum cleaning robot may have rule that says \"if current location is dirty, then vacuum the location; otherwise, move to next location\" |\n",
    "| Model-Based Agents| - Maintains internal model or representation of world<br>- Uses this model to keep track of state of environment, as well as possible actions and their outcomes<br>- Uses model to infer how environment will evolve in response to its actions                                                                                    | A chess-playing agent may have model that represents current state of board and possible moves that can be made by each player, can use this model to predict how board will look after each possible move and select move that leads to best outcome |\n",
    "| Goal-Based Agents | - Operates based on set of predefined goals or objectives<br>- Has knowledge about current state of environment and uses this information to determine actions required to achieve goals<br>- Continually assesses current state, compares it to desired goal state, and selects actions that bring it closer to achieving objectives<br>- Often employs search algorithms or planning techniques to find sequence of actions that will lead to goals<br>- Considers current state, available actions, and transition model of environment to make decisions  | A delivery robot that aims to deliver packages to specific locations would have goal-based agent, evaluates current state such as location of packages and obstacles, and plans actions to navigate environment efficiently and complete deliveries |\n",
    "| Utility-Based Agents | - Operates based on utility function that maps state of environment to real number representing degree of satisfaction or desirability of that state<br>- Selects actions that maximize expected utility<br>- Considers current state, available actions, and transition model of environment to make decisions<br>- Typically used in domains where goals are not well-defined or known                                                     | An autonomous car navigating in traffic could be considered utility-based agent, evaluates factors such as safety, efficiency, and passenger comfort when selecting actions, aiming to maximize overall utility of journey |\n",
    "| Learning Agents  | - Intelligent agents that can acquire knowledge and improve their performance through experience<br>- Learn from interactions with environment, including feedback and rewards, to update internal representations and adjust behavior over time<br>- Can adapt and improve decision-making abilities without explicit programming<br>- Types of learning agents:<br>&emsp;&emsp;1. Supervised learning agents<br>&emsp;&emsp;2. Reinforcement learning agents<br>&emsp;&emsp;3. Unsupervised learning agents    | A recommendation system that learns user preferences and provides personalized recommendations based on previous interactions would be considered learning agent, learns from user feedback and adjusts recommendations to improve accuracy and user satisfaction |\n",
    "\n",
    "```\n",
    "function MODEL-BASED-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: \n",
    "        state, some description of the current world state\n",
    "        transition_model, a description of how the next state depends on current state and action\n",
    "        sensor_model, a description of how current state is determined by percept\n",
    "        rules, a set of condition-action rules\n",
    "        action, the most recent action, initially none\n",
    "\n",
    "    state <- UPDATE-STATE(state, action, percept, transition_model, sensor_model)\n",
    "    rule_match <- RULE-MATCH(state, rules)\n",
    "    action <- rule_match.ACTION\n",
    "    return action\n",
    "```\n",
    "\n",
    "To summarize, the model-based reflex agent maintains an internal representation of the world state and uses a transition model and a sensor model to update its understanding of the environment based on received percepts. It then matches the updated state against a set of rules to determine the appropriate action to take in the given state. Finally, it returns the selected action. This agent combines knowledge of the world with its current perception to make intelligent decisions.\n",
    "\n",
    "#### Learning agent\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/IiPoo.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "The four components are:\n",
    "- learning element: makes improvements to the performance element (an example would be Q-learning)\n",
    "- performance element: chooses the actions to take in the environment (this is analogous to a model, e.g. a neural network, that contains the knowledge or rules to act in the environment)\n",
    "- critic: provides feedback (based on some performance metric) to the learning element in order for it to improve the performance element (so this is how you evaluate the potential improvements)\n",
    "- problem generator: suggests actions that will lead to new informative experiences (this would be a behavior policy in reinforcement learning)\n",
    "    \n",
    "```\n",
    "function LEARNING-AGENT-WITH-EXPLORATION(percept) returns an action\n",
    "    persistent:\n",
    "        Q, a table of action values indexed by state and action, initially zero\n",
    "        N, a table of frequencies for state-action pairs, initially zero\n",
    "        s, a, the previous state and action, initially null\n",
    "    if TERMINAL?(s) then Q[s, a] <- R\n",
    "    if s is not null then\n",
    "        increment N[s, a]\n",
    "        Q[s, a] <- Q[s, a] + α(N[s, a])(R + γmaxa'Q[s', a'] - Q[s, a])\n",
    "    if TERMINAL?(s) then s <- null\n",
    "    else s <- s'\n",
    "    if EXPLORE(s) then a <- random action\n",
    "    else a <- argmaxa'Q[s, a']\n",
    "    return a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem solving agents\n",
    "\n",
    "An agent that tries to come up with a sequence of actions that will bring the environment into a desired state.\n",
    "\n",
    "| Phase                 | Description                                                                                                                                             |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Goal Formulation | The agent determines the objectives based on the initial state and percepts from the environment.<br>A goal can be simple or complex, consisting of multiple sub-goals.          |\n",
    "| Problem Formulation | The agent defines the problem in a structured way.<br>1. Initial State: The situation or state where the agent starts.<br>2. Possible actions: Actions that could be taken from the current state. <br>3. Successor Function / Transition Model: Description of how the state evolves based on actions and current state. <br>4. Goal Test: Method used to determine if a state is a goal state.<br>5. Path Cost: Numerical cost assigned to each path, typically aiming to minimize this cost. |\n",
    "| Search Phase | The agent explores the state space using different search algorithms to find a solution.<br>The search strategy depends on the nature of the problem, agent's knowledge, and available resources. <br>1. Uninformed Search Strategies: Algorithms like Breadth-First Search, Depth-First Search that have no additional information about the problem beyond its definition.<br>2. Informed Search Strategies: Algorithms like A* Search, Greedy Best-First Search that use additional knowledge about the problem.<br>3. Search Tree/Graph: Representation of the problem as a tree or graph where nodes are states and edges are actions.<br>4. Exploration Strategy: The agent's approach to exploring the state space, broadly or deeply or a combination of both. |\n",
    "| Execution Phase | The agent implements the sequence of actions found in the search phase.<br>Actions are executed until the goal state is reached.<br>In a dynamic environment, the agent may need to react to changes and potentially redo some of the previous steps. |\n",
    "\n",
    "Breakdown of problem for different use cases:\n",
    "\n",
    "| Problem              | Initial State                                | Possible Actions                                        | Transition Model (How actions affect the state) | Goal Test (What defines success)    | Path Cost (Measure of the cost of a path) |\n",
    "|----------------------|----------------------------------------------|---------------------------------------------------------|--------------------------------------------------|-------------------------------------|---------------------------------------------|\n",
    "| Vacuum World         | Any state (room could be clean or dirty)     | Move Left, Move Right, Suck (clean the room)            | If at position A and Move Left (ML), then end up at position B which could be dirty or clean.    | All rooms are clean.                 | Number of steps taken in the cleaning process |\n",
    "| 8 – Queen Problem    | Chessboard with no queens placed             | Add a queen to any empty square on the chessboard       | If a queen is added at position A1, it might make position B2 unsafe (FAIL) or B3 safe (SAFE). | All queens are placed safely (no two queens threaten each other). | Number of moves made, including backtracking |\n",
    "| Travelling Problem   | Starting location, defined by the problem     | Take a flight, train, or go shopping (context-specific actions) | If at position A and decide to move to position S, then you end up at position S. | You have arrived at your destination (position B). | Sum of costs, time spent, and quality of travel |\n",
    "\n",
    "#### Different search strategies\n",
    "\n",
    "Search strategies can be generally categorized into Uninformed Search Strategies and Informed Search Strategies.\n",
    "\n",
    "1. Uninformed Search Strategies [Link](https://www.javatpoint.com/ai-uninformed-search-algorithms)\n",
    "    These are algorithms that have no additional information about the problem beyond its definition. They include:\n",
    "    - Breadth-First Search (BFS): This strategy explores all nodes at a given depth before moving to the next depth level.\n",
    "        - implemented using a queue\n",
    "        - time complexity: $O(b^d)$, where $b$ is the branching factor and $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(b^d)$, size of the frontier\n",
    "        - optimality: if path cost is a non-decreasing function of the depth of the node, then BFS is optimal\n",
    "        - completeness: BFS is complete if the branching factor is finite\n",
    "    - Depth-First Search (DFS): This strategy explores all nodes along a path as far as possible before backtracking.\n",
    "        - implemented using a stack / recursion\n",
    "        - time complexity: $O(b^m)$, where $m$ is the maximum depth of the search tree\n",
    "        - space complexity: $O(bm)$\n",
    "        - optimality: DFS is not optimal, may generate a long path to a goal node while a shorter path exists\n",
    "        - completeness: DFS is not complete, may get stuck in infinite loops, but it is complete if the search tree is finite\n",
    "    - Uniform-Cost Search: This strategy explores the node with the lowest path cost.\n",
    "        - also known as Best-First Search\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^{(1 + C*/e)})$, where $C*$ is the cost of the optimal solution and $e$ is the minimum edge cost\n",
    "        - space complexity: $O(b^{(1 + C*/e)})$\n",
    "        - optimality: optimal\n",
    "        - completeness: UCS is complete if the cost of every step exceeds some positive constant\n",
    "    - Depth-Limited Search: This is a depth-first search strategy but with a limit on the depth of the search.\n",
    "        - implemented using a stack / recursion, but completion is not guaranteed if the depth limit $l$ is reached before the goal $d$ is found $(l < d)$.\n",
    "        - time complexity: $O(b^l)$, where $l$ is the depth limit\n",
    "        - space complexity: $O(bl)$\n",
    "        - optimality: not optimal if there is more than one solution at depth $l$\n",
    "        - completeness: complete if the solution is above the depth limit $l$\n",
    "    - Iterative Deepening Search: This is a depth-limited search strategy but with an increasing depth limit.\n",
    "        - implemented using a stack / recursion\n",
    "        - time complexity: $O(b^d)$, where $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(bd)$\n",
    "        - optimality: optimal if the path cost is a non-decreasing function of the depth of the node\n",
    "        - completeness: complete if the branching factor is finite\n",
    "2. Informed Search Strategies\n",
    "    These are algorithms that use additional knowledge about the problem, in the form of a heuristic function, the admissibility of which is given by :\n",
    "    1. admissible heuristic : $h(n) \\leq h^*(n)$, where $h(n)$ is the estimated cost of the cheapest path from node $n$ to a goal node and $h^*(n)$ is the actual cost of the cheapest path from node $n$ to a goal node\n",
    "    2. consistency : $h(n) \\leq c(n, a, n') + h(n')$, where $c(n, a, n')$ is the cost of taking action $a$ from node $n$ to node $n'$ and $h(n')$ is the estimated cost of the cheapest path from node $n'$ to a goal node.\n",
    "    \n",
    "    They include:\n",
    "    - Greedy Best-First Search: This strategy expands the node that is closest to the goal, as determined by a heuristic function.\n",
    "        - cost function: $f(n) = h(n)$\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^m)$, where $m$ is the maximum depth of the search tree\n",
    "        - space complexity: $O(b^m)$\n",
    "        - optimality: not optimal, may not find the shortest path\n",
    "        - completeness: not complete\n",
    "    - A* Search: This strategy expands the node that minimizes the cost function $f(n) = g(n) + h(n)$, where $g(n)$ is the cost of the path from the initial state to node $n$ and $h(n)$ is the heuristic estimate of the cost to reach the goal from node $n$.\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^d)$, where $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(b^d)$\n",
    "        - optimality: optimal if the heuristic function is admissible\n",
    "        - completeness: complete if the branching factor is finite\n",
    "        - two variants :\n",
    "            1. Iterative Deepening A* Search: This is an A* Search strategy but with an increasing depth limit.\n",
    "                - main difference from IDS is that it uses f-cost (g-cost + h-cost) instead of depth as the limit\n",
    "                - at each iteration, the cutoff is the lowest f-cost of any node that exceeded the cutoff on the previous iteration\n",
    "            2. Recursive Best-First A* Search: This is an A* Search strategy that attempts to mimic the operation of standard best-first search but with only linear space complexity\n",
    "                - uses f-limit value to keep track of the best alternative path available from any ancestor of the current node\n",
    "                - if the f-cost of the current node exceeds the f-limit, the recursion unwinds back to the alternative path\n",
    "                - as the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value—the best f-value of its children\n",
    "    - AO* Search: This is an A* Search strategy but with a limit on the cost of the path.\n",
    "\n",
    "#### Comparison between tree search and graph search algorithms:\n",
    "\n",
    "| Aspect                | Tree Search Algorithms              | Graph Search Algorithms                            |\n",
    "|-----------------------|-------------------------------------|----------------------------------------------------|\n",
    "| Node Expansion        | Depth-first or breadth-first        | Avoids redundant expansions (e.g., cycle detection)|\n",
    "| Space Complexity      | Higher space complexity             | Better space efficiency                            |\n",
    "| Completeness          | May not be complete                 | Designed to be complete                            |\n",
    "| Time Complexity       | Can be time-consuming               | Potentially more time-efficient                    |\n",
    "| Applications          | Games, puzzles                      | Route planning, optimization, navigation           |\n",
    "\n",
    "```\n",
    "function Tree-Search (problem, strategy) returns a solution, or failure\n",
    "    initialize the search tree using the initial state of the problem\n",
    "    loop do\n",
    "        if there are no candidates for expansion\n",
    "            then return failure\n",
    "        choose: leaf node for expansion according to the strategy\n",
    "        if the node contains a goal state\n",
    "            then return the corresponding solution\n",
    "        else\n",
    "            Expand the node\n",
    "            Add the resulting nodes to the search tree\n",
    "        end\n",
    "    end\n",
    "```\n",
    "\n",
    "```\n",
    "function Graph-Search (problem, fringe) returns a solution, or failure\n",
    "    initialize the search space using the initial state of the problem\n",
    "    memory to store visited nodes\n",
    "    fringe: an empty set\n",
    "    Insert(Make-Node(Initial-State[problem]), fringe)\n",
    "    closed <- fringe\n",
    "    loop do\n",
    "        if fringe is empty\n",
    "            then return failure\n",
    "        node <- Remove-Front(fringe)\n",
    "        if the node contains a goal state\n",
    "            then return the corresponding solution\n",
    "        else\n",
    "            if the node is not in closed (i.e., not visited yet)\n",
    "                Add the node to the closed set\n",
    "\n",
    "                Expand all the fringe of the node\n",
    "                Add all expanded sorted successors into the fringe\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "```\n",
    "\n",
    "#### Heuristic functions\n",
    "Factors to consider when designing a heuristic function:\n",
    "1. Branching factor: The number of possible actions from a given state.\n",
    "    - If number of nodes generated by A* is $N$ and the solution depth is $d$, then the effective branching factor is given by:\n",
    "        $$ N + 1 = 1 + b* + (b*)^2 + ... + (b*)^d $$, where $b*$ is the effective branching factor.\n",
    "    - The effective branching factor can be approximated by:\n",
    "        $$ b* = (N + 1)^{1/d} $$\n",
    "    - Higher branching factor means more nodes to explore, which increases the time and space complexity of the search.\n",
    "2. Generating admissible heuristics from relaxed problem:\n",
    "    - A relaxed problem is one where some constraints are removed.\n",
    "    - cost of an admissible heuristic in the relaxed problem <= cost of an admissible heuristic in the original problem\n",
    "3. Generating admissible heuristics from subproblems:\n",
    "    - store exact solutions for subproblems and use them to estimate the cost of the original problem, in a pattern database\n",
    "4. Learn heuristics form experience:\n",
    "    - use machine learning techniques to learn heuristics from experience, eg. neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search Algorithms & Optimization Problems\n",
    "- optimization problems : problems that require finding the best solution from a set of possible solutions\n",
    "- local search algorithms : algorithms that start with an initial solution and iteratively improve the solution by moving to a better neighboring solution\n",
    "- typically used for optimization problems that have a large search space and no easy way to determine the optimal solution\n",
    "- not guaranteed to find the optimal solution, but can be more efficient than uninformed search algorithms\n",
    "- examples of optimization problems : \n",
    "    - traveling salesman problem, job scheduling, vehicle routing problem, knapsack problem, etc.\n",
    "- examples of local search algorithms : \n",
    "    - hill climbing, simulated annealing, local beam search, genetic algorithm, ant colony optimization, particle swarm optimization\n",
    "- what differentiates local search algorithms is:\n",
    "    - how they represent the solution space\n",
    "    - how they move from one solution to another\n",
    "    - how they decide which solution to move to\n",
    "- objective function : function that maps a solution to a real number representing the quality of the solution\n",
    "- objective : maximize or minimize the objective function\n",
    "- types of local search algorithms :\n",
    "    1. single instance based\n",
    "        - hill climbing, simulated annealing, local beam search, tabu search\n",
    "    2. population based\n",
    "        - genetic algorithm, ant colony optimization, particle swarm optimization\n",
    "\n",
    "### Hill Climbing Search [Link](https://en.wikipedia.org/wiki/Hill_climbing)\n",
    "- hill climbing : iterative improvement algorithm that starts with an initial solution and iteratively moves to a better neighboring solution\n",
    "- objective : maximize or minimize the objective function, $f(\\bf{x})$\n",
    "- terminates when it reaches a local maximum or minimum, i.e. no better solution can be found in the neighborhood\n",
    "- features:\n",
    "    - generate and test algorithm\n",
    "    - no backtracking\n",
    "    - greedy algorithm\n",
    "- optimality : finds optimal solutions for convex objective functions, but not for non-convex objective functions\n",
    "- attempts to maximize or minimize $f(\\bf{x})$ where $\\bf{x}$ is a vector of decision variables, continuous or discrete\n",
    "- steps :\n",
    "    1. generate initial solution $\\bf{x}$\n",
    "    2. generate neighboring solution $\\bf{x'}$ by making a small change to $\\bf{x}$\n",
    "    3. if $f(\\bf{x'}) > f(\\bf{x})$, then $\\bf{x'}$ is the new solution\n",
    "    4. repeat steps 2 and 3 until no better solution can be found\n",
    "- in discrete vector space, each possible value of $\\bf{x}$ can be visualized as a vertex in a graph\n",
    "    - hill climbing algorithm can be visualized as a walk on the graph, always locally moving to a higher vertex until it reaches a peak\n",
    "- variants:\n",
    "    1. steepest ascent hill climbing : always moves to the best neighboring solution\n",
    "    2. first-choice hill climbing : randomly selects a neighboring solution and moves to it if it is better than the current solution\n",
    "    3. random-restart hill climbing : restarts the algorithm from a random initial solution if it reaches a local maximum or minimum\n",
    "    4. stochastic hill climbing : selects at random from among the uphill moves, the probability of selection can be based on the steepness of the uphill move\n",
    "- problems:\n",
    "    - local maxima : algorithm may get stuck at a local maximum or minimum and fail to find the global maximum or minimum\n",
    "    - plateaus : algorithm may get stuck at a plateau, where all neighboring solutions have the same objective function value\n",
    "    - oscillations : algorithm may oscillate between two or more solutions\n",
    "    - ridges : algorithm may get stuck at a ridge, where the target function ascends in a non-axis aligned direction\n",
    "        - since the algorithm can only move in one direction at a time, it may not be able to move to a better solution without zig-zagging\n",
    "        - hill climber may be forced to take very tiny steps to move along the ridge, taking unreasonable amount of time to reach the peak\n",
    "\n",
    "### Simulated Annealing (not in midsem)\n",
    "\n",
    "### Local Beam Search\n",
    "- an optimization of best first search that reduces memory requirements\n",
    "- explores by expanding the most promising nodes in a limited set of nodes, called the beam of width $k$\n",
    "- maintains a list of $k$ best states instead of all of them, as in best first search\n",
    "- uses breadth first search to build it's search tree\n",
    "    - at each iteration, all successors of states in the current beam are generated, sorted by their evaluation function, and the $k$ best states are selected to form the next beam\n",
    "- the greater the value of $k$, the more states are explored in parallel, but the more memory is required, lesser the states are pruned\n",
    "    - with infinite memory, local beam search is identical to breadth first search\n",
    "    - with $k = 1$, local beam search is identical to hill climbing\n",
    "- since goal state could be pruned, local beam search is not complete\n",
    "- not optimal either, since a better solution could be found in a different branch of the search tree that gets pruned\n",
    "- different from $k$ random restarts, since userful information is shared between states in the beam\n",
    "- problems:\n",
    "    - the beam may become concentrated in a small region of the search space, preventing the algorithm from exploring other regions\n",
    "        - variant called stochastic beam search can be used to address this problem : instead of choosing the $k$ best states, $k$ states are chosen at random from the successors of the current beam\n",
    "    - the beam may contain duplicate states, which can be avoided by using a hash table to store the states in the beam\n",
    "\n",
    "### Genetic Algorithm\n",
    "- variant of local search algorithm that is inspired by the process of natural selection\n",
    "- variant of stochastic beam search in which the successors of the current beam are generated by applying genetic operators to the states in the current beam\n",
    "- begin with a population of $k$ randomly generated states, called the initial population\n",
    "    - population consists of $k$ states, called individuals, each of which is a vector of decision variables\n",
    "    - each state is rated by an objective function $f(\\bf{x})$, which is used to determine the fitness of the state\n",
    "- genetic operators :\n",
    "    1. selection : selects the best states from the current beam to form the next beam\n",
    "    2. crossover : combines two states to form a new state\n",
    "    3. mutation : randomly modifies a state\n",
    "- steps :\n",
    "    1. generate initial population of $k$ states\n",
    "    2. evaluate fitness of each state\n",
    "    3. select random pairs of states from the current population\n",
    "        - apply crossover operator to each pair to generate two new states\n",
    "        - some locations may be subject to random mutation, with a small independent probability\n",
    "\n",
    "<img src=\"https://i.imgur.com/mhk2m51.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "\n",
    "### Ant Colony Optimization\n",
    "- variant of local search algorithm that is inspired by the foraging behavior of ants\n",
    "- artificial ants locate optimal solutions by moving through a parameter space representing all possible solutions\n",
    "- real ants deposit pheromones on the ground to mark paths between the colony and food sources\n",
    "    - pheromones evaporate over time, so paths that are not used frequently disappear\n",
    "    - ants are more likely to follow paths with higher pheromone concentrations\n",
    "- similarly, simulated ants record the positions and quality of solutions they have visited in a pheromone matrix\n",
    "- in the ACO algorithm:\n",
    "    - an artificial ant is a simple computational agent that searches for good solutions to a given optimization problem\n",
    "    - a colony is a set of artificial ants\n",
    "    - to apply, the optimization problem needs to be converted into the problem of finding the shortest path on a weighted graph\n",
    "- steps :\n",
    "    - each ant stochasticly constructs a solution to the problem, the order in which the edges in the graph should be followed\n",
    "    - the paths found by the ants are evaluated by an objective function\n",
    "    - update the pheromone matrix based on the quality of the solutions found by the ants\n",
    "- evaporation:\n",
    "    - a short path gets marched on more frequently, depositing more pheromone on it\n",
    "    - evaporation has the advantage of avoiding the convergence to a locally optimal solution\n",
    "- in general, \n",
    "    - the $k$ 'th ant goes from state $i$ to state $j$ with probability $p_{ij}^k$, given by:\n",
    "        $$ p_{ij}^k = \\frac{[\\tau_{ij}]^\\alpha * [\\eta_{ij}]^\\beta}{\\sum_{l \\in allowed} [\\tau_{il}]^\\alpha * [\\eta_{il}]^\\beta} $$\n",
    "    - where $\\tau_{ij}$ is the amount of pheromone on the edge from state $i$ to state $j$, $\\alpha$ is the parameter that controls the influence of pheromone, $\\eta_{ij}$ describes the desirability of the edge from state $i$ to state $j$, typically the inverse of the distance between the states, $\\beta$ is the parameter that controls the influence of desirability, and $allowed$ is the set of states that can be reached from state $i$\n",
    "    - the pheromone matrix is updated using the following formula:\n",
    "        $$ \\tau_{ij} \\leftarrow (1 - \\rho) * \\tau_{ij} + \\sum_{k = 1}^m \\Delta \\tau_{ij}^k $$\n",
    "    - where $\\rho$ is the pheromone evaporation rate, $\\Delta \\tau_{ij}^k$ is the amount of pheromone deposited by the $k$ 'th ant on the edge from state $i$ to state $j$\n",
    "        - $\\Delta \\tau_{ij}^k$ is given by:\n",
    "            $$ \\Delta \\tau_{ij}^k = \\begin{cases} \\frac{Q}{L_k} & \\text{if ant } k \\text{ uses edge } (i, j) \\text{ in its solution} \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "            - where $Q$ is a constant, $L_k$ is the length of the path found by the $k$ 'th ant\n",
    "    - usually updated when all ants have completed their tours, increasing or decreasing the pheromone level on each edge based on the quality of the solutions found by the ants\n",
    "- parameter tuning:\n",
    "    - high evaporation causes exploration, low evaporation causes exploitation \n",
    "    - high $\\alpha$ increases the weight of pheromone, increasing exploitation\n",
    "    - high $\\beta$ increases the weight of desirability, making shorter paths more attractive\n",
    "    - number of ants $m$ should be large enough to cover the search space, but not too large to avoid excessive computation\n",
    "\n",
    "### Particle Swarm Optimization (not in midsem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
