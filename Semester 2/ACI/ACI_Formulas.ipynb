{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial and Computational Intelligence Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and act like humans. AI can be classified into two categories:\n",
    "- **Narrow or Weak AI**: AI that is designed to perform a specific task, such as playing chess or recognizing speech.\n",
    "- **General or Strong AI**: AI that has the ability to perform any intellectual task that a human can.\n",
    "\n",
    "#### Definition\n",
    "- the simulation of human intelligence processes by machines, especially computer systems\n",
    "- processes include learning, reasoning, problem-solving, perception, and language understanding\n",
    "- aspects :\n",
    "1. Acting Humanly: The Turing Test Approach\n",
    "    - proposed by Alan Turing (1950), is designed to provide a satisfactory operational definition of intelligence\n",
    "    - computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer\n",
    "    - skills required : NLP, knowledge representation, automated reasoning, machine learning, computer vision, robotics\n",
    "2. Thinking Humanly: The Cognitive Modeling Approach\n",
    "    - involves having machines understand, mimic, and replicate human thought processes\n",
    "    - cognitive science brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind\n",
    "    - skills required : cognitive science, neuroscience, philosophy\n",
    "3. Thinking Rationally: The \"Laws of Thought\" Approach\n",
    "    - programming computers to mimic the cognitive processes humans go through when they're thinking rationally\n",
    "    - could involve logical reasoning or decision-making processes\n",
    "    - skills required : logic, algorithms, probability\n",
    "4. Acting Rationally: The Rational Agent Approach\n",
    "    - rational agent is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome\n",
    "    - skills required : game theory, economics, decision theory\n",
    "    - `(percept sequence) -> (agent function) -> (action sequence)`\n",
    "\n",
    "#### Applications of AI\n",
    "\n",
    "AI has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Healthcare**: AI can be used to diagnose diseases, develop personalized treatment plans, and monitor patient health.\n",
    "- **Finance**: AI can be used for fraud detection, risk assessment, and investment analysis.\n",
    "- **Transportation**: AI can be used for autonomous vehicles, traffic management, and logistics optimization.\n",
    "- **Education**: AI can be used for personalized learning, student assessment, and educational research.\n",
    "\n",
    "### Computational Intelligence\n",
    "\n",
    "Computational Intelligence (CI) is a subfield of AI that focuses on the development of intelligent algorithms that can learn from data and adapt to new situations. CI can be classified into three categories:\n",
    "\n",
    "- **Neural Networks**: CI that is inspired by the structure and function of the human brain.\n",
    "- **Fuzzy Systems**: CI that is based on the theory of fuzzy sets, which allows for reasoning with uncertain or imprecise information.\n",
    "- **Evolutionary Computation**: CI that is based on the principles of natural selection and genetic algorithms.\n",
    "\n",
    "#### Applications of CI\n",
    "\n",
    "CI has a wide range of applications in various fields, including:\n",
    "\n",
    "- **Data Mining**: CI can be used to extract useful information from large datasets.\n",
    "- **Robotics**: CI can be used to develop intelligent robots that can perform complex tasks.\n",
    "- **Image Processing**: CI can be used to enhance and analyze digital images and videos.\n",
    "- **Natural Language Processing**: CI can be used to understand and generate human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent Agents\n",
    "\n",
    "- program that perceives its environment through sensors and acts upon that environment through actuators\n",
    "- PEAS description of task environment\n",
    "    - Performance measure\n",
    "    - Environment\n",
    "    - Actuators\n",
    "    - Sensors\n",
    "- table with PEAS environment for Medical diagnosis system, satellite image analysis system, interactive English tutor\n",
    "\n",
    "| Agent Type              | Performance Measure     | Environment               | Actuators                          | Sensors                      |\n",
    "|-------------------------|-------------------------|---------------------------|------------------------------------|------------------------------|\n",
    "| Medical diagnosis system| Healthy patient, reduced costs | Patient, hospital, staff | Display of questions, tests, diagnoses, treatments, referrals | Keyboard entry of symptoms, findings, patient’s answers |\n",
    "| Satellite image analysis system | Correct image categorization | Downlink from orbiting satellite | Display of scene categorization | Color pixel arrays |\n",
    "| Part-picking robot      | Percentage of parts in correct bins | Conveyor belt with parts; bins | Jointed arm and hand | Camera, joint angle sensors |\n",
    "| Refinery controller     | Purity, yield, safety | Refinery, operators | Valves, pumps, heaters, displays | Temperature, pressure, chemical sensors |\n",
    "| Interactive English tutor | Student’s score on test | Set of students, testing agency | Display of exercises, suggestions, corrections | Keyboard entry |\n",
    "\n",
    "#### Task Environment Dimensions\n",
    "1. sensing\n",
    "    - fully observable : sensors give access to the complete state of the environment at each point in time (eg. chess)\n",
    "    - partially observable : sensors give access to only partial state of the environment at each point in time (eg. poker)\n",
    "2. state determinism\n",
    "    - deterministic : next state of environment is completely determined by current state and action executed by agent\n",
    "    - stochastic : next state of environment is not completely determined by current state and action executed by agent\n",
    "3. action dependance\n",
    "    - episodic : agent's experience is divided into atomic episodes, agent's next action does not depend on actions taken earlier\n",
    "    - sequential : agent's next action depends on actions taken earlier\n",
    "4. state continuity\n",
    "    - static : environment is unchanged while agent is deliberating\n",
    "    - dynamic : environment can change while agent is deliberating\n",
    "    - semi-dynamic : environment does not change with passage of time, but the agent's performance score does\n",
    "5. number of agents\n",
    "    - single agent : agent is operating by itself in the environment\n",
    "    - multiagent : agent is operating with other agents\n",
    "6. number of states / actions\n",
    "    - discrete : finite number of states / actions, represented by integers\n",
    "    - continuous : infinite number of states / actions, represented by real numbers\n",
    "\n",
    "- table with task environment dimensions for medical diagnosis system, satellite image analysis system, interactive English tutor\n",
    "\n",
    "| Task Environment | Fully / Partially Observable | Single / Multi Agent | Deterministic / Stochastic | Episodic / Sequential | Static / Dynamic / Semi-Dynamic | Discrete / Continuous |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| Medical diagnosis system | partially | single | stochastic | sequential | dynamic | continuous |\n",
    "| Satellite image analysis system | fully | single | deterministic | episodic | static | continuous |\n",
    "| Interactive English tutor | partially | multiagent | stochastic | sequential | dynamic | discrete |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Architectures\n",
    "\n",
    "| Agent Type | Description | Example |\n",
    "| -- | -- | -- |\n",
    "| Reflex Agents    | - Makes decision based on current percept only, without considering history of previous percepts or actions<br>- Uses set of predefined rules or condition-action pairs to determine its actions<br>- Selects action based on current state of environment and corresponding rule that matches percept<br>- Does not have internal representation or model of world<br>- Typically simple and efficient but lack ability to plan or reason about future states                                  | A simple reflex agent for vacuum cleaning robot may have rule that says \"if current location is dirty, then vacuum the location; otherwise, move to next location\" |\n",
    "| Model-Based Agents| - Maintains internal model or representation of world<br>- Uses this model to keep track of state of environment, as well as possible actions and their outcomes<br>- Uses model to infer how environment will evolve in response to its actions                                                                                    | A chess-playing agent may have model that represents current state of board and possible moves that can be made by each player, can use this model to predict how board will look after each possible move and select move that leads to best outcome |\n",
    "| Goal-Based Agents | - Operates based on set of predefined goals or objectives<br>- Has knowledge about current state of environment and uses this information to determine actions required to achieve goals<br>- Continually assesses current state, compares it to desired goal state, and selects actions that bring it closer to achieving objectives<br>- Often employs search algorithms or planning techniques to find sequence of actions that will lead to goals<br>- Considers current state, available actions, and transition model of environment to make decisions  | A delivery robot that aims to deliver packages to specific locations would have goal-based agent, evaluates current state such as location of packages and obstacles, and plans actions to navigate environment efficiently and complete deliveries |\n",
    "| Utility-Based Agents | - Operates based on utility function that maps state of environment to real number representing degree of satisfaction or desirability of that state<br>- Selects actions that maximize expected utility<br>- Considers current state, available actions, and transition model of environment to make decisions<br>- Typically used in domains where goals are not well-defined or known                                                     | An autonomous car navigating in traffic could be considered utility-based agent, evaluates factors such as safety, efficiency, and passenger comfort when selecting actions, aiming to maximize overall utility of journey |\n",
    "| Learning Agents  | - Intelligent agents that can acquire knowledge and improve their performance through experience<br>- Learn from interactions with environment, including feedback and rewards, to update internal representations and adjust behavior over time<br>- Can adapt and improve decision-making abilities without explicit programming<br>- Types of learning agents:<br>&emsp;&emsp;1. Supervised learning agents<br>&emsp;&emsp;2. Reinforcement learning agents<br>&emsp;&emsp;3. Unsupervised learning agents    | A recommendation system that learns user preferences and provides personalized recommendations based on previous interactions would be considered learning agent, learns from user feedback and adjusts recommendations to improve accuracy and user satisfaction |\n",
    "\n",
    "```\n",
    "function MODEL-BASED-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: \n",
    "        state, some description of the current world state\n",
    "        transition_model, a description of how the next state depends on current state and action\n",
    "        sensor_model, a description of how current state is determined by percept\n",
    "        rules, a set of condition-action rules\n",
    "        action, the most recent action, initially none\n",
    "\n",
    "    state <- UPDATE-STATE(state, action, percept, transition_model, sensor_model)\n",
    "    rule_match <- RULE-MATCH(state, rules)\n",
    "    action <- rule_match.ACTION\n",
    "    return action\n",
    "```\n",
    "\n",
    "To summarize, the model-based reflex agent maintains an internal representation of the world state and uses a transition model and a sensor model to update its understanding of the environment based on received percepts. It then matches the updated state against a set of rules to determine the appropriate action to take in the given state. Finally, it returns the selected action. This agent combines knowledge of the world with its current perception to make intelligent decisions.\n",
    "\n",
    "#### Learning agent\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/IiPoo.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "The four components are:\n",
    "- learning element: makes improvements to the performance element (an example would be Q-learning)\n",
    "- performance element: chooses the actions to take in the environment (this is analogous to a model, e.g. a neural network, that contains the knowledge or rules to act in the environment)\n",
    "- critic: provides feedback (based on some performance metric) to the learning element in order for it to improve the performance element (so this is how you evaluate the potential improvements)\n",
    "- problem generator: suggests actions that will lead to new informative experiences (this would be a behavior policy in reinforcement learning)\n",
    "    \n",
    "```\n",
    "function LEARNING-AGENT-WITH-EXPLORATION(percept) returns an action\n",
    "    persistent:\n",
    "        Q, a table of action values indexed by state and action, initially zero\n",
    "        N, a table of frequencies for state-action pairs, initially zero\n",
    "        s, a, the previous state and action, initially null\n",
    "    if TERMINAL?(s) then Q[s, a] <- R\n",
    "    if s is not null then\n",
    "        increment N[s, a]\n",
    "        Q[s, a] <- Q[s, a] + α(N[s, a])(R + γmaxa'Q[s', a'] - Q[s, a])\n",
    "    if TERMINAL?(s) then s <- null\n",
    "    else s <- s'\n",
    "    if EXPLORE(s) then a <- random action\n",
    "    else a <- argmaxa'Q[s, a']\n",
    "    return a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem solving agents\n",
    "\n",
    "An agent that tries to come up with a sequence of actions that will bring the environment into a desired state.\n",
    "\n",
    "| Phase                 | Description                                                                                                                                             |\n",
    "|-----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Goal Formulation | The agent determines the objectives based on the initial state and percepts from the environment.<br>A goal can be simple or complex, consisting of multiple sub-goals.          |\n",
    "| Problem Formulation | The agent defines the problem in a structured way.<br>1. Initial State: The situation or state where the agent starts.<br>2. Possible actions: Actions that could be taken from the current state. <br>3. Successor Function / Transition Model: Description of how the state evolves based on actions and current state. <br>4. Goal Test: Method used to determine if a state is a goal state.<br>5. Path Cost: Numerical cost assigned to each path, typically aiming to minimize this cost. |\n",
    "| Search Phase | The agent explores the state space using different search algorithms to find a solution.<br>The search strategy depends on the nature of the problem, agent's knowledge, and available resources. <br>1. Uninformed Search Strategies: Algorithms like Breadth-First Search, Depth-First Search that have no additional information about the problem beyond its definition.<br>2. Informed Search Strategies: Algorithms like A* Search, Greedy Best-First Search that use additional knowledge about the problem.<br>3. Search Tree/Graph: Representation of the problem as a tree or graph where nodes are states and edges are actions.<br>4. Exploration Strategy: The agent's approach to exploring the state space, broadly or deeply or a combination of both. |\n",
    "| Execution Phase | The agent implements the sequence of actions found in the search phase.<br>Actions are executed until the goal state is reached.<br>In a dynamic environment, the agent may need to react to changes and potentially redo some of the previous steps. |\n",
    "\n",
    "Breakdown of problem for different use cases:\n",
    "\n",
    "| Problem              | Initial State                                | Possible Actions                                        | Transition Model (How actions affect the state) | Goal Test (What defines success)    | Path Cost (Measure of the cost of a path) |\n",
    "|----------------------|----------------------------------------------|---------------------------------------------------------|--------------------------------------------------|-------------------------------------|---------------------------------------------|\n",
    "| Vacuum World         | Any state (room could be clean or dirty)     | Move Left, Move Right, Suck (clean the room)            | If at position A and Move Left (ML), then end up at position B which could be dirty or clean.    | All rooms are clean.                 | Number of steps taken in the cleaning process |\n",
    "| 8 – Queen Problem    | Chessboard with no queens placed             | Add a queen to any empty square on the chessboard       | If a queen is added at position A1, it might make position B2 unsafe (FAIL) or B3 safe (SAFE). | All queens are placed safely (no two queens threaten each other). | Number of moves made, including backtracking |\n",
    "| Travelling Problem   | Starting location, defined by the problem     | Take a flight, train, or go shopping (context-specific actions) | If at position A and decide to move to position S, then you end up at position S. | You have arrived at your destination (position B). | Sum of costs, time spent, and quality of travel |\n",
    "\n",
    "#### Different search strategies\n",
    "\n",
    "Search strategies can be generally categorized into Uninformed Search Strategies and Informed Search Strategies.\n",
    "\n",
    "1. Uninformed Search Strategies [Link](https://www.javatpoint.com/ai-uninformed-search-algorithms)\n",
    "    These are algorithms that have no additional information about the problem beyond its definition. They include:\n",
    "    - Breadth-First Search (BFS): This strategy explores all nodes at a given depth before moving to the next depth level.\n",
    "        - implemented using a queue\n",
    "        - time complexity: $O(b^d)$, where $b$ is the branching factor and $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(b^d)$, size of the frontier\n",
    "        - optimality: if path cost is a non-decreasing function of the depth of the node, then BFS is optimal\n",
    "        - completeness: BFS is complete if the branching factor is finite\n",
    "    - Depth-First Search (DFS): This strategy explores all nodes along a path as far as possible before backtracking.\n",
    "        - implemented using a stack / recursion\n",
    "        - time complexity: $O(b^m)$, where $m$ is the maximum depth of the search tree\n",
    "        - space complexity: $O(bm)$\n",
    "        - optimality: DFS is not optimal, may generate a long path to a goal node while a shorter path exists\n",
    "        - completeness: DFS is not complete, may get stuck in infinite loops, but it is complete if the search tree is finite\n",
    "    - Uniform-Cost Search: This strategy explores the node with the lowest path cost.\n",
    "        - also known as Best-First Search\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^{(1 + C*/e)})$, where $C*$ is the cost of the optimal solution and $e$ is the minimum edge cost\n",
    "        - space complexity: $O(b^{(1 + C*/e)})$\n",
    "        - optimality: optimal\n",
    "        - completeness: UCS is complete if the cost of every step exceeds some positive constant\n",
    "    - Depth-Limited Search: This is a depth-first search strategy but with a limit on the depth of the search.\n",
    "        - implemented using a stack / recursion, but completion is not guaranteed if the depth limit $l$ is reached before the goal $d$ is found $(l < d)$.\n",
    "        - time complexity: $O(b^l)$, where $l$ is the depth limit\n",
    "        - space complexity: $O(bl)$\n",
    "        - optimality: not optimal if there is more than one solution at depth $l$\n",
    "        - completeness: complete if the solution is above the depth limit $l$\n",
    "    - Iterative Deepening Search: This is a depth-limited search strategy but with an increasing depth limit.\n",
    "        - implemented using a stack / recursion\n",
    "        - time complexity: $O(b^d)$, where $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(bd)$\n",
    "        - optimality: optimal if the path cost is a non-decreasing function of the depth of the node\n",
    "        - completeness: complete if the branching factor is finite\n",
    "2. Informed Search Strategies\n",
    "    These are algorithms that use additional knowledge about the problem, in the form of a heuristic function, the admissibility of which is given by :\n",
    "    1. admissible heuristic : $h(n) \\leq h^*(n)$, where $h(n)$ is the estimated cost of the cheapest path from node $n$ to a goal node and $h^*(n)$ is the actual cost of the cheapest path from node $n$ to a goal node\n",
    "    2. consistency : $h(n) \\leq c(n, a, n') + h(n')$, where $c(n, a, n')$ is the cost of taking action $a$ from node $n$ to node $n'$ and $h(n')$ is the estimated cost of the cheapest path from node $n'$ to a goal node.\n",
    "    \n",
    "    They include:\n",
    "    - Greedy Best-First Search: This strategy expands the node that is closest to the goal, as determined by a heuristic function.\n",
    "        - cost function: $f(n) = h(n)$\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^m)$, where $m$ is the maximum depth of the search tree\n",
    "        - space complexity: $O(b^m)$\n",
    "        - optimality: not optimal, may not find the shortest path\n",
    "        - completeness: not complete\n",
    "    - A* Search: This strategy expands the node that minimizes the cost function $f(n) = g(n) + h(n)$, where $g(n)$ is the cost of the path from the initial state to node $n$ and $h(n)$ is the heuristic estimate of the cost to reach the goal from node $n$.\n",
    "        - implemented using a priority queue\n",
    "        - time complexity: $O(b^d)$, where $d$ is the depth of the shallowest goal node\n",
    "        - space complexity: $O(b^d)$\n",
    "        - optimality: optimal if the heuristic function is admissible\n",
    "        - completeness: complete if the branching factor is finite\n",
    "        - two variants :\n",
    "            1. Iterative Deepening A* Search: This is an A* Search strategy but with an increasing depth limit.\n",
    "                - main difference from IDS is that it uses f-cost (g-cost + h-cost) instead of depth as the limit\n",
    "                - at each iteration, the cutoff is the lowest f-cost of any node that exceeded the cutoff on the previous iteration\n",
    "            2. Recursive Best-First A* Search: This is an A* Search strategy that attempts to mimic the operation of standard best-first search but with only linear space complexity\n",
    "                - uses f-limit value to keep track of the best alternative path available from any ancestor of the current node\n",
    "                - if the f-cost of the current node exceeds the f-limit, the recursion unwinds back to the alternative path\n",
    "                - as the recursion unwinds, RBFS replaces the f-value of each node along the path with a backed-up value—the best f-value of its children\n",
    "    - AO* Search: This is an A* Search strategy but with a limit on the cost of the path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm tracing\n",
    "\n",
    "|Iter| Open list / Frontiers / Fringe | Closed list | Goal test | \n",
    "|----|--------------------------------|-------------|-----------|\n",
    "| 0  | (S)                            | []          | Fail on (S)     |\n",
    "| 1  | (S,A) (S,B) (S,C)              | (S)        | Fail on (S,A)   |\n",
    "| 2  | (S,A,D) (S,A,E) (S,B) (S,C)    | (S), (S,A)| Fail on (S,A,D) |\n",
    "\n",
    "Also specify `Expanded`, `Generated`, `Maximum queue length`.\n",
    "\n",
    "1. For BFS, add nodes to the end of the queue and remove nodes from the front of the open list.\n",
    "2. For DFS, add nodes to the front of the queue and remove nodes from the front of the open list.\n",
    "3. For DLS, add nodes to the front of the queue and remove nodes from the front of the open list until the depth limit is reached.\n",
    "4. For IDDFS, add nodes to the front of the queue and remove nodes from the front of the open list until the depth limit is reached, then increase the depth limit and repeat.\n",
    "\n",
    "|Iter| Open list / Frontiers / Fringe | Closed list | Goal test |\n",
    "|----|--------------------------------|-------------|-----------|\n",
    "| 0  | (S:$f(S)$)                          | []          | Fail on (S) |\n",
    "| 1  | (S,A:70) (S,B:85) (S,C:90)     | (S)         | Fail on (S,A) |\n",
    "| 2  | (S,A,D:130) (S,A,E:135) (S,B:85) (S,C:90) | (S), (S,A) | Fail on (S,A,D) |\n",
    "\n",
    "1. For UCS, add nodes to the queue based on the path cost and remove nodes from the front of the open list to expand.\n",
    "    - $f(S)$ is the path cost from the initial state to the current state\n",
    "2. For GBFS, add nodes to the queue based on the heuristic value and remove nodes from the front of the open list to expand.\n",
    "    - $f(S)$ is the heuristic value of the current state, which is the estimated cost of the cheapest path from the current state to a goal state\n",
    "3. For A*, add nodes to the queue based on the sum of the path cost and heuristic value and remove nodes from the front of the open list to expand.\n",
    "    - $f(S) = g(S) + h(S)$, where $g(S)$ is the path cost from the initial state to the current state and $h(S)$ is the heuristic value of the current state, which is the estimated cost of the cheapest path from the current state to a goal state\n",
    "\n",
    "\n",
    "#### Comparison between tree search and graph search algorithms:\n",
    "\n",
    "| Aspect                | Tree Search Algorithms              | Graph Search Algorithms                            |\n",
    "|-----------------------|-------------------------------------|----------------------------------------------------|\n",
    "| Node Expansion        | Depth-first or breadth-first        | Avoids redundant expansions (e.g., cycle detection)|\n",
    "| Space Complexity      | Higher space complexity             | Better space efficiency                            |\n",
    "| Completeness          | May not be complete                 | Designed to be complete                            |\n",
    "| Time Complexity       | Can be time-consuming               | Potentially more time-efficient                    |\n",
    "| Applications          | Games, puzzles                      | Route planning, optimization, navigation           |\n",
    "\n",
    "```\n",
    "function Tree-Search (problem, strategy) returns a solution, or failure\n",
    "    initialize the search tree using the initial state of the problem\n",
    "    loop do\n",
    "        if there are no candidates for expansion\n",
    "            then return failure\n",
    "        choose: leaf node for expansion according to the strategy\n",
    "        if the node contains a goal state\n",
    "            then return the corresponding solution\n",
    "        else\n",
    "            Expand the node\n",
    "            Add the resulting nodes to the search tree\n",
    "        end\n",
    "    end\n",
    "```\n",
    "\n",
    "```\n",
    "function Graph-Search (problem, fringe) returns a solution, or failure\n",
    "    initialize the search space using the initial state of the problem\n",
    "    memory to store visited nodes\n",
    "    fringe: an empty set\n",
    "    Insert(Make-Node(Initial-State[problem]), fringe)\n",
    "    closed <- fringe\n",
    "    loop do\n",
    "        if fringe is empty\n",
    "            then return failure\n",
    "        node <- Remove-Front(fringe)\n",
    "        if the node contains a goal state\n",
    "            then return the corresponding solution\n",
    "        else\n",
    "            if the node is not in closed (i.e., not visited yet)\n",
    "                Add the node to the closed set\n",
    "\n",
    "                Expand all the fringe of the node\n",
    "                Add all expanded sorted successors into the fringe\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "```\n",
    "\n",
    "#### Heuristic functions\n",
    "Factors to consider when designing a heuristic function:\n",
    "1. Branching factor: The number of possible actions from a given state.\n",
    "    - If number of nodes generated by A* is $N$ and the solution depth is $d$, then the effective branching factor is given by: $$ N + 1 = 1 + b* + (b*)^2 + ... + (b*)^d $$ where $b*$ is the effective branching factor.\n",
    "    - The effective branching factor can be approximated by:\n",
    "        $$ b* = (N + 1)^{1/d} $$\n",
    "    - Higher branching factor means more nodes to explore, which increases the time and space complexity of the search.\n",
    "2. Generating admissible heuristics from relaxed problem:\n",
    "    - A relaxed problem is one where some constraints are removed.\n",
    "    - cost of an admissible heuristic in the relaxed problem <= cost of an admissible heuristic in the original problem\n",
    "3. Generating admissible heuristics from subproblems:\n",
    "    - store exact solutions for subproblems and use them to estimate the cost of the original problem, in a pattern database\n",
    "4. Learn heuristics form experience:\n",
    "    - use machine learning techniques to learn heuristics from experience, eg. neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search Algorithms & Optimization Problems\n",
    "- optimization problems : problems that require finding the best solution from a set of possible solutions\n",
    "- local search algorithms : algorithms that start with an initial solution and iteratively improve the solution by moving to a better neighboring solution\n",
    "- typically used for optimization problems that have a large search space and no easy way to determine the optimal solution\n",
    "- not guaranteed to find the optimal solution, but can be more efficient than uninformed search algorithms\n",
    "- examples of optimization problems : \n",
    "    - traveling salesman problem, job scheduling, vehicle routing problem, knapsack problem, etc.\n",
    "- examples of local search algorithms : \n",
    "    - hill climbing, simulated annealing, local beam search, genetic algorithm, ant colony optimization, particle swarm optimization\n",
    "- what differentiates local search algorithms is:\n",
    "    - how they represent the solution space\n",
    "    - how they move from one solution to another\n",
    "    - how they decide which solution to move to\n",
    "- objective function : function that maps a solution to a real number representing the quality of the solution\n",
    "- objective : maximize or minimize the objective function\n",
    "- types of local search algorithms :\n",
    "    1. single instance based\n",
    "        - hill climbing, simulated annealing, local beam search, tabu search\n",
    "    2. population based\n",
    "        - genetic algorithm, ant colony optimization, particle swarm optimization\n",
    "\n",
    "### Hill Climbing Search [Link](https://en.wikipedia.org/wiki/Hill_climbing)\n",
    "- hill climbing : iterative improvement algorithm that starts with an initial solution and iteratively moves to a better neighboring solution\n",
    "- objective : maximize or minimize the objective function, $f(\\bf{x})$\n",
    "- terminates when it reaches a local maximum or minimum, i.e. no better solution can be found in the neighborhood\n",
    "- features:\n",
    "    - generate and test algorithm\n",
    "    - no backtracking\n",
    "    - greedy algorithm\n",
    "- optimality : finds optimal solutions for convex objective functions, but not for non-convex objective functions\n",
    "- attempts to maximize or minimize $f(\\bf{x})$ where $\\bf{x}$ is a vector of decision variables, continuous or discrete\n",
    "- steps :\n",
    "    1. generate initial solution $\\bf{x}$\n",
    "    2. generate neighboring solution $\\bf{x'}$ by making a small change to $\\bf{x}$\n",
    "    3. if $f(\\bf{x'}) > f(\\bf{x})$, then $\\bf{x'}$ is the new solution\n",
    "    4. repeat steps 2 and 3 until no better solution can be found\n",
    "- in discrete vector space, each possible value of $\\bf{x}$ can be visualized as a vertex in a graph\n",
    "    - hill climbing algorithm can be visualized as a walk on the graph, always locally moving to a higher vertex until it reaches a peak\n",
    "- variants:\n",
    "    1. steepest ascent hill climbing : always moves to the best neighboring solution\n",
    "    2. first-choice hill climbing : randomly selects a neighboring solution and moves to it if it is better than the current solution\n",
    "    3. random-restart hill climbing : restarts the algorithm from a random initial solution if it reaches a local maximum or minimum\n",
    "    4. stochastic hill climbing : selects at random from among the uphill moves, the probability of selection can be based on the steepness of the uphill move\n",
    "- problems:\n",
    "    - local maxima : algorithm may get stuck at a local maximum or minimum and fail to find the global maximum or minimum\n",
    "    - plateaus : algorithm may get stuck at a plateau, where all neighboring solutions have the same objective function value\n",
    "    - oscillations : algorithm may oscillate between two or more solutions\n",
    "    - ridges : algorithm may get stuck at a ridge, where the target function ascends in a non-axis aligned direction\n",
    "        - since the algorithm can only move in one direction at a time, it may not be able to move to a better solution without zig-zagging\n",
    "        - hill climber may be forced to take very tiny steps to move along the ridge, taking unreasonable amount of time to reach the peak\n",
    "\n",
    "### Simulated Annealing [Link](https://en.wikipedia.org/wiki/Simulated_annealing)\n",
    "- variant of hill climbing that attempts to avoid getting stuck at local maxima or minima\n",
    "- inspired by the process of annealing in metallurgy, where a metal is heated and then slowly cooled to settle into a highly ordered crystal structure\n",
    "- objective : minimize the objective function, $f(\\bf{x})$\n",
    "- steps :\n",
    "    1. generate initial solution $\\bf{x}$\n",
    "    2. generate neighboring solution $\\bf{x'}$ by making a small change to $\\bf{x}$\n",
    "    3. if $f(\\bf{x'}) < f(\\bf{x})$, then $\\bf{x'}$ is the new solution\n",
    "    4. if $f(\\bf{x'}) > f(\\bf{x})$, then $\\bf{x'}$ is the new solution with probability $p = e^{\\frac{f(\\bf{x}) - f(\\bf{x'})}{T}}$, where $T$ is the temperature parameter\n",
    "    5. repeat steps 2 to 4 until no better solution can be found\n",
    "\n",
    "<img src=\"https://i.imgur.com/XYMOFpS.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "- the probability of moving to a worse solution decreases as the temperature decreases\n",
    "- the temperature parameter controls the probability of moving to a worse solution\n",
    "    - higher temperature : higher probability of moving to a worse solution\n",
    "    - lower temperature : lower probability of moving to a worse solution\n",
    "- temperature schedule : the rate at which the temperature parameter is decreased, typically exponential, by multiplying it by a constant factor $< 1$ at each iteration\n",
    "- the temperature parameter is typically initialized to a high value and gradually decreased over time\n",
    "- the algorithm is guaranteed to converge to the optimal solution if the temperature is decreased slowly enough\n",
    "\n",
    "<img src=\"https://i.imgur.com/za8V9oz.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "- problems:\n",
    "    - the algorithm may get stuck at a local maximum or minimum if the temperature is decreased too quickly\n",
    "    - the algorithm may take a long time to converge if the temperature is decreased too slowly\n",
    "- variants:\n",
    "    1. fast simulated annealing : uses a fast cooling schedule to decrease the temperature more quickly\n",
    "    2. very fast simulated re-annealing : uses a fast cooling schedule to decrease the temperature more quickly, and restarts the algorithm from a random initial solution if it gets stuck at a local maximum or minimum\n",
    "    3. parallel tempering : uses multiple copies of the algorithm with different temperature parameters, and allows the copies to exchange solutions with each other\n",
    "\n",
    "### Local Beam Search\n",
    "- an optimization of best first search that reduces memory requirements\n",
    "- explores by expanding the most promising nodes in a limited set of nodes, called the beam of width $k$\n",
    "- maintains a list of $k$ best states instead of all of them, as in best first search\n",
    "- uses breadth first search to build it's search tree\n",
    "    - at each iteration, all successors of states in the current beam are generated, sorted by their evaluation function, and the $k$ best states are selected to form the next beam\n",
    "- the greater the value of $k$, the more states are explored in parallel, but the more memory is required, lesser the states are pruned\n",
    "    - with infinite memory, local beam search is identical to breadth first search\n",
    "    - with $k = 1$, local beam search is identical to hill climbing\n",
    "- since goal state could be pruned, local beam search is not complete\n",
    "- not optimal either, since a better solution could be found in a different branch of the search tree that gets pruned\n",
    "- different from $k$ random restarts, since userful information is shared between states in the beam\n",
    "- problems:\n",
    "    - the beam may become concentrated in a small region of the search space, preventing the algorithm from exploring other regions\n",
    "        - variant called stochastic beam search can be used to address this problem : instead of choosing the $k$ best states, $k$ states are chosen at random from the successors of the current beam\n",
    "    - the beam may contain duplicate states, which can be avoided by using a hash table to store the states in the beam\n",
    "\n",
    "### Genetic Algorithm\n",
    "- variant of local search algorithm that is inspired by the process of natural selection\n",
    "- variant of stochastic beam search in which the successors of the current beam are generated by applying genetic operators to the states in the current beam\n",
    "- begin with a population of $k$ randomly generated states, called the initial population\n",
    "    - population consists of $k$ states, called individuals, each of which is a vector of decision variables\n",
    "    - each state is rated by an objective function $f(\\bf{x})$, which is used to determine the fitness of the state\n",
    "- genetic operators :\n",
    "    1. selection : selects the best states from the current beam to form the next beam\n",
    "    2. crossover : combines two states to form a new state\n",
    "    3. mutation : randomly modifies a state\n",
    "- steps :\n",
    "    1. generate initial population of $k$ states\n",
    "    2. evaluate fitness of each state\n",
    "    3. select random pairs of states from the current population\n",
    "        - apply crossover operator to each pair to generate two new states\n",
    "        - some locations may be subject to random mutation, with a small independent probability\n",
    "\n",
    "<img src=\"https://i.imgur.com/mhk2m51.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "\n",
    "### Ant Colony Optimization\n",
    "- variant of local search algorithm that is inspired by the foraging behavior of ants\n",
    "- artificial ants locate optimal solutions by moving through a parameter space representing all possible solutions\n",
    "- real ants deposit pheromones on the ground to mark paths between the colony and food sources\n",
    "    - pheromones evaporate over time, so paths that are not used frequently disappear\n",
    "    - ants are more likely to follow paths with higher pheromone concentrations\n",
    "- similarly, simulated ants record the positions and quality of solutions they have visited in a pheromone matrix\n",
    "- in the ACO algorithm:\n",
    "    - an artificial ant is a simple computational agent that searches for good solutions to a given optimization problem\n",
    "    - a colony is a set of artificial ants\n",
    "    - to apply, the optimization problem needs to be converted into the problem of finding the shortest path on a weighted graph\n",
    "- steps :\n",
    "    - each ant stochasticly constructs a solution to the problem, the order in which the edges in the graph should be followed\n",
    "    - the paths found by the ants are evaluated by an objective function\n",
    "    - update the pheromone matrix based on the quality of the solutions found by the ants\n",
    "- evaporation:\n",
    "    - a short path gets marched on more frequently, depositing more pheromone on it\n",
    "    - evaporation has the advantage of avoiding the convergence to a locally optimal solution\n",
    "- in general, \n",
    "    - the $k$ 'th ant goes from state $i$ to state $j$ with probability $p_{ij}^k$, given by:\n",
    "        $$ p_{ij}^k = \\frac{[\\tau_{ij}]^\\alpha * [\\eta_{ij}]^\\beta}{\\sum_{l \\in allowed} [\\tau_{il}]^\\alpha * [\\eta_{il}]^\\beta} $$\n",
    "    - where $\\tau_{ij}$ is the amount of pheromone on the edge from state $i$ to state $j$, $\\alpha$ is the parameter that controls the influence of pheromone, $\\eta_{ij}$ describes the desirability of the edge from state $i$ to state $j$, typically the inverse of the distance between the states, $\\beta$ is the parameter that controls the influence of desirability, and $allowed$ is the set of states that can be reached from state $i$\n",
    "    - the pheromone matrix is updated using the following formula:\n",
    "        $$ \\tau_{ij} \\leftarrow (1 - \\rho) * \\tau_{ij} + \\sum_{k = 1}^m \\Delta \\tau_{ij}^k $$\n",
    "    - where $\\rho$ is the pheromone evaporation rate, $\\Delta \\tau_{ij}^k$ is the amount of pheromone deposited by the $k$ 'th ant on the edge from state $i$ to state $j$\n",
    "        - $\\Delta \\tau_{ij}^k$ is given by:\n",
    "            $$ \\Delta \\tau_{ij}^k = \\begin{cases} \\frac{Q}{L_k} & \\text{if ant } k \\text{ uses edge } (i, j) \\text{ in its solution} \\\\ 0 & \\text{otherwise} \\end{cases} $$\n",
    "            - where $Q$ is a constant, $L_k$ is the length of the path found by the $k$ 'th ant\n",
    "    - usually updated when all ants have completed their tours, increasing or decreasing the pheromone level on each edge based on the quality of the solutions found by the ants\n",
    "- parameter tuning:\n",
    "    - high evaporation causes exploration, low evaporation causes exploitation \n",
    "    - high $\\alpha$ increases the weight of pheromone, increasing exploitation\n",
    "    - high $\\beta$ increases the weight of desirability, making shorter paths more attractive\n",
    "    - number of ants $m$ should be large enough to cover the search space, but not too large to avoid excessive computation\n",
    "\n",
    "### Particle Swarm Optimization\n",
    "- variant of local search algorithm that is inspired by the flocking behavior of birds\n",
    "- a swarm of particles move through a parameter space representing all possible solutions, each particle representing a potential solution\n",
    "- each particle has a position and velocity, and moves through the parameter space by updating its position and velocity based on its own best solution and the best solution found by the swarm\n",
    "- assume we have $P$ particles, each having a position $\\bf{x}_i$ and velocity $\\bf{v}_i$, where $i = 1, 2, ..., P$\n",
    "- for example, say $\\bf{x}_i = [x_i, y_i]$ and $\\bf{v}_i = [v_x, v_y]$\n",
    "- at the next iteration, the position and velocity is updated as follows:\n",
    "    $$ \\bf{x}_i \\leftarrow \\bf{x}_i + \\bf{v}_i $$\n",
    "    $$ \\bf{v}_i \\leftarrow \\omega \\bf{v}_i + c_1 r_1 (\\bf{p}_i - \\bf{x}_i) + c_2 r_2 (\\bf{g} - \\bf{x}_i) $$\n",
    "    where $\\omega$ is the inertia weight, $c_1$ and $c_2$ are the cognitive and social coefficients respectively, $r_1$ and $r_2$ are random numbers between 0 and 1, $\\bf{p}_i$ is the best position found by particle $i$, and $\\bf{g}$ is the best position found by the swarm\n",
    "- the cognitive coefficient $c_1$ controls the weight of the particle's best position, and the social coefficient $c_2$ controls the weight of the swarm's best position\n",
    "- the inertia weight $\\omega$ controls the impact of the previous velocity on the current velocity\n",
    "- the position and velocity of each particle is initialized randomly\n",
    "- exploration is encouraged by setting a high inertia weight, low cognitive coefficient, and low social coefficient\n",
    "- exploitation is encouraged by setting a low inertia weight, high cognitive coefficient, and high social coefficient\n",
    "- we usually run it for a fixed number of iterations, or until a solution of sufficient quality is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniMax Algorithm\n",
    "- minimax algorithm : algorithm for finding the optimal move in a two-player, zero-sum game\n",
    "- zero-sum game : a game in which the gain of one player is equal to the loss of the other player\n",
    "- assumes that the opponent plays optimally\n",
    "- is a depth-first search algorithm that uses a heuristic evaluation function to evaluate non-terminal states\n",
    "- is optimal, complete, and time and space complexity is $O(b^m)$, where $b$ is the branching factor and $m$ is the maximum depth of the search tree\n",
    "- is used in games like chess, checkers, tic-tac-toe, etc\n",
    "- At each step it assumes that player A (maximizing player) is trying to maximize the chances of A winning, while on the next turn player B (minimizing player) is trying to minimize the chances of A winning (i.e., to maximize B's own chances of winning)\n",
    "- The algorithm recursively searches the game tree, starting at the current state of the game and generating all possible moves for the current player. It then evaluates each of these moves by using the minimax algorithm to simulate a game that continues from that move until it reaches a terminal state. The move with the highest score is then selected.\n",
    "\n",
    "### Alpha-Beta Pruning\n",
    "- alpha-beta pruning : optimization of the minimax algorithm that reduces the number of nodes that are evaluated by the minimax algorithm in its search tree\n",
    "- alpha-beta pruning is based on the observation that since the opponent is trying to minimize the score, it is not necessary to evaluate all the nodes in the search tree\n",
    "- alpha-beta pruning maintains two values:\n",
    "    - $\\alpha$ : the minimum score that the maximizing player can guarantee at that level or above\n",
    "    - $\\beta$ : the maximum score that the minimizing player can guarantee at that level or above\n",
    "\n",
    "<img src=\"https://i.imgur.com/cuvlg8k.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Representation using Logics\n",
    "\n",
    "Terms:\n",
    "- syntax : the form of the expressions\n",
    "- semantics : the meaning of the expressions\n",
    "- model : a structure that satisfies the expressions\n",
    "    - $M \\models \\alpha$ means $M$ satisfies $\\alpha$, meaning that $\\alpha$ is true in model $M$\n",
    "    - $M \\not\\models \\alpha$ means $M$ does not satisfy $\\alpha$, meaning that $\\alpha$ is false in model $M$\n",
    "    - $M(\\alpha)$ means the set of models of $\\alpha$\n",
    "- satisfiable : an expression is satisfiable if it has a model that satisfies it\n",
    "- entailment : $\\alpha \\models \\beta$ means $\\alpha$ entails $\\beta$, meaning that $\\beta$ follows logically from $\\alpha$\n",
    "    - $\\alpha \\models \\beta$ iff $M(\\alpha) \\subseteq M(\\beta)$, ie. in every model in which $\\alpha$ is true, $\\beta$ is also true\n",
    "- model checking : given a model $M$ and an expression $\\alpha$, determine whether $M \\models \\alpha$\n",
    "- knowledge base : a set of expressions that represent knowledge about the world\n",
    "- logical reasoning : given a knowledge base $KB$ and an expression $\\alpha$, determine whether $KB \\models \\alpha$\n",
    "- inference : the process of deriving new expressions from old ones (using logical reasoning)\n",
    "\n",
    "Think of KB as a haystack and $\\alpha$ as a needle. We want to find out whether $\\alpha$ is in the haystack. Entailment is like the needle being in the haystack, inference is like finding the needle in the haystack.\n",
    "- If an inference algorithm $i$ can derive $\\alpha$ from $KB$, then $KB \\vdash_i \\alpha$, which is read as \"$\\alpha$ is derivable from $KB$ by algorithm $i$\"\n",
    "\n",
    "### Propositional Logic\n",
    "\n",
    "Syntax:\n",
    "- Propositional symbols: $P, Q, R, \\dots$\n",
    "- Connectives: $\\neg, \\wedge, \\vee, \\rightarrow, \\leftrightarrow$\n",
    "    - $\\neg$ : not, negation\n",
    "    - $\\wedge$ : and, conjunction, parts are called *conjuncts*\n",
    "    - $\\vee$ : or, disjunction, parts are called *disjuncts*\n",
    "    - $\\implies$ : implies, *premise / antecedent* $\\implies$ *conclusion / consequent*\n",
    "    - $\\iff$ : if and only if, biconditional\n",
    "- Parentheses: $(, )$\n",
    "- Operator precedence: $\\neg, \\wedge, \\vee, \\implies, \\iff$\n",
    "\n",
    "Semantics:\n",
    "- A model $M$ : simply fixes the truth value of each propositional symbol\n",
    "    - $M(P) = T$ means $P$ is true in model $M$\n",
    "    - $M(P) = F$ means $P$ is false in model $M$\n",
    "- Truth tables: a table that shows the truth value of a compound expression for every possible combination of truth values of its components\n",
    "    - $P \\wedge Q$ is true iff $P$ is true and $Q$ is true in model $M$\n",
    "    - $P \\vee Q$ is true iff $P$ is true or $Q$ is true in model $M$\n",
    "    - $P \\implies Q$ is true iff $P$ is false or $Q$ is true in model $M$\n",
    "    - $P \\iff Q$ is true iff $P$ and $Q$ have the same truth value in model $M$\n",
    "    - $\\neg P$ is true iff $P$ is false in model $M$\n",
    "\n",
    "#### Standard logical equivalences:\n",
    "Two sentences are logically equivalent if they are true in the same set of models. We write $\\alpha \\equiv \\beta$ to mean that $\\alpha$ and $\\beta$ are logically equivalent. $ \\alpha \\equiv \\beta$ means $\\alpha \\models \\beta$ and $\\beta \\models \\alpha$.\n",
    "- Commutativity: $P \\wedge Q \\equiv Q \\wedge P$ and $P \\vee Q \\equiv Q \\vee P$\n",
    "- Associativity: $(P \\wedge Q) \\wedge R \\equiv P \\wedge (Q \\wedge R)$ and $(P \\vee Q) \\vee R \\equiv P \\vee (Q \\vee R)$\n",
    "- Distributivity: $P \\wedge (Q \\vee R) \\equiv (P \\wedge Q) \\vee (P \\wedge R)$ and $P \\vee (Q \\wedge R) \\equiv (P \\vee Q) \\wedge (P \\vee R)$\n",
    "- De Morgan's laws: $\\neg (P \\wedge Q) \\equiv \\neg P \\vee \\neg Q$ and $\\neg (P \\vee Q) \\equiv \\neg P \\wedge \\neg Q$\n",
    "- Double negation: $\\neg \\neg P \\equiv P$\n",
    "- Contraposition: $P \\implies Q \\equiv \\neg Q \\implies \\neg P$\n",
    "- Biconditional: $P \\iff Q \\equiv (P \\implies Q) \\wedge (Q \\implies P)$\n",
    "- Absorption: $P \\vee (P \\wedge Q) \\equiv P$ and $P \\wedge (P \\vee Q) \\equiv P$\n",
    "- Implication elimination: $P \\implies Q \\equiv \\neg P \\vee Q$\n",
    "\n",
    "#### Checking entailment:\n",
    "- We wish to check whether $KB \\models \\alpha$ for some knowledge base $KB$ and some expression $\\alpha$. \n",
    "- first approach: model checking approach (pg. 248 AIMA)\n",
    "    - enumerate all possible models\n",
    "    - check whether each model that satisfies $KB$ also satisfies $\\alpha$\n",
    "    - if so, then $KB \\models \\alpha$\n",
    "    - if not, then $KB \\not\\models \\alpha$\n",
    "    - time complexity: $O(2^n)$, where $n$ is the number of propositional symbols\n",
    "\n",
    "#### Theorem proving:\n",
    "\n",
    "Validity: an expression is valid if it is true in all models. \n",
    "\n",
    "Satisfiability: an expression is satisfiable if it is true in some model. \n",
    "\n",
    "- apply rules of inference to $KB$ to derive $\\alpha$ without consulting any models\n",
    "- one property of logical systems is monotonicity: adding more sentences to a knowledge base cannot cause any previously entailed sentences to become unentailed\n",
    "    - if $KB \\models \\alpha$, then $(KB \\wedge \\beta) \\models \\alpha$ for any $\\beta$\n",
    "- Validity and satisfiability are connected: $\\alpha$ is valid iff $\\neg \\alpha$ is unsatisfiable, and $\\alpha$ is satisfiable iff $\\neg \\alpha$ is not valid.\n",
    "- proof by contradiction: to prove $\\beta$ from $KB$, assume $\\neg \\beta$ and derive a contradiction\n",
    "    - $\\alpha \\models \\beta$ iff the sentence $(\\alpha \\wedge \\neg \\beta)$ is unsatisfiable\n",
    "- modus ponens: $$ \\frac{\\alpha \\implies \\beta , \\alpha}{\\beta} $$\n",
    "- and elimination: $$ \\frac{\\alpha \\wedge \\beta}{\\alpha} $$\n",
    "- resolution: it takes a clause: a disjunction of literals, and another clause, and produces a resolvent: a new clause that is the disjunction of all the literals in the two clauses except for two complementary literals, which are removed. $$ \\frac{\\alpha \\vee \\beta, \\neg \\beta}{\\alpha} $$\n",
    "- unit resolution rule: $$ \\frac{v_1 \\vee \\dots \\vee v_n, \\neg v_i}{v_1 \\vee \\dots \\vee v_{i-1} \\vee v_{i+1} \\vee \\dots \\vee v_n} $$\n",
    "- full resolution rule: $$ \\frac{v_1 \\vee \\dots \\vee v_k, m_1 \\vee \\dots \\vee m_n}{v_1 \\vee \\dots \\vee v_{i-1} \\vee v_{i+1} \\vee \\dots \\vee v_k \\vee m_1 \\vee \\dots \\vee m_{j-1} \\vee m_{j+1} \\vee \\dots \\vee m_n} $$ where $v_i = \\neg m_j$\n",
    "\n",
    "#### Conjunction normal form (CNF):\n",
    "- every sentence of propositional logic is logically equivalent to a conjunction of clauses\n",
    "- steps for converting to CNF:\n",
    "    - eliminate biconditionals and implications by replacing $\\alpha \\iff \\beta$ with $(\\alpha \\implies \\beta) \\wedge (\\beta \\implies \\alpha)$ and $\\alpha \\implies \\beta$ with $\\neg \\alpha \\vee \\beta$\n",
    "    - move negation inwards using De Morgan's laws and double negation\n",
    "    - apply distributivity law to move $\\vee$ inwards over $\\wedge$\n",
    "- resolution theorem proving algorithm:\n",
    "    - convert $KB$ and $\\neg \\alpha$ to CNF\n",
    "    - apply resolution rule repeatedly to the clauses in $KB \\wedge \\neg \\alpha$ until either the empty clause is derived (in which case $KB \\models \\alpha$) or no new clauses can be derived (in which case $KB \\not\\models \\alpha$)\n",
    "- example CNF sentence:\n",
    "    $$ (P \\vee Q \\vee R) \\wedge (\\neg P \\vee \\neg Q) \\wedge (\\neg P \\vee \\neg R) \\wedge (\\neg Q \\vee \\neg R) \\wedge (P \\vee Q) \\wedge (P \\vee R) \\wedge (Q \\vee R) $$\n",
    "\n",
    "#### Horn clauses:\n",
    "- a definite clause is a disjunction of literals of which exactly one is positive $$ \\neg \\alpha_1 \\vee \\dots \\vee \\neg \\alpha_n \\vee \\beta $$\n",
    "- a Horn clause is a disjunction of literals of which at most one is positive $$ \\neg \\alpha_1 \\vee \\dots \\vee \\neg \\alpha_n \\vee \\beta $$\n",
    "    - interesting because it is equivalent to $$ (\\alpha_1 \\wedge \\dots \\wedge \\alpha_n) \\implies \\beta $$\n",
    "- a definite clause is a Horn clause, but not vice versa\n",
    "- a goal clause is a Horn clause with no positive literals $$ \\neg \\alpha_1 \\vee \\dots \\vee \\neg \\alpha_n $$\n",
    "\n",
    "#### Forward chaining:\n",
    "- `FC(KB, q)` determines if a single proposition symbol `q` can be inferred from a knowledge base `KB`\n",
    "- we start with the set of known facts, and repeatedly apply modus ponens to derive new facts\n",
    "- the process continues until either `q` is found, which means that `KB` entails `q`, or no new facts can be derived, which means that `KB` does not entail `q`\n",
    "- `FC` is sound and complete for definite clauses\n",
    "\n",
    "#### Backward chaining:\n",
    "- `BC(KB, q)` determines if a single proposition symbol `q` can be inferred from a knowledge base `KB`\n",
    "- we start with the goal `q`, and works backward through a set of rules and facts to determine whether the goal can be satisfied\n",
    "- if the goal is to be satisfied, it is done by satisfying all the subgoals that appear in the body of the rule\n",
    "\n",
    "| Forward Chaining | Backward Chaining |\n",
    "| --- | --- |\n",
    "| starts from known facts and applies inference rule to extract more data unit it reaches to the goal | starts from the goal and works backward through inference rules to find the required facts that support the goal |\n",
    "| bottom-up approach | top-down approach |\n",
    "| known as data-driven inference technique as we reach to the goal using the available data | known as goal-driven technique as we start from the goal and divide into sub-goal to extract the facts |\n",
    "| applies a breadth-first search strategy | applies a depth-first search strategy |\n",
    "| tests for all the available rules | only tests for few required rules |\n",
    "| operates in the forward direction | operates in the backward direction |\n",
    "| aimed for any conclusion | only aimed for the required data |\n",
    "\n",
    "#### DPLL algorithm:\n",
    "- a complete algorithm for deciding the satisfiability of propositional logic sentences in CNF\n",
    "- it is a backtracking DFS algorithm that uses unit propagation and pure symbol heuristic to speed up the search\n",
    "    - unit propagation: if a clause contains only one unassigned literal, then that literal must be assigned the value that makes the clause true\n",
    "    - pure symbol heuristic: a pure symbol is a symbol that always appears with the same \"sign\" in all clauses. A pure symbol can be assigned the value that makes all clauses containing it true\n",
    "- also checks for early termination conditions:\n",
    "    - if a clause is empty, then the partial model does not satisfy the sentence\n",
    "    - if all clauses are true, then the partial model satisfies the sentence\n",
    "\n",
    "<img src=\"https://i.imgur.com/5qrh1Lu.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### First Order Logic\n",
    "\n",
    "#### Syntax:\n",
    "- terms: constants, variables, functions\n",
    "    - Function symbols: $Function(Term, \\dots)$\n",
    "    - Constant symbols: $A, X_1, John, \\dots$\n",
    "    - Variable symbols: $a, y, z, \\dots$\n",
    "- predicates: $\\text{True}, \\text{BrotherOf}, \\text{LargerThan}, \\dots$\n",
    "    - Predicate symbols: $Predicate(Term, \\dots)$\n",
    "- quantifiers: $\\forall, \\exists$\n",
    "- connectives: $\\neg, \\wedge, \\vee, \\implies, \\iff$\n",
    "- parentheses: $( | )$\n",
    "- operator precedence: $\\neg, \\forall, \\exists, \\wedge, \\vee, \\implies, \\iff$\n",
    "\n",
    "#### Semantics:\n",
    "- A model $M$ : a set of objects and an interpretation that maps constant symbols to objects, predicate symbols to relations on those objects, and function symbols to functions on those objects\n",
    "    - $M(Predicate) = T$ means $Predicate$ is true in model $M$\n",
    "    - $M(Predicate) = F$ means $Predicate$ is false in model $M$\n",
    "- term : a constant symbol, a variable symbol, or a function symbol applied to a tuple of terms, eg. $\\text{John}$, $\\text{LeftLeg(John)}$\n",
    "- atomic sentence : a predicate symbol applied to a tuple of terms, eg. $\\text{BrotherOf(John, Mary)}$\n",
    "- complex sentence : a sentence with logical connectives, eg. $\\text{BrotherOf(John, Mary)} \\wedge \\text{BrotherOf(Mary, John)}$\n",
    "- quantified sentence : a sentence with quantifiers, eg. $\\forall x \\text{BrotherOf(x, Mary)}$\n",
    "    - e.g. \"All kings are persons\" is $\\forall x \\text{King}(x) \\implies \\text{Person}(x)$\n",
    "    - \"Everybody loves somebody\" is $\\forall x \\exists y \\text{ Loves}(x, y)$\n",
    "    - \"There is someone who is loved by everybody\" is $\\exists x \\forall y \\text{ Loves}(y, x)$\n",
    "- connection between quantifiers:\n",
    "    - $\\neg \\forall x \\alpha \\equiv \\exists x \\neg \\alpha$\n",
    "    - $\\neg \\exists x \\alpha \\equiv \\forall x \\neg \\alpha$\n",
    "\n",
    "#### Inference in first order logic:\n",
    "- universal instantiation: $$ \\frac{\\forall x \\alpha}{\\alpha[x/t]} $$ where $t$ is a term. We can substitute any term for $x$ in $\\alpha$.\n",
    "- existential instantiation: $$ \\frac{\\exists x \\alpha}{\\alpha[x/C_1]} $$ where $C_1$ is a constant symbol. We can substitute any constant symbol for $x$ in $\\alpha$, provided that the constant symbol does not appear elsewhere in the knowledge base.\n",
    "- unifications: a process of making two differenct logical atoms identical by substituting terms for variables\n",
    "    - `UNIFY(Knows(John, x), Knows(John, Jane)) = {x/Jane}`\n",
    "    - `UNIFY(Knows(John, x), Knows(y, Bill)) = {x/Bill, y/John}`\n",
    "    - `UNIFY(Knows(John, x), Knows(y, Mother (y))) = {y/John, x/Mother (John)}`\n",
    "    - `UNIFY(Knows(John, x), Knows(x,Elizabeth)) = fail`\n",
    "    - most general unifier (mgu): a unifier that is more general than any other unifier\n",
    "        - `UNIFY(Knows(John, x), Knows(y, z)) = {y/John, x/z}` or `UNIFY(Knows(John, x), Knows(y, z)) = {y/John, x/John, z/John}`\n",
    "        - in this case, the `{y/John, x/z}` is more general\n",
    "    \n",
    "#### Resolution in first order logic:\n",
    "- convert to CNF form:\n",
    "    - eliminate biconditionals and implications by replacing $\\alpha \\iff \\beta$ with $(\\alpha \\implies \\beta) \\wedge (\\beta \\implies \\alpha)$ and $\\alpha \\implies \\beta$ with $\\neg \\alpha \\vee \\beta$\n",
    "    - move negation inwards using $\\neg \\forall x \\alpha \\equiv \\exists x \\neg \\alpha$ and $\\neg \\exists x \\alpha \\equiv \\forall x \\neg \\alpha$\n",
    "    - standardize variables apart by renaming variables to make them unique\n",
    "    - skolemize existential quantifiers by replacing them with Skolem functions\n",
    "        - $∀ x [∃ y Animal(y) ∧ ¬Loves(x, y)] ∨ [∃ z Loves(z, x)]$ becomes $∀ x [Animal(F(x)) ∧ ¬Loves(x, F(x))] ∨ Loves(G(z), x)$ where $F$ and $G$ are Skolem functions\n",
    "    - drop universal quantifiers\n",
    "    - distribute $\\vee$ over $\\wedge$\n",
    "\n",
    "#### Forward chaining in FOL:\n",
    "<img src=\"https://i.imgur.com/O44ajFJ.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "#### Backward chaining in FOL:\n",
    "<img src=\"https://i.imgur.com/Jxx9KIZ.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Networks\n",
    "- In general, joint distribution P over set of variables $X_1, \\dots, X_n$ can be represented by a table of size $2^n$, requiring exponential space for representation and inference\n",
    "- BN's provide a compact representation of joint distributions by exploiting conditional independence relations in $P$.\n",
    "    - usually more compact than full joint distribution\n",
    "    - can be used to efficiently compute conditional probabilities\n",
    "    - BN's are directed acyclic graphs (DAG's) whose nodes represent random variables and whose edges represent direct dependencies between the variables\n",
    "    - each node $X_i$ has a conditional probability distribution $P(X_i | Parents(X_i))$ that quantifies the effect of the parents on the node\n",
    "    - for discrete variables, each node has a conditional probability table (CPT) that specifies the conditional probability of each value of the node given each combination of values of its parents\n",
    "\n",
    "<img src=\"https://i.imgur.com/nydJpPT.png\" width=\"300\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "- weather is independent of the other variables\n",
    "- toothache and catch are conditionally independent given cavity\n",
    "\n",
    "### Bayesian Networks Factorization\n",
    "\n",
    "Example:\n",
    "- Question : Is your home being burglarized?\n",
    "- Variables : $B$ (burglary), $E$ (earthquake), $A$ (alarm), $J$ (John calls), $M$ (Mary calls)\n",
    "<img src=\"https://i.imgur.com/Ulv1iQs.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "- if we know alarm, no other evdiences are needed to determine out degree of belief in JohnCalls\n",
    "    - $P(J | M, A, B, E) = P(J | A)$ and $P(M | J, A, B, E) = P(M | A)$ and P(E | B) = P(E)\n",
    "- by chain rule we have $P(J, M, A, B, E) = P(J | M, A, B, E) P(M | A, B, E) P(A | B, E) P(B | E) P(E) = P(J | A) P(M | A) P(A | B, E) P(B) P(E)$\n",
    "- thus, full joint distribution requires only 10 numbers instead of 32\n",
    "- in general, full joint distribution $$ P(X_1, \\dots, X_n) = \\prod_{i=1}^n P(X_i | Parents(X_i)) $$\n",
    "\n",
    "#### General principles:\n",
    "- Given the parents, $X_i$ is conditionally independent of all it's non descendants\n",
    "- A node $X_i$ is conditionally independent of all other nodes in the network given the Markov blanket of the node $MB(X_i)$, i.e. given the node's parents, children, and children's parents $MB(X_i) = Parents(X_i) \\cup Children(X_i) \\cup Parents(Children(X_i))$\n",
    "<img src=\"https://i.imgur.com/iDIlFa5.png\" width=\"600\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "#### d-separation:\n",
    "- an undirected path between two nodes $X$ and $Y$ is cut off if:\n",
    "    - information cannot flow from $X$ to $Y$ through the path\n",
    "- two nodes are d-separated if all paths between them are cut off\n",
    "- two sets of nodes $X$ and $Y$ are d-separated if all paths between any node in $X$ and any node in $Y$ are cut off\n",
    "- two sets of nodes are independent if they are d-separated\n",
    "- three rules to check d-separation:\n",
    "    - Linear connection: $A \\rightarrow B \\rightarrow C$, information can flow between $A$ and $C$ iff we do not have evidence on $B$\n",
    "    - Diverging connection: $A \\leftarrow B \\rightarrow C$, information can flow between $A$ and $C$ iff we do not have evidence on $B$\n",
    "    - Converging connection: $A \\rightarrow B \\leftarrow C$, information can flow between $A$ and $C$ iff we have evidence on $B$ or any of its descendants\n",
    "\n",
    "#### Bayesian Networks Inference\n",
    "- the graphical independence relations in a BN can be exploited to perform inference more efficiently\n",
    "- inference in BN's is:\n",
    "    - the task of computing the posterior distribution $P(Z | E)$, where $Z$ is a set of query variables and $E$ is a set of evidence variables\n",
    "    - the task of computing the marginal probability of a subset of the query variables given the evidence variables\n",
    "    - the task of computing the most probable explanation (MPE) of the query variables given the evidence variables\n",
    "- examples:\n",
    "    - say we are trying to compute $P(B | j, m)$\n",
    "    - lowercase letters means we know the value of the variable, uppercase means we don't\n",
    "    - $P(B | j, m) = \\frac{P(B, j, m)}{P(j, m)}$ where $P(j, m) = \\sum_b P(B, j, m)$\n",
    "    - $P(B | j, m) = \\alpha P(B, j, m) = \\alpha \\sum_E \\sum_A P(B, E, A, j, m) = \\alpha \\sum_E \\sum_A P(B) P(E) P(A | B, E) P(j | A) P(m | A)$\n",
    "        - called marginalizing over the hidden variables $A$ and $E$, or summing over\n",
    "    - $P(B | j, m) = \\alpha P(B) \\sum_E P(E) \\sum_A P(A | B, E) P(j | A) P(m | A)$\n",
    "\n",
    "<img src=\"https://i.imgur.com/bOYUnLb.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "    - computation graph for B = True, J = True, M = True\n",
    "    - observe that there are redundant computations\n",
    "    - start doing variable elimination from the bottom\n",
    "\n",
    "#### Variable elimination:\n",
    "- a factor is a function from a set of variables into a specific value: eg. $f(E, A, N1)\n",
    "    - CPT's are factors, i.e. $P(A | B, E)$ is a function from $A, B, E$\n",
    "- VE works by eliminating one variable at a time until there is a factor with only one query variable left\n",
    "- to eliminate a variable:\n",
    "    - join all factors that mention the variable\n",
    "    - sum out the variable\n",
    "    - exploits product form of joint distribution\n",
    "<img src=\"https://i.imgur.com/oMT8jDC.png\"  width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "For our particular example:\n",
    "- $P(B | j, m) = \\alpha P(B) \\sum_E P(E) \\sum_A P(A | B, E) P(j | A) P(m | A)$\n",
    "- $P(B | j, m) = \\alpha P(B) \\sum_E P(E) \\sum_A P(A | B, E) f1(A)$ where $f1(A) = P(j | A) P(m | A)$, create table with columns $A, f1(A)$, rows $a, \\hat{a}$\n",
    "- $P(B | j, m) = \\alpha P(B) \\sum_E P(E) f2(B, E)$ where $f2(B, E) = \\sum_A P(A | B, E) f1(A)$, create table with columns $B, E, f2(B, E)$, rows $(b, e), (\\hat{b}, e), (b, \\hat{e}), (\\hat{b}, \\hat{e})$\n",
    "- $P(B | j, m) = \\alpha f3(B)$ where $f3(B) = P(B) \\sum_E P(E) f2(B, E)$, create table with columns $B, f3(B)$, rows $b, \\hat{b}$\n",
    "- go from $\\alpha f3(B)$ to $P(B | j, m)$ by normalizing\n",
    "\n",
    "#### Rejection sampling:\n",
    "- a simple Monte Carlo algorithm for computing probabilities in BN's\n",
    "- given a query variable $X$ and evidence $E$, we can estimate $P(X | E)$ by sampling from the joint distribution $P(X, E)$ and counting the fraction of samples that satisfy $E$\n",
    "- the algorithm is correct because the fraction of samples that satisfy $E$ is equal to $P(E)$, and the fraction of samples that satisfy $E$ and $X$ is equal to $P(X, E)$\n",
    "- we sample from the prior\n",
    "    - the algorithm is inefficient because most samples will be rejected if $E$ is unlikely\n",
    "- returns consistent posterior estimates as the number of samples increases\n",
    "\n",
    "#### Likelihood weighting:\n",
    "- in rejection sampling, we sample from the prior and reject samples that do not match the evidence\n",
    "    - however, we can do better by sampling from the posterior directly in which evidence variables are fixed to their observed values\n",
    "- idea:\n",
    "    - each sample agrees with the evidence\n",
    "    - pays some price for the agreement (weight)\n",
    "- algorithm:\n",
    "    - fix evidence variables to their observed values\n",
    "    - sample only the non-evidence variables\n",
    "    - weight each sample by the product of the conditional probabilities of the evidence variables given their parents\n",
    "\n",
    "<img src=\"https://i.imgur.com/0PMWwAx.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Reasoning over Time\n",
    "\n",
    "### Markov Models\n",
    "- states and observations:\n",
    "    - $X_0, X_1, \\dots, X_t$ are the states of the system at time $t$\n",
    "    - $E_1, E_2, \\dots, E_t$ are the observations of the system at time $t$\n",
    "- transition model: it specifies the probability of the next state given the previous states\n",
    "    - $P(X_{t} | X_{0:t-1})$\n",
    "- assumption: the current state depends only on a finite history of previous states\n",
    "    - $P(X_{t} | X_{0:t-1}) = P(X_{t} | X_{t-1})$ for first order Markov model\n",
    "    - $P(X_{t} | X_{0:t-1}) = P(X_{t} | X_{t-1}, X_{t-2})$ for second order Markov model.. and so on\n",
    "- sensor model / observation model : it specifies the probability of the observation given the current state\n",
    "    - $P(E_{t} | X_{0:t-1}, E_{0:t-1}) = P(E_{t} | X_{t})$ since any state worth its salt should be able to predict the current observation given the current state\n",
    "\n",
    "#### Inferences in Markov Models\n",
    "\n",
    "Consult slides, though it has much easier problems than the book does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinfocement Learning\n",
    "\n",
    "### Markov Decision Processes\n",
    "- give us a way to formalize sequential decision making\n",
    "- this formalization is the basis for structuring problems that are to be solved using reinforcement learning\n",
    "- components in MDP:\n",
    "    - states: $S$\n",
    "    - actions: $A$\n",
    "    - transition model: $T(s, a, s') = P(s' | s, a)$\n",
    "    - reward function: $R(s, a, s')$\n",
    "    - discount factor: $\\gamma$\n",
    "- we have a decision maker, called an agent, that interacts with the environment in discrete time steps\n",
    "- at each time step $t$, the agent receives a representation of the environment's state $S_t \\in S$, and on that basis selects an action $A_t \\in A$\n",
    "- the environment transtions to a new state $S_{t+1}$, and the agent receives a reward $R_{t+1}$\n",
    "- the process continues until a terminal state is reached\n",
    "    - note that $t+1$ is no longer the future state, but the current state\n",
    "\n",
    "<img src=\"https://i.imgur.com/bYSnjk5.png\" width=\"500\" style=\"display: block; padding: 10px; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Terms in Reinforcement Learning\n",
    "- transition probability: $P(s', r | s, a) = P(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ for all $s, s' \\in S, a \\in A(s), r \\in R$\n",
    "- return: $G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$\n",
    "    - discounted sum of future rewards, where $\\gamma \\in [0, 1]$ is the discount factor and $t+k+1$ is the final time step\n",
    "    - $$ G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} $$\n",
    "- policy: a function that maps a given state to a probability distribution over actions\n",
    "    - $\\pi(a | s) = P(A_t = a | S_t = s)$\n",
    "    - $\\pi$ is a stationary policy if $\\pi(a | s) = \\pi(a | s')$ for all $s, s' \\in S$ and $a \\in A(s)$\n",
    "    - $\\pi$ is a deterministic policy if $\\pi(a | s) = 1$ for some $a \\in A(s)$ and $\\pi(a' | s) = 0$ for all $a' \\neq a$\n",
    "- value function: are functions of states, or state-action pairs, that estimate how good it is for the agent to be in a given state, or how good it is to perform a given action in a given state\n",
    "    - state value function: $v_\\pi(s) = E_\\pi[G_t | S_t = s]$ tells us how good it is for the agent to be in a given state $s$ following policy $\\pi$\n",
    "        - formally it is the expected return starting from state $s$ at time $t$ and following policy $\\pi$\n",
    "        - $$ v_\\pi(s) = E_\\pi[G_t | S_t = s] = E_\\pi[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t = s] $$\n",
    "    - state-action value function: $q_\\pi(s, a) = E_\\pi[G_t | S_t = s, A_t = a]$ tells us how good it is for the agent to perform a given action $a$ in a given state $s$ following policy $\\pi$\n",
    "        - formally it is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "        - $$ q_\\pi(s, a) = E_\\pi[G_t | S_t = s, A_t = a] = E_\\pi[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} | S_t = s, A_t = a] $$\n",
    "- optimal policy: a policy that maximizes the value function\n",
    "    - $\\pi^* = \\arg \\max_\\pi v_\\pi(s)$ for all $s \\in S$\n",
    "    - $\\pi^* = \\arg \\max_\\pi q_\\pi(s, a)$ for all $s \\in S$ and $a \\in A(s)$\n",
    "\n",
    "#### Bellman optimality equation:\n",
    "- the optimal value function $v_*(s)$ must satisfy the following self-consistency condition:\n",
    "    - $$ v_*(s) = \\max_a \\sum_{s', r} P(s', r | s, a) [r + \\gamma v_*(s')] = \\max_a E[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$\n",
    "    - intuitively, the value of a state under an optimal policy must equal the expected return for the best action from that state\n",
    "- the optimal state-action value function $q_*(s, a)$ must satisfy the following self-consistency condition:\n",
    "    - $$ q_*(s, a) = \\sum_{s', r} P(s', r | s, a) [r + \\gamma \\max_{a'} q_*(s', a')] = E[R_{t+1} + \\gamma \\max_{a'} q_*(s', a') | S_t = s, A_t = a] $$\n",
    "    - intuitively, the value of taking an optimal action in a state must equal the expected return for the best policy from that state\n",
    "\n",
    "#### Filling Q-table\n",
    "- use equation $q_*(s, a) = E[R_{t+1} + \\gamma \\max_{a'} q_*(s', a')]$ to fill the Q-table\n",
    "- $q_{new}(s, a) = (1 - \\alpha) q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} q(s', a')]$\n",
    "    - $\\alpha$ is the learning rate\n",
    "    - $q_{new}(s, a)$ is the new value of $q(s, a)$\n",
    "    - $q(s, a)$ is the old value of $q(s, a)$\n",
    "    - $R_{t+1} + \\gamma \\max_{a'} q(s', a')$ is the learned value\n",
    "- over time, the Q-table will converge to the optimal Q-table on the right side of the equation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
