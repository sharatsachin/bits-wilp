{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance based learning\n",
    "\n",
    "- systems that learn the training examples by heart and then generalizes to new instances based on some similarity measure\n",
    "- called instance-based because it builds the hypotheses from the training instances\n",
    "- also called lazy learning, memory-based learning, or case-based reasoning\n",
    "- time complexity is $O(n)$ where $n$ is the number of training instances\n",
    "\n",
    "### k-Nearest Neighbors (kNN) \n",
    "- non-parametric method, supervised learning\n",
    "- used for classification\n",
    "    - object is classified by a majority vote of its $k$ nearest neighbors\n",
    "    - if $k=1$, then the object is simply assigned to the class of that single nearest neighbor\n",
    "- used for regression\n",
    "    - object's value is estimated by the average of its $k$ nearest neighbors\n",
    "- $k$ is a hyperparameter that is usually chosen by cross-validation\n",
    "- kNN is sensitive to the local structure of the data\n",
    "    - if the data is not uniformly sampled, then the nearest neighbors will not be representative of the entire data set\n",
    "    - in this case, the decision boundary will be irregular\n",
    "- kNN is sensitive to the distance metric used (Euclidean, Manhattan, Minowski, etc.)\n",
    "- drawback when class distribution is skewed\n",
    "    - if one class is much more frequent than the others, then the nearest neighbors will be dominated by the most frequent class\n",
    "    - solution: use weighted voting, where the weights are the inverse of the distance to the query point\n",
    "\n",
    "### Locally Weighted Regression (LWR)\n",
    "- memory-based method that performs a regression around a point of interest using only training data that are local to that point\n",
    "- non-parametric method (parameters are computed individually for each query point)\n",
    "- cost function is modified as $$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m w^{(i)} (h_\\theta(x^{(i)}) - y^{(i)})^2$$ where $w^{(i)} = \\exp \\left( - \\frac{(x^{(i)} - x)^2}{2\\tau^2} \\right)$ and $\\tau$ is the bandwidth parameter that controls the degree of smoothing\n",
    "    - if $(x^{(i)} - x)^2$ is small, then $w^{(i)}$ is close to 1, and vice versa\n",
    "- training data must be available at the time of prediction\n",
    "\n",
    "### Kernel function\n",
    "- for linear regression, the hypothesis is $h_\\theta(x) = \\theta^T x$, the dot product is integral to the prediction operation\n",
    "- suppose the vectors are not linearly separable, then we can use a function $\\phi(x)$ to map the vectors to a higher dimensional space where they are linearly separable\n",
    "- kernel function is a function that maps a function of the vectors in the original space to a dot product of the vectors in a higher dimensional space\n",
    "- Let $\\phi(x) : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ be a function that maps $x$ from 2D to 3D space, and the kernel function is defined as $K(x, x^*) = \\phi(x) \\cdot \\phi(x^*)$\n",
    "- Consider $x = [x_1, x_2]$,  $x^* = [x^*_1, x^*_2]$, then $\\phi(x) = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]$ and $\\phi(x^*) = [x_1^{*2}, \\sqrt{2}x_1^*x_2^*, x_2^{*2}]$\n",
    "- Here $\\phi(x) \\cdot \\phi(x^*) = x_1^2x_1^{*2} + 2x_1x_2x_1^*x_2^* + x_2^2x_2^{*2} = (x_1x_1^* + x_2x_2^*)^2 = (x \\cdot x^*)^2$\n",
    "- So $K(x, x^*) = (x \\cdot x^*)^2$ is a kernel function, and we were able to perform the dot product in a higher dimensional space without explicitly computing $\\phi(x)$ and $\\phi(x^*)$, which is computationally expensive operation\n",
    "- $x \\rightarrow \\phi(x)$, $x^* \\rightarrow \\phi(x^*)$, then $x \\cdot x^* \\rightarrow K(x, x^*) = \\phi(x) \\cdot \\phi(x^*)$\n",
    "\n",
    "### Radial Basis Functions (RBF) \n",
    "- The basic idea behind RBFs is to model the data using a set of basis functions, where each basis function represents a localized influence on the data.\n",
    "- a real-valued function $\\varphi$ whose value depends only on the distance from a fixed point\n",
    "    - point is either the origin, so that $\\varphi(\\mathbf{x}) = \\hat{\\varphi}(\\|\\mathbf{x}\\|)$\n",
    "    - a fixed point $\\mathbf{c}$, called a center, so that $\\varphi_c(\\mathbf{x}) = \\hat{\\varphi}(\\|\\mathbf{x} - \\mathbf{c}\\|)$\n",
    "- examples of RBFs:\n",
    "    - Gaussian: $\\varphi(r) = e^{{-(\\epsilon r)}^2}$ where $r = \\|\\mathbf{x} - \\mathbf{c}\\|$ and $\\epsilon$ is a shape parameter\n",
    "    - Multiquadric: $\\varphi(r) = \\sqrt{1 + (\\epsilon r)^2}$\n",
    "    - Inverse quadratic: $\\varphi(r) = \\frac{1}{1 + (\\epsilon r)^2}$\n",
    "    - Inverse multiquadric: $\\varphi(r) = \\frac{1}{\\sqrt{1 + (\\epsilon r)^2}}$\n",
    "    - Thin plate spline: $\\varphi(r) = r^2 \\ln(r)$\n",
    "    - Cubic: $\\varphi(r) = r^3$\n",
    "    - Wendland $\\varphi(r) = (1 - \\epsilon r)^4_+ (4\\epsilon r + 1)$\n",
    "- used to build function approximations of the form $$y(\\mathbf{x}) = \\sum_{i=1}^N w_i \\varphi(\\|\\mathbf{x} - \\mathbf{x}_i\\|)$$\n",
    "    - approximating function is represented as a sum of $N$ radial basis functions, each associated with a different center $\\mathbf{x}_i$, and weighted by an appropriate coefficient $w_i$\n",
    "    - weights $w_i$ can be estimated using the matrix methods of linear least squares, because the approximating function is linear in the weights $w_i$\n",
    "- numerical: finding target value for a new point\n",
    "    - say the data points are [1, 2, 3, 6, 7] and their targets [4, 6, 2, 10, 8]\n",
    "    - new point is $x_{new} = 4$\n",
    "    - choosing Gaussian RBF, $\\varphi(r) = e^{{-(\\epsilon r)}^2}$\n",
    "    - for simplicity, let $\\epsilon = 1$\n",
    "    - choose the data points themselves as the centers, so $\\mathbf{x}_i = [1, 2, 3, 6, 7]$\n",
    "    - RBF for each center using the formula is $\\varphi(x_{new}, c_i) = e^{-1 \\times (x_{new} - c_i)^2}$\n",
    "    - $predicted\\_target = \\frac{\\sum_{i=1}^N target(c_i) \\times \\varphi(x_{new}, c_i)}{\\sum_{i=1}^N \\varphi(x_{new}, c_i)}$\n",
    "- numerical: interpolation function that passes through the data points\n",
    "    - say the data points are (x, y): (1, 2) (2, 3) (3, 4) (4, 5) (5, 6)\n",
    "    - choose 3 centers: $c_1 = 1, c_2 = 3, c_3 = 5$\n",
    "    - choose Gaussian RBF, $\\varphi_i(r) = e^{{-(\\epsilon ||x - c_i||)}^2}$ where $\\epsilon = 1$\n",
    "    - then interpolation function, $F(x) = \\sum_{i=1}^3 w_i \\varphi_i(x, c_i)$\n",
    "    - to find the weights, we need to solve the system of linear equations\n",
    "        1. $2 = w_1 \\varphi_1(1, 1) + w_2 \\varphi_2(1, 3) + w_3 \\varphi_3(1, 5)$\n",
    "        2. $3 = w_1 \\varphi_1(2, 1) + w_2 \\varphi_2(2, 3) + w_3 \\varphi_3(2, 5)$ and so on...\n",
    "\n",
    "\n",
    "### RBF Network\n",
    "- fundamental idea is that an item's predicted target value is likely to be the same as other items with close values of predictor variables\n",
    "- places one or many RBF neurons in the space described by the predictor variables\n",
    "- space has multiple dimensions corresponding to the number of predictor variables present\n",
    "- calculates the Euclidean distance from the evaluated point to the center of each neuron\n",
    "- RBF (kernel function) is applied to the distance to calculate every neuron's weight (influence)\n",
    "- the greater the distance of a neuron from the point being evaluated, the less influence (weight) it has\n",
    "- predict value for new points by adding the output values of RBF functions applied to the distance between the new point and the center of each neuron multiplied by the weight of each neuron\n",
    "- RBF network is a three-layer neural network\n",
    "    - input layer: neurons that receive the input values\n",
    "    - hidden layer: neurons that apply the RBF function to the distance between the input values and the center of each neuron\n",
    "    - output layer: neurons that sum the output values of the hidden layer neurons multiplied by the weight of each neuron $$ y(\\mathbf{x}) = \\sum_{i=1}^N w_i \\varphi(\\|\\mathbf{x} - \\mathbf{x}_i\\|)$$\n",
    "- the approximant $y(\\mathbf{x})$ is differentiable with respect to the weights $w_i$, hence the weights can be estimated using any of the standard iterative methods for neural networks\n",
    "- numerical: RBF network, you have output value for a few 2D points\n",
    "    - method is same as interpolation (pg. 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM) [ðŸ”—](https://drive.google.com/file/d/12KgpHBHalf4WFJHsVfbim1dQDdVyPk8d/view?usp=drive_link)\n",
    "- maps training examples to points in space so as to maximise the width of the gap between the two categories\n",
    "- new examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall\n",
    "- SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces\n",
    "- [Link](https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/)\n",
    "\n",
    "The functional margin is represented as $\\hat{\\gamma}$ and the geometric margin is represented as $\\gamma$. The geometric margin can be expressed as $\\gamma = \\frac{\\hat{\\gamma}}{||w||}$, where $w$ is the weight vector. So, the geometric margin is a scaled version of the functional margin. Here $\\hat{\\gamma} = y_i(w^Tx_i + b)$. The functional margin represents the correctness and confidence of the prediction if the magnitude of the $w^T$ orthogonal to the hyperplane has a constant value all the time.\n",
    "\n",
    "The functional margin gives the position of a point with respect to the hyperplane, which does not depend on the magnitude. The geometric margin is a scaled version of the functional margin and gives the distance between a given training example and the given hyperplane. It is invariant to the scaling of the vector orthogonal to the hyperplane.\n",
    "\n",
    "The optimization equation for hard margin SVM is to maximize the margin between two classes subject to the constraint that all data points are classified correctly. The margin is defined as the distance between two parallel hyperplanes that separate the two classes. The optimization problem can be expressed as minimizing $\\frac{1}{2}||w||^2$ subject to the constraint $y_i(w^Tx_i + b) \\geq 1$ for all data points $i$. [Video](https://www.youtube.com/watch?v=vNt_WCM1M3M&list=PLAoF4o7zqskR7U98D799FKHkZ4YrHKPqs&index=82)\n",
    "\n",
    "The primal optimization problem for SVM is $$ L(w, b, \\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^n \\alpha_i[y_i(w^Tx_i + b) - 1] $$\n",
    "\n",
    "The dual optimization problem for SVM is \n",
    "$$ \\Theta_D(\\alpha) = \\min_{w, b} L(w, b, \\alpha) $$\n",
    "\n",
    "Solving for $w$ and $b$ in the dual optimization problem gives $$ \\Theta_D(\\alpha) = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j <x_i^T x_j> $$\n",
    "\n",
    "This gives us the dual problem, which is a quadratic optimization problem. The dual problem is easier to solve than the primal problem because it is a convex optimization problem. It is given by $$ \\max_{\\alpha} \\Theta_D(\\alpha) $$ subject to $ \\sum_{i=1}^n \\alpha_i y_i = 0 $ and $ \\alpha_i \\geq 0 $ for all $i$.\n",
    "\n",
    "So we can first solve for $\\alpha$ using the dual optimization problem, and then solve for $w$ and $b$ using the values of $\\alpha$.\n",
    "\n",
    "#### Kernel Trick\n",
    "\n",
    "Kernel functions are functions that map data points from a low-dimensional space to a higher-dimensional space. The kernel function is used to transform the data into a higher-dimensional space so that the data becomes linearly separable. The kernel function is defined as $$ K(x_i, x_j) = \\phi(x_i)^T \\phi(x_j) $$ where $\\phi(x_i)$ is the transformed data point. The kernel function is a dot product between the transformed data points.\n",
    "\n",
    "Equation of linear kernel function is $$ K(x_i, x_j) = x_i^T x_j $$\n",
    "Equation of polynomial kernel function is $$ K(x_i, x_j) = (x_i^T x_j + c)^d $$\n",
    "Equation of radial basis function kernel function is $$ K(x_i, x_j) = \\exp(\\gamma ||x_i - x_j||^2) $$\n",
    "    - small value of $\\gamma$ will make the model behave like a linear SVM\n",
    "    - large value of $\\gamma$ will make the model heavily impacted by the support vectors examples\n",
    "Equation of sigmoid kernel function is $$ K(x_i, x_j) = \\tanh(\\beta x_i^T x_j + \\theta) $$\n",
    "\n",
    "#### Soft margin SVM\n",
    "\n",
    "The optimization problem is given by $$ \\min_{w, b} \\frac{1}{2}||w||^2 + C \\sum_{i=1}^n \\xi_i $$ subject to $ y_i(w^Tx_i + b) \\geq 1 - \\xi_i $ and $ \\xi_i \\geq 0 $ for all $i$.\n",
    "\n",
    "In soft margin SVM, a slack variable $\\xi_i$ is introduced for every data point $x_i$. The value of $\\xi_i$ is the distance of $x_i$ from the corresponding classâ€™s margin if $x_i$ is on the wrong side of the margin, otherwise zeroÂ¹. This allows some misclassifications to happen while keeping the margin as wide as possible so that other points can still be classified correctly.\n",
    "\n",
    "#### Regularization parameter C\n",
    "- determines how important $xi$ should be\n",
    "    - smaller $C$ emphasizes the importance of $xi$\n",
    "    - larger $C$ diminishes the importance of $xi$\n",
    "- controls how the SVM will handle errors\n",
    "    - if $C$ is positive infinite, then we will get the same result as the hard margin SVM\n",
    "    - if $C$ is 0, then there will be no constraint anymore, and we will end up with a hyperplane not classifying anything\n",
    "- small values of $C$ will result in a wider margin, at the cost of some misclassifications\n",
    "- large values of $C$ will give you the hard margin classifier and tolerates zero constraint violation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "- Naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features.\n",
    "- We have a set of features $X = {X_1, X_2, ..., X_n}$ and a class variable $Y$.\n",
    "- We want to find the class $Y$ that maximizes the posterior probability $P(Y|X)$.\n",
    "- Then $$ P(Y = y_k|X_1, X_2, ..., X_n) = \\frac{P(Y = y_k)P(X_1, X_2, ..., X_n|Y = y_k)}{\\sum\\limits_{j} P(Y = y_j)P(X_1, X_2, ..., X_n|Y = y_j)} $$\n",
    "- Assuming conditional independence, we have $P(X_1, X_2, ..., X_n|Y = y_k) = \\prod\\limits_{i=1}^n P(X_i|Y = y_k)$. Therefore, $$P(Y = y_k|X_1, X_2, ..., X_n) = \\frac{P(Y = y_k)\\prod\\limits_{i=1}^n P(X_i|Y = y_k)}{\\sum\\limits_{j} P(Y = y_j)\\prod\\limits_{i=1}^n P(X_i|Y = y_j)}$$\n",
    "- Pick the most probable class: $$\\hat{y} = \\arg\\max\\limits_{y_k} P(Y = y_k)\\prod\\limits_{i} P(X_i|Y = y_k)$$\n",
    "\n",
    "Steps to apply Naive Bayes classifier, given a table like this:\n",
    "|Weather|Play|\n",
    "|---|---|\n",
    "|Sunny|No|\n",
    "|...|...|\n",
    "|Rainy|Yes|\n",
    "\n",
    "We convert it into a frequency table like this:\n",
    "|Weather|No|Yes|Total|Prob|\n",
    "|---|---|---|---|---|\n",
    "|Sunny|2|3|5| P(Sunny) = $\\frac{5}{14}$|\n",
    "|Overcast|0|4|4| P(Overcast) = $\\frac{4}{14}$|\n",
    "|Rainy|3|2|5| P(Rainy) = $\\frac{5}{14}$|\n",
    "|Total|5|9|14|1|\n",
    "|Prob|P(No) = $\\frac{5}{14}$|P(Yes) = $\\frac{9}{14}$|1|\n",
    "\n",
    "Then we can calculate the posterior probability of each class, given the evidence (weather), for example, $P(Yes|Sunny)$:\n",
    "$$ P(Yes|Sunny) = \\frac{P(Sunny|Yes)P(Yes)}{P(Sunny)} = \\frac{\\frac{3}{9}\\frac{9}{14}}{\\frac{5}{14}} = \\frac{3}{5}$$\n",
    "\n",
    "If there are multiple features, we can calculate the posterior probability of each class, given the evidence (weather and temperature), for example, $P(Yes|Sunny, Cool)$:\n",
    "$$ P(Yes|Sunny, Cool) = \\frac{P(Sunny, Cool|Yes)P(Yes)}{P(Sunny, Cool)} = \\frac{P(Sunny|Yes)P(Cool|Yes)P(Yes)}{P(Sunny)P(Cool)} $$\n",
    "\n",
    "## Naive Bayes for text classification\n",
    "You need a document $d$, a set of classes $C = {c_1, c_2, ..., c_n}$, and a set of $m$ hand-labelled documents $(d_1, c_1), (d_2, c_2), ..., (d_m, c_m)$. The for a document $d$, we want to find the class $c$ that maximizes the posterior probability $P(c|d)$.\n",
    "$$ P(c|d) = \\frac{P(c)P(d|c)}{P(d)} = \\frac{P(c)\\prod\\limits_{i=1}^n P(w_i|c)}{P(d)}$$\n",
    "Here, there are two assumptions : bag of words (position doesn't matter) and conditional independence.\n",
    "Then, we pick the most probable class: $$c_{MAP} = \\arg\\max\\limits_{c} P(c)\\prod\\limits_{i=1}^n P(w_i|c)$$\n",
    "Here, $$ P(c_j) = \\frac{docCount(C = c_j)}{N_{doc}} $$ and $$ P(w_i|c_j) = \\frac{wordCount(w_i, C = c_j)}{\\sum\\limits_{w \\in V} wordCount(w, C = c_j)} $$, where $V$ is the vocabulary.\n",
    "This has a problem of zero probability, so we use Laplace smoothing: $$ P(w_i|c_j) = \\frac{wordCount(w_i, C = c_j) + 1}{\\sum\\limits_{w \\in V} wordCount(w, C = c_j) + |V|} $$\n",
    "\n",
    "[Example](https://www.fi.muni.cz/~sojka/PV211/p13bayes.pdf):\n",
    "<img src=\"https://i.imgur.com/p3nZUNM.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "<img src=\"https://i.imgur.com/kcNsCro.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "Therefore, $$P(C|d_5) = \\frac{3}{4} {(\\frac{3}{7})}^3 \\frac{1}{14} \\frac{1}{14} \\frac{1}{P(d_5)}$$\n",
    "and $$P(\\bar{C} | d_5) = \\frac{1}{4} {(\\frac{2}{9})}^3 \\frac{2}{9} \\frac{2}{9} \\frac{1}{P(d_5)}$$\n",
    "\n",
    "$P(d_5)$ is the same for both classes, so we can ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "- Ensemble learning is a machine learning paradigm where multiple learners are trained to solve the same problem, and then combined to get better results.\n",
    "- types of ensemble methods:\n",
    "1. bagging - decrease variance\n",
    "    - building multiple models (typically of the same type) from different subsamples of the training dataset (with replacement) \n",
    "    - considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process\n",
    "    - eg. random forest, extra trees\n",
    "2. boosting - decrease bias\n",
    "    - building multiple models (typically of the same type) each of which learns to fix the prediction errors of a prior model in the chain\n",
    "    - considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy\n",
    "3. stacking  - increase predictive power\n",
    "    - building multiple models (typically of differing types) and supervisor model that learns how to best combine the predictions of the base models\n",
    "    - considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions\n",
    "\n",
    "### Random forest vs Extra trees\n",
    "- Random forest is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set. The steps for random forest are:\n",
    "    1. Sample $n$ cases at random with replacement to create a subset of the data, called the bag used to build a tree\n",
    "    2. At each node, randomly select $d$ features without replacement\n",
    "    3. Calculate the best split point for the selected features\n",
    "    4. Split the node into two daughter nodes\n",
    "    5. Repeat steps 1 to 4 $k$ times\n",
    "    6. Aggregate the prediction by each tree to assign the class label by majority vote (classification) or average (regression)\n",
    "- Extra trees is an ensemble learning method for classification, regression and other tasks that constructs a multitude of decision trees at training time and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n",
    "\n",
    "### AdaBoost vs Gradient Boosting vs XGBoost\n",
    "- AdaBoost is an ensemble learning method for classification and regression. It is a meta-algorithm, and can be used in conjunction with many other learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers.\n",
    "\n",
    "<img src=\"https://i.imgur.com/uYsoCL2.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "- Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. Gradient boosting is a greedy algorithm and can overfit if run for too many iterations.\n",
    "\n",
    "<img src=\"https://i.imgur.com/Fz2HzoG.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 10px; padding-bottom: 10px;\">\n",
    "\n",
    "- XGBoost is short for eXtreme Gradient Boosting. It is an optimized distributed gradient boosting library. It provides a parallel tree boosting (also known as GBDT, GBM) that solves many ML problems quickly and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "- Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters).\n",
    "\n",
    "### K-means clustering\n",
    "- K-means clustering aims to partition $n$ observations into $k$ clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\n",
    "    - K-means clustering minimizes within-cluster variances (squared Euclidean distances)\n",
    "    - Given a set of observations $(x_1, x_2, ..., x_n)$, where each observation is a $d$-dimensional real vector, k-means clustering aims to partition the $n$ observations into $k$ sets $S = {S_1, S_2, ..., S_k}$ so as to minimize the within-cluster sum of squares (WCSS) $$ \\sum_{i=1}^k \\sum_{x \\in S_i} ||x - \\mu_i||^2 $$ where $\\mu_i$ is the mean of points in $S_i$.\n",
    "\n",
    "#### Evaluation metrics\n",
    "- Distortion\n",
    "    - the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.\n",
    "    - The distortion is given by $$ J = \\sum_{i=1}^k \\frac{1}{|S_i|} \\sum_{x \\in S_i} ||x - \\mu_i||^2 $$\n",
    "- Inertia\n",
    "    - the sum of squared distances of samples to their closest cluster center.\n",
    "    - The inertia is given by $$ I = \\sum_{i=1}^k \\sum_{x \\in S_i} ||x - \\mu_i||^2 $$\n",
    "\n",
    "- Dunn index\n",
    "    - the ratio between the minimum inter-cluster distance to maximum intra-cluster distance. The higher the value of Dunn index, the better the clustering.\n",
    "    - The Dunn index is defined as $$ D = \\frac{\\min\\limits_{1 \\leq i < j \\leq n} d(i, j)}{\\max\\limits_{1 \\leq k \\leq n} \\Delta(k)} = \\frac{\\min(\\text{inter-cluster distance})}{\\max(\\text{intra-cluster distance})}$$ where $d(i, j)$ is the distance between clusters $i$ and $j$, and $\\Delta(k)$ is the diameter of cluster $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points: [[1 1]\n",
      " [2 2]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [5 6]\n",
      " [5 7]\n",
      " [6 7]\n",
      " [6 6]]\n",
      "Iteration 1:\n",
      "Centroids: [[1. 1.]\n",
      " [5. 6.]]\n",
      "Distances: [[0.         6.40312424]\n",
      " [1.41421356 5.        ]\n",
      " [2.23606798 4.24264069]\n",
      " [1.         5.65685425]\n",
      " [6.40312424 0.        ]\n",
      " [7.21110255 1.        ]\n",
      " [7.81024968 1.41421356]\n",
      " [7.07106781 1.        ]]\n",
      "Iteration 2:\n",
      "Centroids: [[1.5 2. ]\n",
      " [5.5 6.5]]\n",
      "Distances: [[1.11803399 7.1063352 ]\n",
      " [0.5        5.70087713]\n",
      " [1.11803399 4.94974747]\n",
      " [0.5        6.36396103]\n",
      " [5.31507291 0.70710678]\n",
      " [6.10327781 0.70710678]\n",
      " [6.72681202 0.70710678]\n",
      " [6.02079729 0.70710678]]\n",
      "Iteration 3:\n",
      "Centroids: [[1.5 2. ]\n",
      " [5.5 6.5]]\n",
      "Distances: [[1.11803399 7.1063352 ]\n",
      " [0.5        5.70087713]\n",
      " [1.11803399 4.94974747]\n",
      " [0.5        6.36396103]\n",
      " [5.31507291 0.70710678]\n",
      " [6.10327781 0.70710678]\n",
      " [6.72681202 0.70710678]\n",
      " [6.02079729 0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of clusters and the number of iterations\n",
    "K = 2\n",
    "max_iterations = 3\n",
    "\n",
    "# Generate some sample data\n",
    "# data = np.random.randint(0, 10, size=(5, 2))\n",
    "data = np.array([[1, 1], [2, 2], [2, 3], [1, 2], [5,6], [5, 7], [6, 7], [6, 6]])\n",
    "print(f\"Points: {data}\")\n",
    "\n",
    "# Initialize the centroids by randomly selecting K data points\n",
    "# centroids = data[np.random.choice(data.shape[0], K, replace=False)]\n",
    "centroids = np.array([[1, 1], [5, 6]])\n",
    "centroids = centroids.astype(float)\n",
    "\n",
    "# Iterate the k-means algorithm\n",
    "for i in range(max_iterations):\n",
    "    # Assign each point to the nearest centroid\n",
    "    distances = np.sqrt(np.sum((data[:, np.newaxis, :] - centroids) ** 2, axis=2))\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    \n",
    "    # Print the centroids and the distances at each iteration\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    print(f\"Centroids: {centroids}\")\n",
    "    print(f\"Distances: {distances}\")\n",
    "    \n",
    "    # Update the centroids to the mean of the assigned points  \n",
    "    for k in range(K):\n",
    "        centroids[k] = np.mean(data[labels == k], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means++ algorithm\n",
    "\n",
    "- K-means++ algorithm is an algorithm for choosing the initial values (or \"seeds\") for the k-means clustering algorithm\n",
    "- The algorithm is as follows:\n",
    "    1. Choose one center uniformly at random from among the data points.\n",
    "    2. For each data point $x$, compute $D(x)$, the distance between $x$ and the nearest center that has already been chosen.\n",
    "    3. Choose one new data point at random as a new center, using a weighted probability distribution where a point $x$ is chosen with probability proportional to $D(x)^2$.\n",
    "    4. Repeat Steps 2 and 3 until $k$ centers have been chosen.\n",
    "    5. Now that the initial centers have been chosen, proceed using standard k-means clustering.\n",
    "- The k-means++ seeding method gives a provable upper bound on the expected running time of the resulting k-means algorithm, which is nearly-optimal up to constant factors.\n",
    "- main advantage is that it helps avoid poor local minima, by ensuring a more spread out initial set of centers, leading to faster convergence and better final solution\n",
    "\n",
    "### K Medoids clustering\n",
    "- k-medoids chooses datapoints as centers (medoids or exemplars) and forms clusters around them. It is similar to k-means clustering, but the difference is that the center of the cluster is always a data point. In k-means, the center of the cluster is the mean of the data points in the cluster.\n",
    "- The algorithm is as follows:\n",
    "    1. Initialize: randomly select $k$ of the $n$ data points as the medoids\n",
    "    2. Associate each data point to the closest medoid. (Thus forming $k$ clusters of data points.)\n",
    "    3. For each cluster $k$ and its medoid $m$:\n",
    "        - For each non-medoid data point $o$ in the cluster:\n",
    "            - Swap $o$ and $m$ and compute the total cost of the configuration\n",
    "        - Select the configuration with the lowest cost.\n",
    "    4. Repeat Steps 2 and 3 until there is no change in the medoid.\n",
    "\n",
    "### Fuzzy C-means clustering\n",
    "- Fuzzy C-means clustering is a method of clustering which allows one piece of data to belong to two or more clusters\n",
    "- The algorithm is as follows:\n",
    "    1. Specify the number of clusters $c$ and the fuzzy parameter $m$.\n",
    "    2. Initialize the cluster centers randomly, $v_j \\in \\mathbb{R}^d$ for $j = 1, 2, ..., c$.\n",
    "    3. For each data point, compute the degree of membership of that point to each cluster center: $$\\mu_{ij} = \\frac{1}{\\sum\\limits_{k=1}^c \\left( \\frac{d_{ij}}{d_{kj}} \\right) ^ \\frac{2}{m-1}}$$ where $d_{ij}$ is the distance between the $i^{th}$ data point and the $j^{th}$ cluster center.\n",
    "    4. Recompute the cluster centers: $$v_j = \\frac{\\sum\\limits_{i=1}^n \\mu_{ij}^m x_i}{\\sum\\limits_{i=1}^n \\mu_{ij}^m}$$\n",
    "    5. Repeat Steps 3 and 4 until the membership coefficients $\\mu_{ij}$ do not change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization (EM) algorithm\n",
    "- Expectation Maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between:\n",
    "    - Expectation step (E-step): create a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters\n",
    "        $$ Q(\\theta | \\theta^{(t)}) = E_{Z|X, \\theta^{(t)}} [\\log L(\\theta; X, Z)] $$\n",
    "    - Maximization step (M-step): compute parameters maximizing the expected log-likelihood found on the E-step\n",
    "        $$ \\theta^{(t+1)} = \\arg\\max\\limits_{\\theta} Q(\\theta | \\theta^{(t)}) $$\n",
    "- The EM algorithm is guaranteed to converge to a local maximum, but not necessarily to the global maximum of the likelihood. In practice, EM can be susceptible to getting stuck in local maxima, so multiple restarts are used. The EM algorithm can also be generalized to maximize incomplete-data likelihood functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation / Comparison\n",
    "\n",
    "### Confidence Interval for Accuracy\n",
    "$$ CI = \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} $$\n",
    "where $\\hat{p}$ is the observed accuracy, $z_{\\alpha/2}$ is the critical value of the normal distribution at $\\alpha/2$ (e.g. for 95% confidence interval, $\\alpha = 0.05$ and $z_{\\alpha/2} = 1.96$), and $n$ is the number of test instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
