{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management for Machine Learning Concepts and Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models \n",
    "\n",
    "- describes how data is represented in terms of attributes and relationships\n",
    "    - car can be represented by attributes such as make, model, year, color, etc, as well as owner, license plate, etc\n",
    "- applications built by layering one data model on top of another\n",
    "- many different data models exist, which\n",
    "    - embody different assumptions about the data\n",
    "    - are suited to different types of applications\n",
    "- selecting data model affects how data is stored, queried, and updated\n",
    "    - ways systems are built, problems that can be solved, and performance characteristics\n",
    "\n",
    "### Relational Data Model\n",
    "- Edgar Codd, 1970\n",
    "- data is represented as a collection of relations (tables)\n",
    "    - each relation has a set of named attributes (columns)\n",
    "    - each tuple (row) has a value for each attribute\n",
    "    - unordered, can shuffle rows and columns\n",
    "    - usually stored in csv or parquet format\n",
    "- normalization\n",
    "    - process of decomposing relations with anomalies into smaller, well-structured relations (1ND, 2NF, 3NF, BCNF etc)\n",
    "    - reduces redundancy and improves data integrity\n",
    "    - can be expensive to compute\n",
    "- databases built around relational model are called relational databases\n",
    "    - most common type of database\n",
    "    - SQL is the most common language for querying and manipulating data in relational databases\n",
    "    - examples: MySQL, PostgreSQL, SQLite, Oracle, Microsoft SQL Server, IBM DB2\n",
    "\n",
    "#### SQL \n",
    "- is a declarative language\n",
    "- user specifies what data they want, not how to get it\n",
    "    - tables, conditions, transformations such as joins and aggregations\n",
    "- query optimizer determines how to execute query\n",
    "    - which tables to read, which indexes to use, etc\n",
    "    - how to break query into smaller subqueries, order of operations, etc\n",
    "- generalized a lot but is still restrictive, needs a strict schema, schema changes are expensive\n",
    "\n",
    "### NoSQL\n",
    "- non-relational databases\n",
    "- retroactively reinforced as \"not only SQL\"\n",
    "- data is stored in a variety of ways\n",
    "    - key-value / document stores\n",
    "        - targets use cases where data comes in self-contained documents\n",
    "        - single continuous string of data, encoded as JSON, XML, or similar format\n",
    "        - each document has a unique key that is used to retrieve it\n",
    "    - wide-column stores\n",
    "        - targets use cases where data is stored in sparse tables, with many columns\n",
    "        - each row has a unique key, but unlike key-value stores, each row can have different columns\n",
    "        - each column has a name and a value\n",
    "        - examples: Cassandra, HBase, BigTable\n",
    "    - graph databases\n",
    "        - targets use cases where data has complex relationships between data entities exist and are important\n",
    "- does not enforce a schema\n",
    "    - misleading, as schema is still assumed by the reader of the data\n",
    "    - shifts the burden of ensuring data integrity from the database to the application\n",
    "- has better locality than relational databases\n",
    "    - data with complex relationships can be stored together and retrieved in one operation\n",
    "    - can be faster than relational databases for some use cases\n",
    "    - but difficult to execute joins over data from different entities\n",
    "- examples: MongoDB, Cassandra, HBase, Neo4j, Redis, CouchDB\n",
    "\n",
    "| Data Type | Definition | Examples | Advantages | Disadvantages |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Structured Data | This type of data is organized in a highly systematic and predictable manner. It is usually stored in relational databases and can be efficiently queried using a language like SQL. | Databases, CSV files, Excel spreadsheets. | Easy to store, search, and analyze. High accuracy and reliability. | Lack of flexibility. Not suitable for complex, hierarchical, or multi-dimensional data. |\n",
    "| Semi-Structured Data | This type of data does not conform to the formal structure of data models, but contains tags and other markers to separate semantic elements. It is more flexible than structured data, but less organized than unstructured data. | XML, JSON, NoSQL databases. | More flexible than structured data. Can represent more complex and hierarchical relationships. | Less efficient to query and process than structured data. Requires more storage. |\n",
    "| Unstructured Data | This type of data doesn't have a predefined model or is not organized in a predefined manner. It is typically text-heavy, but can also be in the form of images, videos, etc. | Emails, Word documents, PDFs, images, videos, web pages. | Highly flexible. Can represent any type of information. | Difficult to analyze and process. Requires advanced tools and algorithms, such as Natural Language Processing (NLP) for text, or Computer Vision for images and videos. |\n",
    "\n",
    "### Data Warehouses and Data Lakes\n",
    "- data warehouse\n",
    "    - database that is optimized for analytics\n",
    "    - typically used to store structured data\n",
    "    - often used for reporting and dashboarding\n",
    "    - examples: Amazon Redshift, Google BigQuery, Snowflake\n",
    "- data lake\n",
    "    - repository for structured and unstructured data\n",
    "    - typically used for storing large amounts of raw data before processing\n",
    "    - examples: Amazon S3, Google Cloud Storage, Hadoop File System (HDFS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management\n",
    "- about transforming data into a format that is more convenient to work with for later stages of the pipeline\n",
    "- apply data transformations to make data easier to work with\n",
    "    - filtering, aggregating, joining, sorting, etc\n",
    "- train models, anonymize data, etc\n",
    "- delete data that is no longer needed\n",
    "\n",
    "### Multi-phases\n",
    "1. creation\n",
    "    - data created in process, outside of our control, captured in some storage system\n",
    "    - some are:\n",
    "        - static (infrequently updated) eg. photo recognition dataset\n",
    "        - dynamic (frequently updated / real-time) eg. stock market data\n",
    "    - may be structured, semi-structured, or unstructured\n",
    "        - requires different tools and techniques in each case\n",
    "    - may require augmentation, to make it more useful\n",
    "        - eg. adding labels to images, adding timestamps to stock market data    \n",
    "2. ingestion\n",
    "    - filtering / selection / sampling may be done\n",
    "        - we may not necessarily want to keep all the data\n",
    "        - sampling may lose some details, but may be necessary for performance reasons, trade-off between quality cost of model and savings in time and money\n",
    "    - may be simple or complex\n",
    "        - dumping data into a database, or running a complex ETL pipeline\n",
    "    - may be done in real-time or in batches\n",
    "    - reliability concerns focus on correctness and throughput\n",
    "        - correctness: is the data being ingested correctly?\n",
    "        - throughput: how fast can we ingest data?\n",
    "    - monitoring existence and condition of data before and during ingestion is the most difficult part of the process\n",
    "3. processing (validation, cleaning, enrichment)\n",
    "    - validation\n",
    "        - check that data is in the expected format, validate against schema\n",
    "            - both store and reference standard definitions\n",
    "        - common reason for errors is a bug in the data collection pipeline\n",
    "    - cleaning\n",
    "        - fix or remove corrupted or incorrect data\n",
    "        - normalization, deduplication is done\n",
    "        - bucketing, binning, discretization is done for continuous data\n",
    "    - enrichment\n",
    "        - add more data to extend existing data\n",
    "        - bring confirmatory data from other sources\n",
    "        - manual jobs for labeling data, adding timestamps, etc\n",
    "4. post-processing (data management, storage, analysis, visualization)\n",
    "    - data storage\n",
    "        - how and where to store data, based on how it will be used\n",
    "        - two concerns are efficiency and metadata management (data about how the data / model was put together)\n",
    "    - management\n",
    "        - motivated by business purpose, and the model structure and strategy \n",
    "            - what models will be built, how often they're refreshed, how many, how similar they are\n",
    "        - often a trade-off between cost and performance\n",
    "    - analysis and visualization\n",
    "        - data is analyzed and visualized to gain insights\n",
    "        - necessary to make the data less confusing and more accessible\n",
    "        - may be done in real-time or in batches\n",
    "\n",
    "### Data management components\n",
    "- the practice of ingesting, processing, securing, and storing strategic data for improving business operations\n",
    "- big data due to cloud, AI, IoT, edge computing, etc\n",
    "- data management solutions are aimed to clean, unify and secure data\n",
    "- aspects of data management:\n",
    "    1. data governace : support to data management through stewardship, policies, processes, standards, and adherence to compliance requirements\n",
    "    2. data architecture : infrastructure : data lakes, data warehouses, databases\n",
    "    3. metadata : data about data, information about attributes of data for better efficiency and understanding\n",
    "    4. data quality : structure, accuracy, consistency, completeness, validity, uniqueness, timeliness, etc\n",
    "    5. data lifecycle : data is created, stored, used, shared, archived, and destroyed\n",
    "    6. analytics : data is analyzed to gain insights\n",
    "    7. privacy and security : data is protected from unauthorized access, use, disclosure, disruption, modification, inspection, recording, or destruction\n",
    "- components of data management at orgs:\n",
    "    1. processing:\n",
    "        - data ingested from APIs, databases, apps, IoT devices, forms, etc\n",
    "        - processed and loaded though ETL (historical standard) or ELT (growth in cloud computing, real-time)\n",
    "        - data is filtered, merged, aggregated, transformed, etc\n",
    "    2. governance:\n",
    "        - data is governed by policies, processes, standards, and compliance requirements\n",
    "        - includes processes around data quality, access, usability, and security\n",
    "        - governance councils to ensure metadata is accurate and up-to-date, and define roles and responsibilities\n",
    "    3. storage:\n",
    "        - data is stored into data lakes, data warehouses, etc depeding on the type of data and its purpose\n",
    "    4. security:\n",
    "        - set guardrails for data access and use against unauthorized access, use, disclosure, disruption, modification, inspection, recording, or destruction\n",
    "        - data is encrypted, anonymized, and masked\n",
    "\n",
    "### Data platforms\n",
    "- data platform\n",
    "    - a collection of tools and services that are used to ingest, process, store, and analyze data\n",
    "    - have a robust data and compute infrastructure\n",
    "    - examples: AWS, Azure, Google Cloud are public cloud providers\n",
    "    - can also be built on-premise, Hadoop, Spark, etc are open-source tools to leverage\n",
    "- data platform components\n",
    "    - fast (massively parallel) query processing, computing, and storage\n",
    "    - elastic scaling, in-memory processing, and high availability\n",
    "    - columnar storage, compression, and data indexing\n",
    "\n",
    "### Modes of data flow\n",
    "- many ways in which data can flow from one process to another, most common ways are:\n",
    "    1. via databases (encoded by sender while writing, decoded by receiver while reading from database)\n",
    "        - considerations : data compatibility, forward and backward compatibility of the database software code\n",
    "        - database must be able to handle multiple processes accessing it at the same time\n",
    "        - when an older version of code is reading from a database, it must be able to read data written by a newer version of code and handle new fields gracefully\n",
    "        - deal with schema changes, eg. adding a new field, or changing the type of an existing field\n",
    "        - archivable, data dumps should be frequent and easy to access, Avro and Parquet are common formats (analytics friendly)\n",
    "    2. via RPCs and REST APIs (client encodes request, server decodes request, server encodes response, client decodes response)\n",
    "        - two roles : client and server (servers expose APIs, clients consume APIs by making requests)\n",
    "        - different kinds of clients : web, mobile, desktop, etc\n",
    "            - web clients : make GET requests to fetch data, POST requests to create data, PUT requests to update data, DELETE requests to delete data\n",
    "                - protocols and formats : HTTP, JSON, XML, SOAP, SSL/TLS, etc\n",
    "                - REST is a set of constraints on top of HTTP, eg. GET, POST, PUT, DELETE\n",
    "                - SOAP is a XML-based protocol for exchanging information over HTTP, complex multitude of standards\n",
    "                    - API of a SOAP service is described using WSDL (Web Services Description Language)\n",
    "            - mobile clients : make requests to a server, which then makes requests to a database\n",
    "        - service oriented architecture (SOA)\n",
    "            - server can be a collection of services, each service is a collection of APIs\n",
    "            - each service is responsible for a specific task, rebranded as microservices architecture\n",
    "        - remote procedure calls (RPCs) : client calls a function on a server, which returns a value\n",
    "            - tries to make a remote function call look like a local function call, location of the server is abstracted away (location transparency)\n",
    "            - too heavily used, not going away (eg. gRPC, Thrift, etc)\n",
    "        - RESTful API has advantages:\n",
    "            - good for experimentation, easy to use, easy to understand, easy to document\n",
    "            - can be used by any client that can make HTTP requests, vast ecosystem of tools and libraries\n",
    "            - supported by most programming languages\n",
    "    3. via asynchronous message queues (encoded by sender and queued, decoded by receiver when message is dequeued)\n",
    "        - somewhere between databases and RPCs, deals with a message-oriented middleware (MOM) / message broker\n",
    "        - sender and receiver are decoupled, sender sends a message to a queue, receiver receives the message from the queue\n",
    "        - can act as a buffer, improves reliability, sender and receiver don't need to be online at the same time\n",
    "        - usually one way, logically decouples the sender and receiver\n",
    "        - Apache Kafka, RabbitMQ, Amazon SQS, etc\n",
    "        - don't enforce a schema, sender and receiver must agree on a schema\n",
    "\n",
    "### Data sources\n",
    "- data sources, can be internal or external, structured or unstructured\n",
    "1. user-generated content (UGC)\n",
    "    - data generated by users, eg. social media posts, comments, reviews, etc\n",
    "    - can be used to understand user behavior, sentiment analysis, etc\n",
    "    - can be used to train models, eg. spam detection, recommender systems, etc\n",
    "    - can be used to generate new content, eg. chatbots, etc\n",
    "2. system-generated content (SGC)\n",
    "    - data generated by systems, eg. logs, metrics, etc\n",
    "        - many services to process and analyze logs, eg. Logstash, DataDog, CloudWatch, etc\n",
    "    - can be used to monitor systems, eg. health, performance, etc\n",
    "    - can be used to train models, eg. anomaly detection, etc\n",
    "    - can be used to debug systems, eg. errors, crashes, etc\n",
    "3. system-generated user data (SGUD)\n",
    "    - data generated by systems about users, eg. user activity, clicking, scrolling, zooming, etc\n",
    "    - can be used to understand user behavior, eg. user preferences, may be subject to privacy concerns\n",
    "    - internal databases, like inventory, customer relationship management (CRM), enterprise resource planning (ERP), etc\n",
    "4. third-party data\n",
    "    - first-party data : data collected by the company itself\n",
    "    - second-party data : data collected by a company about its own users, but made available to another company\n",
    "    - third-party data : data collected by a company about public, not its own users\n",
    "    - can be used to understand user behavior, eg. demographics, interests, etc\n",
    "    - riddled with privacy concerns, eg. Cambridge Analytica\n",
    "    - data from apps, websites, check-ins, etc collected and anonymized by data brokers to generate activity histories, eg. Acxiom, Experian, etc\n",
    "\n",
    "### Data formats\n",
    "- data formats, can be structured or unstructured, text or binary\n",
    "- important to think about how the data will be used in the future, so that the format will make sense\n",
    "- consider the following: human readability, ease of parsing, access patterns, serialization, storage, cost, \n",
    "- data formats can be categorized into:\n",
    "    1. text-based formats\n",
    "        - human-readable, easy to debug, easy to understand, easy to parse\n",
    "        - CSV : comma-separated values, each row is a record, each column is a field, can be opened in Excel\n",
    "        - JSON : JavaScript Object Notation, each record is a JSON object, each field is a key-value pair\n",
    "        - XML : eXtensible Markup Language, each record is an XML element, each field is an XML attribute\n",
    "    2. binary formats\n",
    "        - not human-readable, not easy to debug, not easy to understand, not easy to parse\n",
    "        - Avro : Apache Avro, each record is an Avro object, each field is an Avro field, can be parsed using Avro parsers\n",
    "        - Parquet : each record is a Parquet object, each field is a Parquet field, can be parsed using Parquet parsers\n",
    "            - more compact than CSV, more efficient to read and write, more efficient to store (AWS recommends Parquet for S3)\n",
    "        - ORC : Optimized Row Columnar, each record is an ORC object, each field is an ORC field, can be parsed using ORC parsers\n",
    "        - pickle : used for serializing and deserializing Python objects\n",
    "    3. hybrid formats\n",
    "        - eg. Apache Thrift, Protocol Buffers, etc\n",
    "\n",
    "| Aspect           | Row-Based (CSV)                                      | Column-Based (Parquet)                                     |\n",
    "|------------------|------------------------------------------------------|-----------------------------------------------------------|\n",
    "| Storage Efficiency | Larger file size due to redundant column values      | Smaller file size due to column-wise compression           |\n",
    "| Compression     | Less efficient compression                          | Efficient compression using various algorithms (e.g., Snappy, Gzip) |\n",
    "| Query Performance | Slower for complex queries involving many columns   | Faster for selective column retrieval and aggregations    |\n",
    "| Column Projection | Reads entire rows, including unused columns         | Reads only the required columns, reducing I/O             |\n",
    "| Schema Evolution | Less flexible for schema changes                    | Supports schema evolution and nested data structures      |\n",
    "| Predicate Pushdown | Limited support for predicate pushdown optimization | Supports predicate pushdown for better query performance  |\n",
    "| Columnar Operations | Inefficient for column-wise operations              | Efficient for columnar operations and vectorized processing |\n",
    "| Parallel Processing | Limited parallelism due to row-oriented nature      | Better parallelism for column scans and computations      |\n",
    "| Data Type Flexibility | Less flexible for handling complex data types       | Better support for complex data types (e.g., arrays, structs) |\n",
    "| Tooling Support | Widely supported by various tools and platforms     | Growing support by tools and platforms, but not as widespread |\n",
    "| Use Cases        | General-purpose data storage and exchange           | Analytical workloads, big data processing, data warehouses |\n",
    "\n",
    "### Data management principles\n",
    "- in ML systems, we are interested in:\n",
    "    1. data used to train the model (important, often simpler model with more / higher quality data is better than a complex model with less / lower quality data)\n",
    "        - collect in compliance with privacy laws, eg. GDPR, organizational policies, etc\n",
    "        - jurisdictional considerations : where the data is collected, where it is stored, where it is processed, etc\n",
    "        - require consent to collect PII (personally identifiable information) data, and delete it when consent is withdrawn\n",
    "            - deleteing data is difficult, eg. backups, metadata, multiple copies, etc\n",
    "        - data collection should be transparent, eg. privacy policy, etc\n",
    "        - anonymize data when possible, eg. remove PII, etc, remove connection between data and the person\n",
    "    2. processing pipeline used to transform the data into a model\n",
    "        - ML pipelines are very sensitive to changes in data, eg. schema, distribution, etc\n",
    "        - ability to aggregate, monitor and process data is important to success of ML system\n",
    "        - need to make sure that:\n",
    "            - durability : data is not lost\n",
    "            - availability : data is ready to be used\n",
    "            - consistency : data is the same across different systems\n",
    "            - latency : data is available in a timely manner\n",
    "            - versioning : data is versioned, and can be rolled back\n",
    "\n",
    "### Data encoding formats\n",
    "There are two different representations of data, which we need to convert between:\n",
    "1. in memory - as objects, data structures, etc (disk to memory -> decoding)\n",
    "2. on disk - as files, databases, etc (memory to disk -> encoding)\n",
    "\n",
    "Encoding libraries encode in-memory data into a specific format, like `pickle` for Python, `java.io.Serializable` for Java, etc with minimal effort.\n",
    "- Problems are related to integration (between different languages), versioning (between different versions of the same language), and performance (encoding and decoding)\n",
    "- to restore data in the same object type, class instantiation is required, which can be a security risk\n",
    "\n",
    "Standardized encoding formats are used to encode data in a language-agnostic way, like JSON (relatively human-readable, but not very space-efficient), XML (too verbose), and CSV (not very expressive)\n",
    "- ambiguity around numbers, dates, etc (cannot distinguish between 1 and 1.0 and \"1\", depending on the specific format)\n",
    "- no support for binary strings (workaround is to encode binary data as base64 strings)\n",
    "- schema support is optional (JSON / XML), non existent in CSV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query languages for data\n",
    "\n",
    "| Aspect           | Declarative Languages | Imperative Languages |\n",
    "|------------------|------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| Paradigm         | Focuses on \"what\" should be achieved | Focuses on \"how\" to achieve the desired outcome |\n",
    "| Program Structure| Emphasizes relationships between components and constraints | Emphasizes step-by-step instructions and control flow |\n",
    "| Execution Order  | Not explicitly defined, relies on underlying runtime system | Sequential execution based on control flow and statements |\n",
    "| State Management | Implicit, programs define relationships and constraints | Explicitly managed through variables and mutable data |\n",
    "| Control Flow     | Data-driven, relies on pattern matching and rule evaluation | Explicitly defined using loops, conditionals, and branching |\n",
    "| Modifiability    | Easier to modify and extend programs due to abstraction | Modifications may require changing multiple instructions |\n",
    "| Concurrency      | Well-suited for parallel and distributed processing | Requires explicit management of threads and synchronization |\n",
    "| Abstraction      | High level of abstraction through built-in functions | Lower level of abstraction, closer to machine instructions |\n",
    "| Examples         | SQL, HTML/CSS (declarative subsets) | C, Java, Python, JavaScript |\n",
    "\n",
    "#### Map Reduce querying\n",
    "- a scalable, fault-tolerant, and distributed data processing paradigm for large-scale data sets\n",
    "- does efficient processing by distributing the data and computation across different nodes in a cluster, and performing parallel computations\n",
    "- two phases:\n",
    "    1. map phase : each node processes its input and generates a set of intermediate key-value pairs (`map()` function)\n",
    "        - initial data is divided into smaller partitions, each partition is processed independently\n",
    "        - performs filtering, transformation, and extraction of relevant information from initial data\n",
    "        - generates a set of intermediate key-value pairs\n",
    "    2. shuffle and sort: all intermediate key value pairs are shuffled and sorted by key\n",
    "        - intermediate values with the same key are grouped together (might shuffle data across the cluster to ensure all values with same key are grouped together)\n",
    "    2. reduce phase : sorted intermediate data reduced by `reduce()` function \n",
    "        - reducer receives set of key value pairs with the same key, then performs aggregation, summarization or any other operation\n",
    "        - results are the final output of the MapReduce job\n",
    "- `map()` and `reduce()` functions are user-defined, must be pure functions (no side effects, same input always produces same output)\n",
    "- combining and iteration steps, to execute multiple MapReduce jobs in sequence, to perform more complex tasks\n",
    "- fault tolerance, if a node fails, the work is rescheduled on another node\n",
    "- scalability, can process large amounts of data in parallel, by adding more nodes to the cluster\n",
    "    - automatically handles data partitioning, scheduling tasks, monitoring tasks, and re-executing failed tasks\n",
    "- querying - by defining a map function that filters and transforms the data, and a reduce function that aggregates the data\n",
    "    - can be used to implement SQL-like queries, eg. `SELECT`, `WHERE`, `GROUP BY`, `ORDER BY`, `JOIN`, etc\n",
    "    - can be used to implement machine learning algorithms, eg. k-means clustering, etc\n",
    "- MapReduce is a programming model, not a specific implementation\n",
    "    - Hadoop and Spark are open-source implementations of MapReduce\n",
    "\n",
    "### Types of workloads\n",
    "\n",
    "| Aspect             | Analytical Workloads                                          | Transactional Workloads                                          |\n",
    "|--------------------|--------------------------------------------------------------|-----------------------------------------------------------------|\n",
    "| Purpose            | Focuses on data exploration, pattern discovery, and insights  | Focuses on data modification, transaction processing, and updates |\n",
    "| Data Volume        | Typically deals with large volumes of historical data         | Deals with relatively smaller amounts of current data            |\n",
    "| Query Complexity   | Involves complex and resource-intensive queries and aggregations | Involves simple and fast queries for data retrieval and updates |\n",
    "| Read vs. Write     | Primarily read-intensive operations                           | Balanced read and write operations                               |\n",
    "| Data Model         | Often uses denormalized or multidimensional data models       | Relational or normalized data models                             |\n",
    "| Latency            | Tolerates higher latency for longer-running queries           | Requires low latency for real-time data processing               |\n",
    "| Concurrency        | Supports parallel processing and batch operations            | Requires high concurrency for concurrent transactions            |\n",
    "| Data Integrity     | Focuses on historical data integrity and accuracy             | Emphasizes real-time data consistency and ACID properties (Atomicity, Consistency, Isolation, Durability) |\n",
    "| Optimization       | Optimized for data scanning, indexing, and query execution    | Optimized for transactional consistency and locking mechanisms   |\n",
    "| Workload Examples  | Data mining, reporting, OLAP, business intelligence           | E-commerce, banking, stock trading, online reservations          |\n",
    "\n",
    "OLAP / OLTP outdated now due to :\n",
    "- we have data warehouses and data lakes, which can be used for both analytical and transactional workloads\n",
    "- latest paradigm to decouple storage and compute, and use a data lake for storage and a data warehouse for compute\n",
    "\n",
    "| Aspect             | ETL Workloads                                                 | ELT Workloads                                                 |\n",
    "|--------------------|--------------------------------------------------------------|--------------------------------------------------------------|\n",
    "| Workflow           | Extract data from source systems, transform it, and then load | Extract data from source systems, load it, and then transform |\n",
    "| Transformation     | Heavy emphasis on data transformation and cleansing           | Transformation primarily performed after data loading        |\n",
    "| Data Processing    | Transformation occurs in dedicated ETL servers or engines     | Transformation often performed in the target data warehouse  |\n",
    "| Data Storage       | Transformed data stored in a separate staging area or store   | Data loaded directly into the target data warehouse          |\n",
    "| Flexibility        | Requires predefined schema and mapping for data transformations | Supports flexible and schema-on-read transformations         |\n",
    "| Data Quality       | Focuses on data cleansing, validation, and integrity checks   | Data quality checks often performed during data loading      |\n",
    "| Data Integration   | ETL consolidates data from multiple sources before transformation | ELT integrates data from various sources directly into the data warehouse |\n",
    "| Use Cases          | Batch-oriented data integration, legacy systems, data warehousing | Real-time analytics, big data processing, cloud-based data warehouses |\n",
    "\n",
    "Difference between batch and stream processing:\n",
    "\n",
    "| Aspect             | Batch Processing                                              | Stream Processing                                              |\n",
    "|--------------------|--------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| Data Processing    | Processes data in fixed-size batches or chunks               | Processes data in real-time or near real-time                  |\n",
    "| Data Arrival       | Assumes data is collected and available in batches            | Handles continuous and ongoing data arrival                    |\n",
    "| Latency            | Typically involves higher latency due to processing in batches | Low latency processing for real-time or near real-time data    |\n",
    "| Processing Model   | Processes data in a predefined sequence or order             | Processes data as it arrives in an ordered or unordered manner |\n",
    "| Query Flexibility  | Well-suited for complex queries and extensive computations   | Optimized for simple queries and lightweight computations     |\n",
    "| Result Availability| Results become available after the entire batch is processed  | Results available immediately or near real-time                |\n",
    "| Use Cases          | Periodic data analysis, reports, ETL, data warehousing       | Real-time analytics, monitoring, anomaly detection, IoT       |\n",
    "| Data Windowing     | Can apply window-based operations on batches of data          | Supports sliding windows and time-based operations            |\n",
    "| Data Order         | Order of data is maintained within a batch                    | Handles out-of-order data arrival and event time ordering     |\n",
    "| Examples           | Hadoop MapReduce, Spark, Hive, Pig, etc | Apache Kafka, Flink, Storm, Amazon Kinesis, etc |\n",
    "\n",
    "### Serialization frameworks [[link]](https://www.linkedin.com/pulse/serialization-frameworks-simplified-taher-borsadwala/?trk=read_related_article-card_title)\n",
    "- serialization : process of converting an object into a stream of bytes, to store the object or transmit it to memory, a database, or a file\n",
    "- deserialization : reverse process of converting a stream of bytes into an object\n",
    "- serialization frameworks are language-neutral, platform-neutral extensible mechanism for serializing structured data\n",
    "- provide a SDL (Schema Definition Language) to define a logical data model made of objects, attributes\n",
    "- tag the objects with specific version numbers, to support schema evolution, forwards and backwards compatibility\n",
    "- support for primitive data types, complex data types, and collections\n",
    "- generate code to serialize and deserialize objects, and to convert between different versions of the same object, in different languages\n",
    "\n",
    "| Aspect | Protocol Buffers (protobuf) | Apache Thrift | Apache Avro |\n",
    "|-|-|-|-|\n",
    "| Schema Definition | Protocol Buffer Language (Protobuf) | Thrift IDL | JSON Schema |\n",
    "| Data Serialization | Binary | Binary, Compact, JSON | Binary, JSON |\n",
    "| Schema Evolution | Limited support for schema evolution | Support for schema evolution and backward compatibility | Support for schema evolution and backward compatibility |\n",
    "| Data Validation | Limited built-in data validation mechanisms | Supports custom data validation mechanisms | Supports custom data validation mechanisms |\n",
    "| RPC Support | Limited built-in RPC support | Built-in support for RPC | Limited built-in RPC support |\n",
    "| Use Cases | Interoperability, messaging, data exchange | Interoperability, RPC, microservices | Data serialization, data storage, messaging |\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
