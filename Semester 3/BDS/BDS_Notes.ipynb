{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Systems Concepts\n",
    "\n",
    "## Types and Storage of Data\n",
    "\n",
    "### Types of Data\n",
    "- Structured data\n",
    "    - data that has a defined length, format, and schema\n",
    "    - stored in relational databases, CRUD operations on records, ACID semantics\n",
    "    - examples: numbers, dates, strings\n",
    "- Unstructured data\n",
    "    - data that has no defined format or structure\n",
    "    - examples: text, images, audio, video\n",
    "- Semi-structured data\n",
    "    - data that has a defined structure, but not a defined schema\n",
    "    - attributes for every record could be different\n",
    "    - examples: XML, JSON\n",
    "\n",
    "3 Vs of Big Data\n",
    "- Volume (amount of data, terabytes, petabytes)\n",
    "- Velocity (speed of data generation, real-time, near real-time, batch, streaming)\n",
    "- Variety (types of data, structured, unstructured, semi-structured)\n",
    "- Veracity (uncertainty of data, trustworthiness, quality, accuracy, completeness) (4th V, newer)\n",
    "\n",
    "Issues with RDBMS:\n",
    "- Bigdata doesn't always need strong ACID semantics (esp systems of engagement)\n",
    "- Fixed schema is not sufficient, as application becomes popular more attributes need to be captured and DB modelling becomes an issue\n",
    "- Very wide de-normalized attribute sets\n",
    "- Data layout formats - column or row major - depends on use case\n",
    "- Expensive to retain and query long term data - need low cost solution\n",
    "\n",
    "Characteristics of Big Data Systems:\n",
    "- Application need not bother about common issues like sharding, replication\n",
    "    - devs focus on application logic rather than data management\n",
    "- Easier to model with flexible schema\n",
    "    - not necessary every record has same set of attributes\n",
    "- If possible, treat data as immutable\n",
    "    - keep adding timestamped versions of data values\n",
    "    - avoid human errors by not destroying a good copy\n",
    "- Built as distributed and incrementally scalable systems\n",
    "    - add new nodes to scale as in a Hadoop cluster\n",
    "- Options to have cheaper long term data retention\n",
    "    - long term data reads can have more latency and can be less expensive to store on commodity hardware, e.g. Hadoop file system (HDFS)\n",
    "- Generalized programming models that work close to the data\n",
    "    - e.g. Hadoop map-reduce that runs tasks on data nodes\n",
    "\n",
    "Challenges in Big Data Systems:\n",
    "- Latency issues in algorithms and data storage working with large data sets\n",
    "- Basic design considerations of Distributed and Parallel systems - reliability, availability, consistency\n",
    "- What data to keep and for how long - depends on analysis use case\n",
    "- Cleaning / Curation of data\n",
    "- Overall orchestration involving large volumes of data\n",
    "- Choose the right technologies from many options, including open source, to build the Big Data System for the use cases\n",
    "- Programming models for analysis\n",
    "- Scale out for high volume, search and analytics\n",
    "- Cloud is the cost effective way long term - but need to host Big Data outside the Enterprise\n",
    "- Data privacy and governance\n",
    "- Skilled coordinated teams to build/maintain Big Data Systems and analyse data\n",
    "\n",
    "Types of scalability:\n",
    "1. Vertical scaling (scaling up)\n",
    "    - increase the resources of a single node, demand architecture-aware algorithm design\n",
    "    - processing on x TB of data takes time t, then processing on (n*x) TB of data takes equal, less or much less than (n*t)\n",
    "    - e.g. more powerful CPU, more memory, \n",
    "2. Horizontal scaling (scaling out)\n",
    "    - increase the number of nodes, distribute the processing and storage tasks in parallel\n",
    "    - processing on x TB of data takes time t, then processing on (p*x) TB of data takes t or slightly more than t\n",
    "    - e.g. parallelization of jobs at several levels: distributing separate tasks onto separate threads on the same CPU, distributing separate tasks onto separate CPUs on the same computer, distributing separate tasks onto separate computers\n",
    "3. Elastic scaling (scaling up and down)\n",
    "    - dynamic provisioning based on computational need\n",
    "    - cloud computing\n",
    "    - on-demand service, resource pooling, scalability, accountability, broad network access\n",
    "\n",
    "Types of big data systems:\n",
    "1. batch processing of big data sources at rest\n",
    "    - building ML models, statistical aggregates\n",
    "2. real-time processing of big data in motion\n",
    "    - fraud detection from real-time financial transaction data\n",
    "3. interactive exploration with ad-hoc queries\n",
    "\n",
    "#### Big data architecture style\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/5e126d8f3906955d4fa2864535921f0b36e80537bc9f6c72908e9dd31a8a7683.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- big data solutions typically involve one or more of the following types of workload:\n",
    "    - batch processing of big data sources at rest\n",
    "    - real-time processing of big data in motion\n",
    "    - interactive exploration of big data\n",
    "    - predictive analytics and machine learning\n",
    "- components are usually: data sources, data storage, batch processing, real-time message ingestion, stream processing, analytical data store, analysis and reporting, orchestration, etc\n",
    "- benefits : choice in technology, performance through parallelism, scalability, interoperability with existing solutions\n",
    "- challenges : complexity, lack of standards, lack of skills, etc\n",
    "- look at Lambda architecture, Kappa architecture\n",
    "\n",
    "### Locality of reference\n",
    "\n",
    "Levels of storage:\n",
    "- computational data is stored in primary memory aka memory\n",
    "- persistent data is stored in secondary memory aka storage\n",
    "- remote data access from another computer's memory or storage is done over network\n",
    "\n",
    "Types:\n",
    "1. Temporal locality\n",
    "    - recently accessed data is likely to be accessed again\n",
    "    - e.g. loop iterations, function calls\n",
    "2. Spatial locality\n",
    "    - data near recently accessed data is likely to be accessed again\n",
    "    - e.g. sequential access, array traversal\n",
    "    - this is why columnar storage is better than row storage, because it is more likely that columns will be accessed together  for searching, filtering, etc\n",
    "\n",
    "#### Cache performance\n",
    "\n",
    "- cache hit: data requested by processor found in cache\n",
    "- cache miss: data requested by processor not found in cache, must be retrieved from main memory\n",
    "- cache hit ratio: fraction of memory accesses found in cache, $h = \\frac{hits}{hits + misses}$\n",
    "- average access time of any memory access: $t_{avg} = h*t_{cache} + (1-h)*t_{memory}$\n",
    "    - $t_{cache}$ - access time of cache\n",
    "    - $t_{memory}$ - access time of main memory\n",
    "- time required to access main memory block = $\\text{block\\_size} * t_{memory}$\n",
    "- time required to update cache block = $\\text{cache\\_block\\_size} * t_{cache}$\n",
    "\n",
    "Distributed Cache in Hadoop is a mechanism that allows copying small read-only files from HDFS to the local disks of the worker nodes, where they are accessible by the MapReduce tasks. These files are called localized and are tracked by the NodeManager. The files are deleted when they are not used by any task or when the cache size exceeds a limit. The cache size can be configured by a property.\n",
    "\n",
    "### Storage for Big Data\n",
    "\n",
    "RDBMS decline for Big Data due to:\n",
    "1. Scalability: RDBMS scale vertically; NoSQL scales horizontally for better cost-effective expansion.\n",
    "2. Schema flexibility: NoSQL adapts to dynamic data structures, unlike RDBMS with fixed schemas.\n",
    "3. Data variety: NoSQL handles diverse data types effectively compared to RDBMS.\n",
    "4. Query performance: NoSQL databases often outperform RDBMS for certain queries, crucial for Big Data analytics.\n",
    "5. Cost considerations: NoSQL's horizontal scaling on commodity hardware is more budget-friendly than vertical scaling of RDBMS.\n",
    "6. Concurrency and transaction overhead: NoSQL relaxes transaction constraints, prioritizing performance and scalability for Big Data use cases.\n",
    "\n",
    "Database Sharding:\n",
    "- Horizontal partitioning into shards for scalability.\n",
    "- Shards have dedicated hardware.\n",
    "- Enhances performance via workload distribution.\n",
    "- Enables parallel processing and better query performance.\n",
    "- Addresses vertical scaling limitations.\n",
    "- Handles increased data volumes and user loads effectively.\n",
    "- Sharding Types:\n",
    "1. Range: Divides data based on value ranges.\n",
    "2. Hash: Distributes data evenly using a hashing algorithm.\n",
    "3. Directory-Based: Uses a lookup directory for flexible shard assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Parallel and Distributed Systems\n",
    "\n",
    "| Parallel System | Distributed System |\n",
    "| --- | --- |\n",
    "| Computer system with several processing units attached to it | Independent, autonomous systems connected in a network accomplishing specific tasks |\n",
    "| A common shared memory can be directly accessed by every processing unit in a network | Coordination is possible between connected computers with own memory and CPU |\n",
    "| Tight coupling of processing resources that are used for solving single, complex problem | Loose coupling of computers connected in network, providing access to data and remotely located resources |\n",
    "| Programs may demand fine grain parallelism | Programs have coarse grain parallelism |\n",
    "\n",
    "\n",
    "#### Speedup calculations on parallel systems\n",
    "\n",
    "$$ \\text{Execution time after improvement} = \\frac{\\text{Execution time affected by improvement}}{\\text{Amount of improvement}} + \\text{Execution time unaffected} $$\n",
    "Let's say $f$ is the fraction of the code that is infinitely parallelizable, and N is the number of processors. Then, \n",
    "1. Amdahl’s Law\n",
    "    A rule stating that the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used.\n",
    "    $$ \\text{Speedup} = \\frac{\\text{Execution time single processor}}{\\text{Execution time on N parallel processors}} = \\frac{T(1-f) + Tf}{T(1-f) + \\frac{Tf}{N}} = \\frac{(1-f) + f}{(1-f) + \\frac{f}{N}} = \\frac{1}{(1-f) + \\frac{f}{N}} $$\n",
    "    It is used when the workload is fixed, and the number of processors is increased.\n",
    "2. Gustafson’s Law\n",
    "    A rule stating that the speedup with 1 processors for a given workload $W$, should be compared with with the speedup with N processors for a workload of size $W(N)$, where $W(N) = (1 - f)W + fNW$ (Parallelizable work can increase $N$ times)\n",
    "    $$ \\text{Speedup} = \\frac{T \\times W(N)}{T \\times W} = \\frac{W(N)}{W} = \\frac{(1 - f)W + fNW}{W} = 1 - f + fN $$\n",
    "    It is used when the workload size is increased proportionally to the number of processors.\n",
    "\n",
    "#### Memory access models\n",
    "- Shared memory\n",
    "    - multiple processors share a single memory space\n",
    "    - processors communicate by reading and writing to the same memory location\n",
    "    - e.g. OpenMP, Pthreads\n",
    "- Distributed memory\n",
    "    - each processor has its own private memory\n",
    "    - processors communicate by sending messages to each other\n",
    "    - e.g. MPI, PVM\n",
    "\n",
    "#### Shared Memory vs Message Passing:\n",
    "\n",
    "Shared Memory:\n",
    "- Tasks on different processors access a common address space.\n",
    "- Easier for programmers to conceptualize.\n",
    "- Single logical address space mapped onto physical memory.\n",
    "- Implemented as threads in a processor.\n",
    "- Options:\n",
    "    - Send Sync/Async, Blocking/Non-blocking.\n",
    "    - Receive Sync/Async, Blocking/Non-blocking.\n",
    "    - Handling complexities in terms of programming and wait times.\n",
    "  \n",
    "Distributed Memory (Message Passing):\n",
    "- Tasks access data from separate, isolated address spaces.\n",
    "- Communicate via sending/receiving messages.\n",
    "- Requires explicit communication for data exchange.\n",
    "- Harder programming abstraction compared to shared memory.\n",
    "- Data moved across virtual memories.\n",
    "- Harder for programmers due to explicit communication.\n",
    "\n",
    "#### Data access strategies - Replication, Partitioning, Messaging\n",
    "\n",
    "1. Partition \n",
    "    - Strategy: Partition data – typically, equally – to the nodes of the (distributed) system\n",
    "    - Cost: Network access and merge cost when query needs to go across partitions\n",
    "    - Advantage(s): Works well if task/algorithm is (mostly) data parallel, Works well when there is Locality of Reference within a partition\n",
    "    - Concerns: Merge across data fetched from multiple partitions, Partition balancing, Row vs Columnar layouts - what improves locality of reference ?\n",
    "2. Replication\n",
    "    - Strategy: Replicate all data across nodes of the (distributed) system\n",
    "    - Cost: Higher storage cost\n",
    "    - Advantage(s): All data accessed from local disk: no (runtime) communication on the network, High performance with parallel access, Fail over across replicas\n",
    "    - Concerns: Keep replicas in sync — various consistency models between readers and writers\n",
    "3. (Dynamic) Communication\n",
    "    - Strategy: Communicate (at runtime) only the data that is required\n",
    "    - Cost: High network cost for loosely coupled systems and data set to be exchanged is large\n",
    "    - Advantage(s): Minimal communication cost when only a small portion of the data is actually required by each node\n",
    "    - Concerns: Highly available and performant network, Fairly independent parallel data processing\n",
    "4. Networked Storage\n",
    "    - Common Storage on the Network:\n",
    "        - Storage Area Network (for raw access – i.e. disk block access)\n",
    "        - Network Attached Storage (for file access)\n",
    "    - Common Storage on the Cloud:\n",
    "        - Use Storage as a Service\n",
    "        - e.g. Amazon S3\n",
    "\n",
    "#### Computer clusters\n",
    "- type of distributed system that consists of a collection of inter-connected stand-alone computers working together as a single, integrated computing resource\n",
    "- examples:\n",
    "    - High Availability Clusters\n",
    "        - ServiceGuard, Lifekeeper, Failsafe, heartbeat, HACMP, failover clusters\n",
    "    - High Performance Clusters\n",
    "        - Beowulf; 1000 nodes; parallel programs; MPI\n",
    "    - Database Clusters\n",
    "        - Oracle Parallel Server (OPS)\n",
    "    - Storage Clusters\n",
    "        - Cluster filesystems; same view of data from each node\n",
    "- goals:\n",
    "    - continuous availability\n",
    "    - data integrity\n",
    "    - linear scalability\n",
    "    - open access\n",
    "    - parallelism in processing\n",
    "    - distributed systems management\n",
    "- built with high peforance, commodity hardware, high availability, and open source software\n",
    "\n",
    "#### Cloud computing\n",
    "- on-demand availability of computer system resources, especially data storage and computing power, without direct active management by the user\n",
    "- cluster is a building block for a datacenter, which is a building block for a cloud service\n",
    "- motivation to use clusters:\n",
    "    - rate of obsolescence of computers is high\n",
    "    - solution: build a cluster of commodity workstations\n",
    "    - scale-out clusters with commodity workstations as nodes are suitable for software environments that are resilient\n",
    "    - on the other hand, (public) cloud infrastructure is typically built as clusters of servers due to higher reliability of individual servers\n",
    "- typical cluster components:\n",
    "    - processor and memory\n",
    "    - network stack\n",
    "    - local storage\n",
    "    - OS and runtimes\n",
    "- split brain: caused by failure of heartbeat network connection(s), two halves of a cluster keep running, recovery options: allow cluster half with majority number of nodes to survive, force cluster with minority number of nodes to shut down\n",
    "- cluster middleware: single system image infrastructure, cluster services for availability, redundancy, fault-tolerance, recovery from failures\n",
    "- execution time on clusters, depends on : distribued scheduling, local on node scheduling, communication, synchronization, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability and Availability\n",
    "\n",
    "#### Metrics for reliability\n",
    "1. Mean Time To Failure (MTTF)\n",
    "    - average time between failures\n",
    "    - $MTTF = \\frac{\\text{Total hours of operation}}{\\text{Number of failures}} = \\frac{1}{\\text{Failure rate}}$\n",
    "2. Failure Rate\n",
    "    - number of failures per unit time\n",
    "    - $Failure rate = \\frac{1}{MTTF}$\n",
    "3. Mean Time To Repair (MTTR)\n",
    "    - average time to repair a failed component\n",
    "    - $MTTR = \\frac{\\text{Total hours for maintenance}}{\\text{Total number of repairs}}$\n",
    "4. Mean Time To Diagnose (MTTD)\n",
    "5. Mean Time Between Failures (MTBF)\n",
    "    - average time between failures\n",
    "    - $MTBF = MTTD + MTTR + MTTF$\n",
    "\n",
    "#### Metrics for availability\n",
    "1. Availability\n",
    "    - $Availability = \\frac{\\text{Time system is UP and accessible}}{\\text{Total time observed}} = \\frac{MTTF}{MTBF}$\n",
    "    - system is highly available when MTTF is high and MTTR is low\n",
    "\n",
    "#### Metrics for system with multiple components\n",
    "For a system with multiple components, the combined availability of the system is:\n",
    "1. Serial assembly of components\n",
    "    - failure of any component results in system failure\n",
    "    - $A_c = A_a \\times A_b$, where $A_a$ is the availability of component A and $A_b$ is the availability of component B\n",
    "    - failure rate of C = failure rate of A + failure rate of B = $\\frac{1}{MTTF_a} + \\frac{1}{MTTF_b}$\n",
    "    - $MTTF_c = \\frac{1}{\\frac{1}{MTTF_a} + \\frac{1}{MTTF_b}}$\n",
    "2. Parallel assembly of components\n",
    "    - failure of all components results in system failure\n",
    "    - $A_c = 1 - (1 - A_a)(1 - A_b)$\n",
    "    - $MTTF_c = MTTF_a + MTTF_b$\n",
    "\n",
    "#### Fault tolerance configurations\n",
    "\n",
    "| Configuration                 | Failover Time        | Active Component  | Replication Type                   |\n",
    "|-------------------------------|----------------------|-------------------|-------------------------------------|\n",
    "| Active-Active (load balanced)  | No failover time     | All components    | Bidirectional replication         |\n",
    "| Active-Passive (hot standby)   | Few seconds          | One active (passive up to date)        | Unidirectional replication        |\n",
    "| Warm standby                  | Few minutes          | One active (passive not fully up to date)         | Unidirectional replication with delay |\n",
    "| Cold standby                  | Few hours            | One active (passive not up-to-date, not running)        | Replication from secondary backup  |\n",
    "\n",
    "Different topologies:\n",
    "1. N+1 : N active nodes, 1 passive node \n",
    "2. N+M : N active nodes, M passive nodes\n",
    "3. N to 1 : N active nodes, 1 temporary passive node which returns services to active nodes after failure\n",
    "4. N to N : Any node failure is handled by distributing the load to other nodes\n",
    "\n",
    "Recovery:\n",
    "1. Diagnostic : Using heartbeat messages, the system detects the failure of a node\n",
    "2. Backward recovery : The system rolls back the transactions that were in progress at failure from the last checkpoint\n",
    "3. Forward recovery : The system re-executes the transactions that were in progress at failure from diagnosis data\n",
    "\n",
    "### Big Data Analytics\n",
    "Types of analytics:\n",
    "1. Descriptive (what happened)\n",
    "    - objective: summarize, interpret historical data for insights into past events\n",
    "    - methodology: involves data aggregation, summarization, visualization\n",
    "    - example: creating charts to illustrate trends in monthly website traffic\n",
    "2. Diagnostic (why did it happen)\n",
    "    - objective: identify reasons behind past events or trends\n",
    "    - methodology: investigates patterns, anomalies in data to understand root causes\n",
    "    - example: analyzing decrease in customer satisfaction scores for contributing factor\n",
    "3. Predictive (what will happen)\n",
    "    - objective: forecast future outcomes using historical data and patterns\n",
    "    - methodology: utilizes statistical models, machine learning algorithms for predictions\n",
    "    - example: predicting next quarter sales based on previous trends\n",
    "4. Prescriptive (how can we make it happen)\n",
    "    - objective: recommend actions to optimize future outcomes based on analysis\n",
    "    - methodology: combines predictive models with optimization techniques for suggestions\n",
    "    - example: recommending pricing adjustments for products based on predicted market trends\n",
    "\n",
    "Different aspects of big data analytics:\n",
    "1. Working with datasets of huge volume, velocity, variety beyond the capabilities of traditional data processing applications\n",
    "2. Processing data in parallel across multiple nodes in a distributed system\n",
    "3. Using specialized tools and techniques for data storage, processing, and analysis\n",
    "4. Use principles of locality to optimize performance and minimize network traffic\n",
    "5. Use of specialized programming models and languages for distributed computing\n",
    "6. Better faster decisions in real-time\n",
    "7. Richer faster insights from data of customers, products, operations, etc\n",
    "\n",
    "#### Big data analytics lifecycle\n",
    "1. Business Case Evaluation\n",
    "    \n",
    "2. Data Identification\n",
    "3. Data Acquisition & Filtering\n",
    "4. Data Extraction\n",
    "5. Data Validation & Cleansing\n",
    "6. Data Aggregation & Representation\n",
    "7. Data Analysis\n",
    "8. Data Visualization\n",
    "9. Utilization of Analysis Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "- open source framework for distributed storage and processing of large datasets\n",
    "- key components:\n",
    "    - HDFS (Hadoop Distributed File System)\n",
    "    - MapReduce\n",
    "    - YARN (Yet Another Resource Negotiator)\n",
    "\n",
    "<img alt=\"picture 0\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/ebd34154f5a8c257141e1647b0a8a5d7638bbef6f1e1dde8398ff3ce7677aab1.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- distributed storage: HDFS, stores data across multiple nodes for scalability and fault tolerance\n",
    "- distributed processing: MapReduce, allows parallel processing of vast datasets across a cluster of computers\n",
    "- scalability: scales horizontally by adding more nodes to the cluster to accommodate growing data volumes\n",
    "- fault tolerance: data redundancy and automatic recovery mechanisms ensure reliability in the event of hardware or software failures\n",
    "- ecosystem: offers a rich ecosystem with additional tools like:\n",
    "    - data ingestion : \n",
    "        - Sqoop: transfers data between Hadoop and relational databases, from RDBMS to HDFS, populating live tables in Hive and HBase\n",
    "        - Flume: collects, aggregates, and moves large amounts of streaming data into HDFS, from web servers, log files, etc\n",
    "    - data processing : \n",
    "        - MapReduce: distributed processing framework for batch processing of large datasets, supports Java, Python, C++, etc\n",
    "        - Spark: in-memory data processing engine, faster than MapReduce, supports multiple programming languages\n",
    "    - data analysis : \n",
    "        - Hive: data warehouse infrastructure, provides SQL-like query language called HiveQL, supports MapReduce and Spark\n",
    "        - Pig: data flow language and execution framework, supports MapReduce and Tez\n",
    "        - Impala: SQL query engine, supports low-latency queries on Hadoop datasets, supports HiveQL and SQL\n",
    "- programming language agnostic: supports various programming languages, allowing developers to use the language of their choice for writing MapReduce jobs\n",
    "- data locality: optimizes performance by processing data on the same node where it is stored, reducing data transfer overhead\n",
    "- use cases: \n",
    "    - widely used for batch processing, large-scale data analytics, and handling unstructured or semi-structured data\n",
    "    - Hadoop is a cornerstone in the big data landscape, providing a cost-effective and scalable solution for managing and analyzing massive datasets\n",
    "\n",
    "#### HDFS Architecture\n",
    "\n",
    "<img alt=\"picture 1\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/1bad92038b580b8fd5e36ac5de6de728a0a9fc8458b89f14e7016228cdb8df75.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Master slave architecture within a HDFS cluster\n",
    "- Master node with NameNode\n",
    "    - maintains namespace - filename to blocks and their replica mappings\n",
    "    - serves as arbitrator and doesn't handle actual data flow\n",
    "    - HDFS client app interacts with NameNode for metadata\n",
    "- Slave nodes with DataNode\n",
    "    - serves block read/write from clients\n",
    "    - serves create/delete/replicate requests from NameNode\n",
    "    - DataNodes interact with each other for pipeline reads and writes\n",
    "- NameNode functions:\n",
    "    - maintains and manages the file system namespace, with two files:\n",
    "        1. FsImage: contains mapping of blocks to file, hierarchy, file properties, permissions\n",
    "        2. EditLog: transaction log of changes to metadata in FsImage\n",
    "    - does not store any data, only metadata about files\n",
    "    - runs on master node while DataNodes run on slave nodes\n",
    "    - records each change that takes place to the metadata, e.g. if a file is deleted in HDFS, the NameNode will immediately record this in the EditLog\n",
    "    - receives periodic heartbeat and a block report from all the DataNodes in the cluster to ensure that the DataNodes are live\n",
    "    - ensures replication factor is maintained across DataNode failures\n",
    "    - in case of the DataNode failure, the NameNode chooses new DataNodes for new replicas, balance disk usage and manages the communication traffic to the DataNodes\n",
    "- DataNode functions:\n",
    "    - stores data in the local file system\n",
    "    - sends heartbeat messages to the NameNode periodically to confirm that it is alive\n",
    "    - sends block report to the NameNode periodically to report the list of blocks it is storing\n",
    "    - serves read/write requests from clients\n",
    "    - serves create/delete/replicate requests from NameNode\n",
    "    - in case of a block failure, the NameNode will choose a new DataNode to create a replica of the block\n",
    "- Secondary NameNode functions:\n",
    "    - performs periodic checkpoints of the namespace by merging the FsImage and EditLog\n",
    "    - downloads the FsImage and EditLog from the NameNode, merges them, and uploads the new FsImage back to the NameNode\n",
    "    - does not store any data, only metadata about files\n",
    "    - runs on a separate node from the NameNode\n",
    "    - performs regular checkpoints of the namespace by merging the FsImage and EditLog, hence called CheckpointNode\n",
    "    \n",
    "#### YARN Architecture\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8e7bb7d535fc51c60dcb642eb68e8e9ebc5c9688579c2011d20d3b059d71596d.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "YARN workflow:\n",
    "1. client program submits the application / job with specs to start AppMaster\n",
    "2. ResourceManager asks a NodeManager to start a container which can host the ApplicationMaster and then launches ApplicationMaster\n",
    "3. ApplicationMaster on start-up registers with ResourceManager. So now the client can contact the ApplicationMaster directly also for application specific details\n",
    "4. As the application executes, AppMaster negotiates resources in the form of containers via the resource request protocol involving the ResourceManager\n",
    "5. As a container is allocated successfully for an application, AppMaster works with the NodeManager on same or diff node to launch the container as per the container spec. The spec involves how the AppMaster can communicate with the container\n",
    "6. App specific code inside container provides runtime information to AppMaster for progress, status etc. via application-specific protocol\n",
    "7. Client that submitted the app / job can directly communicate with the AppMaster for progress, status updates. via the application specific protocol\n",
    "8. On completion of the app / job, AppMaster de-registers from ResourceManager and shuts down. So the containers allocated can be re-purposed.\n",
    "\n",
    "#### Modes of operation\n",
    "1. Local (Standalone) Mode\n",
    "    - default mode, runs on a single node, no HDFS, no YARN\n",
    "    - used for debugging purposes\n",
    "2. Pseudo-Distributed Mode\n",
    "    - runs on a single node, HDFS, YARN\n",
    "    - all the daemons will be running as a separate Java process on separate JVMs\n",
    "    - used for development purposes\n",
    "3. Fully-Distributed Mode\n",
    "    - runs on clusters with multiple nodes, HDFS, YARN\n",
    "    - few of the nodes run master daemons like NameNode, ResourceManager, etc\n",
    "    - rest of the nodes run slave daemons like DataNode, NodeManager, etc\n",
    "    - all the daemons will be running as a separate Java process on separate JVMs\n",
    "    - used for production purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP Theorem\n",
    "- Brewer's conjecture: a distributed system cannot simultaneously provide all three of the following guarantees:\n",
    "    - Consistency: every read receives the most recent write or an error\n",
    "    - Availability: every request receives a (non-error) response, without the guarantee that it contains the most recent write\n",
    "    - Partition tolerance: the system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes\n",
    "- Different design choices for distributed systems:\n",
    "    - CA: All RDBMS, single data center, strong consistency, no network partition\n",
    "    - CP: HDFS, MongoDB, Redis, single data center, strong consistency, network partition\n",
    "    - AP: Cassandra, CouchDB, DynamoDB, multiple data centers, eventual consistency, network partition\n",
    "\n",
    "#### ACID properties\n",
    "- Atomicity: all or nothing, transaction is either fully completed or not at all\n",
    "- Consistency: transaction must bring the database from one valid state to another\n",
    "- Isolation: concurrent transactions do not interfere with each other\n",
    "- Durability: once a transaction is committed, it will remain so\n",
    "\n",
    "#### BASE properties\n",
    "A database design that sacrifices consistency for availability and partition tolerance\n",
    "- Basically Available: system guarantees availability (will always return a response) but not consistency (may return stale data)\n",
    "- Soft state: state of the system may be inconsistent, thus results might change over time even without input (as data is updated asynchronously)\n",
    "- Eventual consistency: system will become consistent over time, given that the system doesn't receive input during that time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB vs Cassandra\n",
    "\n",
    "| Feature                    | MongoDB                         | Cassandra                       |\n",
    "|----------------------------|---------------------------------|---------------------------------|\n",
    "| Data Model             | Document-based (BSON format)    | Wide-column store               |\n",
    "| Query Language         | MongoDB Query Language (MQL)     | CQL (Cassandra Query Language)  |\n",
    "| Schema                 | Dynamic schema (schema-less)     | Schema-agnostic                |\n",
    "| Consistency Model      | Eventual consistency            | Tunable consistency (can be adjusted per query) |\n",
    "| Scaling                | Horizontal scaling              | Linearly scalable               |\n",
    "| Indexing               | Rich indexing options            | Primary and secondary indexes   |\n",
    "| Transactions           | Supports multi-document transactions | Limited support for transactions |\n",
    "| Joins                  | Supports joins (with limitations) | No support for traditional joins |\n",
    "| Data Distribution      | Sharding for horizontal scaling | Automatic data distribution across nodes |\n",
    "| Use Case               | General-purpose, diverse use cases | Time-series data, write-intensive applications |\n",
    "| ACID Compliance        | Supports ACID transactions (in certain configurations) | ACID compliance with tunable consistency |\n",
    "\n",
    "### Common MongoDB commands:\n",
    "1. show databases: `show databases` / `show dbs`\n",
    "2. switch database: `use <database_name>`\n",
    "3. show collections: `show collections`\n",
    "4. create collection: `db.createCollection(\"<collection_name>\")`\n",
    "4. insert document: `db.<collection_name>.insert({ key: value })`\n",
    "5. find documents: `db.<collection_name>.find()`\n",
    "6. query with criteria: \n",
    "    - `db.<collection_name>.find({ key: value })`\n",
    "    - `db.<collection_name>.find({ key: value }, { key: 1, _id: 0 })` (projection)\n",
    "    - `db.<collection_name>.find({ key: { $gt: value } })` (greater than)\n",
    "    - `db.<collection_name>.find({ key1: value1, key2: value2 })` (AND)\n",
    "    - `db.<collection_name>.find({ $or: [{ key1: value1 }, { key2: value2 }] })` (OR)\n",
    "    - `db.<collection_name>.find({ key: { $in: [value1, value2] } })` (IN)\n",
    "7. update document: \n",
    "    - `db.<collection_name>.update({ key: value }, { $set: { new_key: new_value } })`\n",
    "    - `db.<collection_name>.update({ key: value }, { $set: { new_key: new_value } }, { upsert: true })`\n",
    "8. delete document: `db.<collection_name>.remove({ key: value })`\n",
    "9. aggregate: `db.<collection_name>.aggregate([ ... ])`\n",
    "10. create index: `db.<collection_name>.createIndex({ key: 1 })`\n",
    "11. drop index: `db.<collection_name>.dropIndex(\"index_name\")`\n",
    "12. count documents: `db.<collection_name>.count()`\n",
    "13. limit results: `db.<collection_name>.find().limit(5)`\n",
    "14. sort results: \n",
    "    - `db.<collection_name>.find().sort({ key: 1 })`\n",
    "    - `db.<collection_name>.find().sort({ key1: 1, key2: -1 })` (sort by key1 ascending, then by key2 descending)\n",
    "15. projection (select fields): `db.<collection_name>.find({}, { key: 1, _id: 0 })`\n",
    "16. bulk write operations: `db.<collection_name>.bulkWrite([ ... ])`\n",
    "17. show help: `db.<collection_name>.help()`\n",
    "\n",
    "#### Aggregation pipeline\n",
    "- framework for data aggregation modeled on the concept of data processing pipelines\n",
    "- documents enter a multi-stage pipeline that transforms the documents into an aggregated result\n",
    "- each stage transforms the documents as they pass through the pipeline\n",
    "- `db.<collection_name>.aggregate(pipeline, options)`\n",
    "- eg: \n",
    "    1. `$match` stage filters the documents by some criteria\n",
    "    2. `$group` stage groups the documents by some criteria\n",
    "    3. `$sort` stage sorts the documents by some criteria\n",
    "    4. `$project` stage selects some fields from the documents\n",
    "    5. `$limit` stage limits the number of documents to be returned\n",
    "    6. `$project` stage selects some fields from the documents\n",
    "\n",
    "\n",
    "Import data from CSV:\n",
    "\n",
    "`mongoimport --db <database_name> --collection <collection_name> --type csv --headerline --file <file_name>`\n",
    "\n",
    "Export data to CSV:\n",
    "\n",
    "`mongoexport --db <database_name> --collection <collection_name> --type csv --fields <field1,field2> --out <file_name>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of parallelism\n",
    "\n",
    "- Data Parallelism:\n",
    "  - Definition: Distribute data across multiple processing units or nodes; perform the same operation concurrently.\n",
    "  - Example: Distribute portions of a large dataset to multiple processors; each processor independently processes its assigned data.\n",
    "\n",
    "- Tree Parallelism:\n",
    "  - Definition: Organize parallel tasks hierarchically in a tree-like structure; tasks are divided into sub-tasks forming a tree structure.\n",
    "  - Example: Main task divided into sub-tasks; each sub-task further divided into more specific tasks for efficient resource utilization.\n",
    "\n",
    "- Task Parallelism:\n",
    "  - Definition: Break down a program into independent tasks or processes; execute tasks concurrently.\n",
    "  - Example: Execute different program functions or modules concurrently without relying on each other's output; common in parallel programming frameworks.\n",
    "\n",
    "- Request Parallelism:\n",
    "  - Definition: Divide a computation into stages in a pipeline; each stage represents a distinct operation.\n",
    "  - Example: Tasks in a pipeline architecture are handled by separate units; each stage operates on data concurrently. Used in scenarios with sequential dependence of operations.\n",
    "\n",
    "## Map Reduce \n",
    "- programming model for processing large datasets in parallel across a cluster of computers\n",
    "- MapReduce is a framework for processing data in parallel across a cluster of computers\n",
    "\n",
    "A MapReduce framework (or system) is usually composed of three operations (or steps):\n",
    "1. Map: each worker node applies the map function to the local data, and writes the output to a temporary storage. A master node ensures that only one copy of the redundant input data is processed.\n",
    "  `Map(k1,v1) → list(k2,v2)`\n",
    "2. Shuffle: worker nodes redistribute data based on the output keys (produced by the map function), such that all data belonging to one key is located on the same worker node.\n",
    "3. Reduce: worker nodes now process each group of output data, per key, in parallel.\n",
    "  `Reduce(k2, list (v2)) → list((k3, v3))`\n",
    "\n",
    "```python\n",
    "function map(String name, String document):\n",
    "    // name: document name\n",
    "    // document: document contents\n",
    "    for each word w in document:\n",
    "        emit (w, 1)\n",
    "\n",
    "function reduce(String word, Iterator partialCounts):\n",
    "    // word: a word\n",
    "    // partialCounts: a list of aggregated partial counts\n",
    "    sum = 0\n",
    "    for each pc in partialCounts:\n",
    "        sum += pc\n",
    "    emit (word, sum)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "- open-source, distributed computing system that provides a fast and general-purpose cluster-computing framework for big data processing\n",
    "- developed to address the limitations of the MapReduce model and offers a more flexible and efficient alternative\n",
    "- features:\n",
    "    - speed: performs in-memory processing, which significantly improves the processing speed compared to the traditional MapReduce model that relies heavily on disk-based storage\n",
    "    - ease of use: provides high-level APIs in Java, Scala, Python, and R, making it accessible to a broad audience\n",
    "    - versatility: supports a wide range of data processing tasks, including batch processing, interactive queries, streaming analytics, and machine learning\n",
    "    - in-memory processing: utilizes resilient distributed datasets (RDDs), an immutable distributed collection of objects, to store data in-memory across a cluster\n",
    "    - fault tolerance: provides fault tolerance through lineage information stored in RDDs\n",
    "    - data processing libraries: comes with built-in libraries for various data processing tasks, such as Spark SQL for structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time data processing\n",
    "    - ease of integration: can be easily integrated with other popular big data technologies, such as Apache Hadoop, Apache Hive, Apache HBase, and more\n",
    "    - lazy evaluation: uses lazy evaluation, meaning that transformations on RDDs are not executed immediately\n",
    "    - community support: has a large and active open-source community, which contributes to its development and provides support through forums, mailing lists, and documentation\n",
    "    - cluster manager integration: can run on various cluster managers, including Apache Mesos, Apache Hadoop YARN, and its standalone built-in cluster manager\n",
    "\n",
    "<img alt=\"picture 3\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/165f411f2ed28f5b894819b57bc4f58dc4aebf98a14e9194a18cb50f9d98d8b4.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "2 main abstractions:\n",
    "1. Resilient Distributed Dataset (RDD)\n",
    "    - immutable, distributed collection of objects\n",
    "    - partitioned across nodes in a cluster\n",
    "    - can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs\n",
    "    - can be cached in memory across machines, can be recomputed if lost due to node failure\n",
    "2. Directed Acyclic Graph (DAG)\n",
    "    - sequence of computations on data\n",
    "    - each node in the graph represents a RDD, each edge represents a transformation on the data\n",
    "    - transformations are lazy, only executed when an action is called, evaluated in parallel, fault-tolerant, can be recomputed if lost due to node failure\n",
    "\n",
    "## Apache Flume\n",
    "\n",
    "- distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data\n",
    "- designed to ingest streaming data from various sources and deliver it to a centralized data store\n",
    "- key features:\n",
    "    - data ingestion: collects and aggregates log data from multiple sources, such as web servers, application logs, and social media feeds\n",
    "    - reliability: ensures reliable data delivery through fault-tolerant mechanisms, such as data replication and error handling\n",
    "    - scalability: scales horizontally to handle large volumes of data by distributing the workload across multiple nodes\n",
    "    - extensibility: supports custom data sources and sinks through a flexible plugin architecture\n",
    "    - fault tolerance: provides fault tolerance through data replication, checkpointing, and recovery mechanisms\n",
    "- Flume event : unit of data flow having a byte payload and an optional set of string attributes \n",
    "- components:\n",
    "    - Source: generates events and sends them to the channel\n",
    "    - Channel: stores events until they are consumed by the sink\n",
    "    - Sink: removes events from the channel and delivers them to the destination (or forwards them to the next Flume agent in the flow)\n",
    "- allows for multi-hop flows, where events are passed from one agent to another in a chain-like manner and fan-in and fan-out flows, where events are aggregated or distributed across multiple agents\n",
    "\n",
    "<img alt=\"picture 5\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/a40d6417c862e8ac4fcc9752f5fb3f90b08de40422e048ea1285659b4558c067.png\" width=\"500\"  style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- sample configuration file:\n",
    "    ```\n",
    "    # example.conf: A single-node Flume configuration  \n",
    "    # Name the components on this agent  \n",
    "    a1.sources = r1  \n",
    "    a1.sinks = k1  \n",
    "    a1.channels = c1  \n",
    "    # Describe/configure the source  \n",
    "    a1.sources.r1.type = netcat  \n",
    "    a1.sources.r1.bind = localhost  \n",
    "    a1.sources.r1.port = 44444  \n",
    "    # Describe the sink  \n",
    "    a1.sinks.k1.type = logger  \n",
    "    # Use a channel which buffers events in memory  \n",
    "    a1.channels.c1.type = memory  \n",
    "    a1.channels.c1.capacity = 1000  \n",
    "    a1.channels.c1.transactionCapacity = 100  \n",
    "    # Bind the source and sink to the channel  \n",
    "    a1.sources.r1.channels = c1  \n",
    "    a1.sinks.k1.channel = c1  \n",
    "    ```\n",
    "- sample command to start Flume agent:\n",
    "    `bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console`\n",
    "- the config file defines a Flume agent with a source that listens on localhost:44444, a sink that logs events to the console, and a memory channel with a capacity of 1000 events\n",
    "- the config directory contains additional configuration files for Flume agents, sources, sinks, and channels. It would contain the flume-env.sh file for environment variables, flume-site.xml for site-specific configurations, and log4j.properties for logging configuration\n",
    "- it also supports Agent configurations via Zookeeper, where agents can be configured and managed centrally using Zookeeper\n",
    "- doing consolidation:\n",
    "    <img alt=\"picture 6\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/9e4067cddea1a1375f15e964b7f2c467cf6227c2f49b5963b18b0fa44239afff.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "- multiplexing the flow:\n",
    "    <img alt=\"picture 7\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/57d4cf63a41c045f3bd5efc76991e87e3ec4e905636769d0b7d40408eb97e3f3.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "- channel configuration parameters: `capacity`, `transactionCapacity`, `keepAlive`, `byteCapacity`, etc\n",
    "- source configuration parameters: `type`, `bind`, `port`, `channels`, `selector.type`, `selector.optional`, etc\n",
    "- sink configuration parameters: `type`, `channel`, `batchSize`, `eventSize`, `writeFormat`, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Sqoop\n",
    "\n",
    "- tool designed for efficiently transferring bulk data between Apache Hadoop and structured data stores, such as relational databases\n",
    "- you can use Sqoop to import data from a relational database management system (RDBMS) into HDFS, Hive, or HBase, transform the data in Hadoop MapReduce, and then export the data back into an RDBMS\n",
    "- automates the process of importing and exporting data using the database schema to generate the necessary Hadoop code\n",
    "- uses MapReduce to import and export data, providing parallel data transfer\n",
    "\n",
    "Commands:\n",
    "```py\n",
    "# Imports a table from a database into Hadoop.\n",
    "sqoop import --connect jdbc:mysql://hostname/database --username user --password pass --table tablename --target-dir /path/to/hdfs/dir\n",
    "\n",
    "sqoop import \\\n",
    "  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \\\n",
    "  --split-by a.id --target-dir /user/foo/joinresults\n",
    "\n",
    "# Exports a Hadoop dataset back to a database.\n",
    "sqoop export --connect jdbc:mysql://hostname/database --username user --password pass --table tablename --export-dir /path/to/hdfs/dir\n",
    "\n",
    "sqoop list-databases --connect jdbc:mysql://hostname --username user --password pass\n",
    "\n",
    "sqoop list-tables --connect jdbc:mysql://hostname/database --username user --password pass\n",
    "\n",
    "sqoop eval --connect jdbc:mysql://hostname/database --username user --password pass --query \"SELECT * FROM tablename LIMIT 10\"\n",
    "\n",
    "sqoop job --create jobname -- import/export (other options)\n",
    "```\n",
    "\n",
    "| Feature                    | Sqoop                           | Flume                           |\n",
    "|----------------------------|---------------------------------|---------------------------------|\n",
    "| Data Flow | Various RDBMS, NoSQL, can map to Hive/HBase from RDBMS | Streaming data, e.g. logs |\n",
    "| Loading type | Not event driven | Event driven |\n",
    "| Source integration | Connector driven | Agent driven |\n",
    "| Usage | Structured sources | Streaming systems with semi-structured data, e.g. twitter feed, web server logs |\n",
    "| Performance and reliability | Parallel transfer with high utilisation | Reliable transfer with aggregations at low latency |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Zookeeper [[Link]](https://zookeeper.apache.org/doc/r3.9.2/zookeeperOver.html) [[SO Link]](https://stackoverflow.com/a/70486661)\n",
    "\n",
    "- centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\n",
    "- used for managing distributed systems and ensuring consistency across a cluster of machines\n",
    "- key features:\n",
    "    - configuration management: stores and manages configuration information for distributed systems\n",
    "    - naming service: provides a hierarchical namespace for distributed systems\n",
    "    - synchronization: provides distributed synchronization and coordination services\n",
    "    - group services: allows distributed applications to form groups and perform group operations\n",
    "    - fault tolerance: provides fault tolerance through replication and leader election mechanisms\n",
    "    - consistency: ensures consistency across distributed systems through atomic broadcast and consensus algorithms\n",
    "- working:\n",
    "    - Zookeeper maintains a hierarchical namespace called znodes, similar to a file system directory structure\n",
    "    - znodes can be ephemeral, persistent, or sequential, and can store data associated with them\n",
    "    - servers replicated across a set of nodes called an ensemble, with one node acting as the leader and others as followers\n",
    "    - servers maintain an in-memory image of state, along with transaction logs and snapshots\n",
    "    - service is available as long as a majority of the ensemble is available\n",
    "    - clients connect to any server in the ensemble (TCP connection) and can read, write, and watch znodes, and send heartbeats to maintain session\n",
    "- CP system (not A) since if leader cannot be elected (no quorum), no new writes can be processed\n",
    "- All client transactions are time stamped and ordered\n",
    "- Typically meant for read-heavy workloads (10:1)\n",
    "\n",
    "\n",
    "<img alt=\"picture 8\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/035b4d5650d6f9a5b86ab84e1b7bba004278bb67226e4d2833c67401c62c10d3.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Hierarchical Namespace\n",
    "<img alt=\"picture 9\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/076643ad5d2dd8abd12080828d6b8bf77f0df754b586f0373bb8ab511101c587.png\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "It supports only these operations:\n",
    "- create : creates a node at a location in the tree\n",
    "- delete : deletes a node\n",
    "- exists : tests if a node exists at a location\n",
    "- get data : reads the data from a node\n",
    "- set data : writes data to a node\n",
    "- get children : retrieves a list of children of a node\n",
    "- sync : waits for data to be propagated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Oozie\n",
    "\n",
    "- workflow scheduler system to manage Apache Hadoop jobs\n",
    "- allows users to define workflows to execute a series of actions in a specific order (DAGs of actions) - called Oozie workflow jobs\n",
    "- supports various Hadoop jobs, such as MapReduce, Pig, Hive, Sqoop, and Spark\n",
    "- Oozie coordinator jobs are recurrent Oozie workflow jobs triggered by time (frequency) and data availability\n",
    "- Oozie actions can be Hadoop filesystem actions, Hadoop MapReduce jobs, Pig jobs, Hive jobs, Sqoop jobs, and Oozie sub-workflow action\n",
    "\n",
    "Example workflow definition:\n",
    "```xml\n",
    "<workflow-app name='wordcount-wf' xmlns=\"uri:oozie:workflow:0.1\">\n",
    "    <start to='wordcount'/>\n",
    "    <action name='wordcount'>\n",
    "        <map-reduce>\n",
    "            <job-tracker>${jobTracker}</job-tracker>\n",
    "            <name-node>${nameNode}</name-node>\n",
    "            <configuration>\n",
    "                <property>\n",
    "                    <name>mapred.mapper.class</name>\n",
    "                    <value>org.myorg.WordCount.Map</value>\n",
    "                </property>\n",
    "                <property>\n",
    "                    <name>mapred.reducer.class</name>\n",
    "                    <value>org.myorg.WordCount.Reduce</value>\n",
    "                </property>\n",
    "                <property>\n",
    "                    <name>mapred.input.dir</name>\n",
    "                    <value>${inputDir}</value>\n",
    "                </property>\n",
    "                <property>\n",
    "                    <name>mapred.output.dir</name>\n",
    "                    <value>${outputDir}</value>\n",
    "                </property>\n",
    "            </configuration>\n",
    "        </map-reduce>\n",
    "        <ok to='end'/>\n",
    "        <error to='end'/>\n",
    "    </action>\n",
    "    <kill name='kill'>\n",
    "        <message>Something went wrong: ${wf:errorCode('wordcount')}</message>\n",
    "    </kill/>\n",
    "    <end name='end'/>\n",
    "</workflow-app>\n",
    "```\n",
    "\n",
    "<img alt=\"picture 10\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/51ccbd6ff6e968b54e00c973b48c18a04d0936d1805246afa5e97f17d3de2cd3.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NoSQL Databases\n",
    "\n",
    "Characteristics of NoSQL databases:\n",
    "- Not Only SQL: non-relational databases that do not use SQL as their primary query language\n",
    "- Non-relational data model: schema-less, flexible data model that can handle unstructured, semi-structured, and structured data\n",
    "- Schema-less design: allows for dynamic schema changes without predefined schema constraints\n",
    "- Loosen consistency to address scalability and availability requirements in large-scale applications\n",
    "- Open source movement born out of web-scale applications\n",
    "- Distributed for scale\n",
    "- Cluster-friendly\n",
    "- Focus on CP or AP in CAP, while RDBMS focuses on CA for single nodes (ACID properties)\n",
    "- Auto sharding and replication\n",
    "    - sharding : horizontal partitioning of data across multiple nodes to distribute the load and improve performance\n",
    "    - replication : copies of data stored on multiple nodes to ensure fault tolerance and high availability\n",
    "\n",
    "Use cases : big data, real-time web applications, cloud-based applications, content management systems, social media applications, IoT applications, e-commerce applications\n",
    "Typical web scale systems do not need strict consistency and durability guarantees, but need to be highly available and partition tolerant like social networks, recommendation engines, retail catalogs, reviews and blogs, etc\n",
    "\n",
    "Types of NoSQL databases:\n",
    "1. Key-Value Stores: simple data model with a hash table of key and a value, e.g. Redis, DynamoDB\n",
    "    - value can be a complex data structure or a simple object/record\n",
    "2. Document Stores: store data in documents, e.g. MongoDB, CouchDB\n",
    "    - formats like JSON\n",
    "    - documents accessed by unique key, or indexed fields\n",
    "3. Column Stores: store data in columns rather than rows, each storage block contains data from only one column, e.g. Cassandra, HBase\n",
    "    - allows versioning of data, efficient for read-heavy workloads\n",
    "4. Graph Databases: store data in graph structures with nodes, edges, and properties, e.g. Neo4j, Amazon Neptune\n",
    "    - optimized for graph operations like traversals, shortest path, etc\n",
    "\n",
    "## MongoDB \n",
    "- open-source, document-oriented NoSQL database that provides high performance, high availability, and easy scalability\n",
    "- key features:\n",
    "    - document-based data model: stores data in flexible, JSON-like documents called BSON (Binary JSON)\n",
    "    - dynamic schema: allows for schema-less design, enabling changes to the data structure without downtime\n",
    "    - high availability: supports replica sets for automatic failover and data redundancy\n",
    "    - horizontal scalability: scales horizontally through sharding, distributing data across multiple nodes\n",
    "    - rich query language: supports a powerful query language with features like secondary indexes, aggregation, and full-text search\n",
    "    - flexible data model: supports complex data structures, nested arrays, and sub-documents\n",
    "    - secondary indexes: allows for efficient querying and indexing of data\n",
    "    - aggregation framework: provides a powerful framework for data aggregation and analysis\n",
    "    - geospatial indexing: supports geospatial queries and indexing for location-based data\n",
    "    - text search: provides full-text search capabilities for text data\n",
    "    - transactions: supports multi-document transactions for ACID compliance\n",
    "- data model:\n",
    "    - collections: equivalent to tables in relational databases, store documents\n",
    "    - documents: equivalent to rows in relational databases, store data in BSON format\n",
    "    - fields: equivalent to columns in relational databases, store key-value pairs\n",
    "    - indexes: improve query performance by creating indexes on fields\n",
    "\n",
    "Example of a join:\n",
    "```javascript\n",
    "// Sample 'orders' collection\n",
    "db.orders.insertMany([\n",
    "  { _id: 1, productId: 101, quantity: 2 },\n",
    "  { _id: 2, productId: 102, quantity: 1 },\n",
    "  { _id: 3, productId: 101, quantity: 3 }\n",
    "]);\n",
    "\n",
    "// Sample 'products' collection\n",
    "db.products.insertMany([\n",
    "  { _id: 101, name: 'Product 1', price: 10 },\n",
    "  { _id: 102, name: 'Product 2', price: 20 }\n",
    "]);\n",
    "\n",
    "// Perform a join operation using aggregate\n",
    "db.orders.aggregate([\n",
    "  {\n",
    "    $lookup: {\n",
    "      from: 'products',\n",
    "      localField: 'productId',\n",
    "      foreignField: '_id',\n",
    "      as: 'product'\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    $unwind: '$product'\n",
    "  },\n",
    "  {\n",
    "    $project: {\n",
    "      _id: 1,\n",
    "      productId: 1,\n",
    "      quantity: 1,\n",
    "      productName: '$product.name',\n",
    "      productPrice: '$product.price'\n",
    "    }\n",
    "  }\n",
    "]);\n",
    "```\n",
    "\n",
    "Replication:\n",
    "1. Primary-Secondary Replication: primary node for read and write operations, secondary nodes for read-only operations\n",
    "2. Replica Sets: group of MongoDB instances that maintain the same data set, provide redundancy and high availability\n",
    "3. Automatic Failover: in case of primary node failure, a secondary node is automatically elected as the new primary\n",
    "4. Data Redundancy: data is replicated across multiple nodes to ensure fault tolerance and data durability\n",
    "\n",
    "Read concern levels:\n",
    "1. \"local\": returns the most recent data available on the node\n",
    "2. \"majority\": returns data that has been written to a majority of nodes\n",
    "3. \"linearizable\": returns data that reflects all successful writes that have been acknowledged by a majority of nodes\n",
    "\n",
    "Write concern levels:\n",
    "1. \"acknowledged\": returns after the write operation has been acknowledged by the primary node\n",
    "  - 1: returns after the write operation has been acknowledged by the primary node\n",
    "  - 0: returns after the write operation has been sent to the primary node\n",
    "  - n (number): returns after the write operation has been acknowledged by n nodes\n",
    "2. \"majority\": returns after the write operation has been acknowledged by a majority of nodes\n",
    "3. \"journaled\": returns after the write operation has been acknowledged by the primary node and written to the journal\n",
    "\n",
    "Consistency scenarios:\n",
    "1. read=\"majority\", write=\"majority\": casually consistent and durable\n",
    "2. read=\"majority\", write=\"acknowledged\": causally consistent but not durable\n",
    "3. read=\"local\", write=\"majority\": eventually consistent and durable writes\n",
    "4. read=\"local\", write=\"acknowledged\": eventually consistent but not durable\n",
    "\n",
    "## Apache Cassandra\n",
    "- open-source, distributed NoSQL database designed for scalability and high availability without compromising performance\n",
    "- key features:\n",
    "    - linear scalability: scales linearly by adding more nodes to the cluster\n",
    "    - fault tolerance: provides fault tolerance through data replication and automatic data distribution\n",
    "    - tunable consistency: allows users to configure consistency levels based on their requirements\n",
    "    - flexible data model: supports wide-column store with a schema-less design\n",
    "    - eventual consistency: provides eventual consistency for read and write operations\n",
    "    - built-in caching: supports in-memory caching for improved read performance\n",
    "    - support for ACID transactions: supports atomicity, consistency, isolation, and durability for transactions\n",
    "    - support for secondary indexes: allows for efficient querying of data\n",
    "    - support for custom compaction strategies: provides flexibility in managing data compaction\n",
    "- AP system (not C) since it focuses on availability and partition tolerance\n",
    "- data model:\n",
    "    - keyspace: equivalent to a database in relational databases, contains column families\n",
    "    - column family: equivalent to a table in relational databases, contains rows and columns\n",
    "    - partition key: used to distribute data across nodes, determines the partition where the data is stored\n",
    "    - clustering key: used to define the order of data within a partition\n",
    "    - secondary index: allows for querying data based on non-primary key columns\n",
    "- replication strategies:\n",
    "    - SimpleStrategy: used for single data center deployments (specify replication factor = N)\n",
    "    - NetworkTopologyStrategy: used for multi-data center deployments\n",
    "- consistency levels: for both read and write operations\n",
    "    - ONE, TWO, THREE, ALL - number of replicas that must respond to a read/write operation\n",
    "    - QUORUM - majority of replicas must respond\n",
    "    - LOCAL_QUORUM - majority of replicas in the local data center must respond\n",
    "    - ANY - at least one replica must respond\n",
    "\n",
    "Consisitency scenarios:\n",
    "1. QUORUM reads and writes: causally consistent and durable\n",
    "\n",
    "Sample queries:\n",
    "```sql\n",
    "CREATE KEYSPACE my_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };\n",
    "\n",
    "CREATE TABLE my_keyspace.my_table (\n",
    "    id UUID PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    age INT\n",
    ");\n",
    "\n",
    "INSERT INTO my_keyspace.my_table (id, name, age) VALUES (uuid(), 'Alice', 30);\n",
    "\n",
    "SELECT * FROM my_keyspace.my_table WHERE name = 'Alice';\n",
    "\n",
    "UPDATE my_keyspace.my_table SET age = 31 WHERE name = 'Alice';\n",
    "\n",
    "DELETE FROM my_keyspace.my_table WHERE name = 'Alice';\n",
    "```\n",
    "\n",
    "## Neo4j\n",
    "\n",
    "- open-source, graph database that uses graph structures with nodes, edges, and properties to represent and store data\n",
    "- key features:\n",
    "    - graph data model: stores data in nodes, relationships, and properties\n",
    "    - native graph storage: optimized for storing and querying graph data\n",
    "    - property graph model: supports properties on nodes and relationships\n",
    "    - Cypher query language: expressive query language for graph data\n",
    "    - ACID transactions: supports atomicity, consistency, isolation, and durability for transactions\n",
    "    - high performance: optimized for graph traversals and pattern matching\n",
    "    - scalability: scales horizontally through clustering and sharding\n",
    "    - built-in indexing: provides indexing for efficient querying of graph data\n",
    "    - graph algorithms: supports a wide range of graph algorithms for analyzing graph data\n",
    "    - visualization: provides tools for visualizing and exploring graph data\n",
    "    - integration: integrates with various programming languages and frameworks\n",
    "\n",
    "Data model:\n",
    "- nodes: entities in the graph, represent entities like people, products, etc\n",
    "- relationships: connect nodes, represent relationships between entities\n",
    "- properties: key-value pairs associated with nodes and relationships, represent attributes of entities and relationships\n",
    "- labels: used to categorize nodes, represent types of entities\n",
    "- indexes: used to quickly look up nodes based on properties\n",
    "\n",
    "Sample queries:\n",
    "```cypher\n",
    "CREATE (alice:Person { name: 'Alice', age: 30 });\n",
    "CREATE (bob:Person { name: 'Bob', age: 35 });\n",
    "\n",
    "MATCH (alice:Person { name: 'Alice' }) RETURN alice;\n",
    "\n",
    "MATCH (alice:Person { name: 'Alice' }) SET alice.age = 31 RETURN alice;\n",
    "\n",
    "MATCH (alice:Person { name: 'Alice' }) DELETE alice;\n",
    "\n",
    "MATCH (:Person { name: 'Tom Hanks' })-[r:ACTED_IN]->(m:Movie), (:Person { name: 'Ron Howard' })-[r:DIRECTED]->(m) WHERE m.released > 2000 RETURN m LIMIT 10;\n",
    "\n",
    "MATCH (p:Person)-[r:ACTED_IN]->(m:Movie) WHERE p.name = 'Tom Hanks' RETURN m.title;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "- open-source, distributed computing system that provides a fast and general-purpose cluster-computing framework for big data processing\n",
    "- developed to address the limitations of the MapReduce model and offers a more flexible and efficient alternative\n",
    "- features:\n",
    "    - speed: performs in-memory processing, which significantly improves the processing speed compared to the traditional MapReduce model that relies heavily on disk-based storage\n",
    "    - ease of use: provides high-level APIs in Java, Scala, Python, and R, making it accessible to a broad audience\n",
    "    - versatility: supports a wide range of data processing tasks, including batch processing, interactive queries, streaming analytics, and machine learning\n",
    "    - in-memory processing: utilizes resilient distributed datasets (RDDs), an immutable distributed collection of objects, to store data in-memory across a cluster\n",
    "    - fault tolerance: provides fault tolerance through lineage information stored in RDDs\n",
    "    - data processing libraries: comes with built-in libraries for various data processing tasks, such as Spark SQL for structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time data processing\n",
    "    - ease of integration: can be easily integrated with other popular big data technologies, such as Apache Hadoop, Apache Hive, Apache HBase, and more\n",
    "    - lazy evaluation: uses lazy evaluation, meaning that transformations on RDDs are not executed immediately\n",
    "    - community support: has a large and active open-source community, which contributes to its development and provides support through forums, mailing lists, and documentation\n",
    "    - cluster manager integration: can run on various cluster managers, including Apache Mesos, Apache Hadoop YARN, and its standalone built-in cluster manager\n",
    "- covers variety of workloads that earlier required separate distributed systems like:\n",
    "    - batch processing: Hadoop MapReduce\n",
    "    - interactive queries: Hive\n",
    "    - streaming analytics: Storm\n",
    "    - machine learning: Mahout\n",
    "    - graph processing: Giraph\n",
    "- can be used from Python, Scala, Java, and R\n",
    "\n",
    "## RDDs (Resilient Distributed Datasets)\n",
    "- fundamental data structure in Spark, an immutable distributed collection of objects that can be operated on in parallel\n",
    "- can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs\n",
    "    - created by transforming data in stable storage like HDFS or S3 using transformations like `map`, `filter`, `groupByKey`, `join`, etc\n",
    "- can be cached in memory across machines, can be recomputed if lost due to node failure\n",
    "- transformations are lazy, only executed when an action is called, evaluated in parallel, fault-tolerant, can be recomputed if lost due to node failure\n",
    "- operations:\n",
    "    - transformations: create a new RDD from an existing one, eg. `map`, `filter`, `reduceByKey`, `join`\n",
    "        - optimized by Spark through lazy evaluation and pipelining\n",
    "        - recover data from failures by recomputing from the original data\n",
    "    - actions: return a value to the driver program after running a computation on the dataset, eg. `collect`, `count`, `reduce`, `saveAsTextFile`\n",
    "        - trigger the execution of the DAG of transformations\n",
    "        - materialize the result of the computation\n",
    "- tranformations:\n",
    "    1. `map(func)`: returns a new RDD by applying a function to each element of the RDD\n",
    "    2. `filter(func)`: returns a new RDD by selecting elements that satisfy a predicate\n",
    "    3. `flatMap(func)`: similar to `map`, but each input item can be mapped to 0 or more output items, useful for tokenizing\n",
    "    4. `sample(withReplacement, fraction, seed)`: returns a sampled subset of the RDD\n",
    "    5. `union(otherDataset)`: returns the union of the RDD with another one\n",
    "    6. `intersection(otherDataset)`: returns the intersection of the RDD with another one\n",
    "    7. `distinct(numPartitions)`: returns a new RDD with distinct elements\n",
    "    8. `groupByKey(numPartitions)`: returns a new RDD of `(K, Iterable[V])` pairs\n",
    "    9. `reduceByKey(func, [numPartitions])`: when called on a dataset of `(K, V)` pairs, returns a dataset of `(K, V)` pairs where the values for each key are aggregated using the given reduce function\n",
    "    10. `sortByKey(ascending, [numPartitions])`: returns an RDD sorted by key\n",
    "    11. `join(otherDataset, [numPartitions])`: returns an RDD containing all pairs of elements with matching keys in self and other\n",
    "    12. `cogroup(otherDataset, [numPartitions])`: returns an RDD of `(K, (Iterable[V], Iterable[W]))` pairs\n",
    "    13. `cartesian(otherDataset)`: returns the Cartesian product of the two datasets\n",
    "- actions:\n",
    "    1. `reduce(func)`: aggregate the elements of the dataset using a function `func` (which takes two arguments and returns one)\n",
    "    2. `collect()`: return all elements of the dataset as an array at the driver program\n",
    "    3. `count()`: return the number of elements in the dataset\n",
    "    4. `first()`: return the first element of the dataset\n",
    "    5. `take(n)`: return an array with the first `n` elements of the dataset\n",
    "    6. `takeSample(withReplacement, num, [seed])`: return an array with a random sample of `num` elements of the dataset\n",
    "    7. `saveAsTextFile(path)`: write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS, or any other Hadoop-supported file system\n",
    "    8. `countByKey()`: only available on RDDs of type `(K, V)`, returns a hashmap of `(K, Int)` pairs with the count of each key\n",
    "    9. `foreach(func)`: run a function `func` on each element of the dataset\n",
    "\n",
    "Dataframes : \n",
    "- provides a more structured and optimized way to work with data compared to RDDs (named columns, schema, etc) - fastest option\n",
    "\n",
    "Datasets :\n",
    "- provides the type safety of RDDs with the optimization benefits of DataFrames - best of both worlds\n",
    "- can be used with Java, Scala only\n",
    "\n",
    "<img alt=\"picture 0\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/2d656b48ab268f1f9e135d2624301114f7759880295fb8c32b3a82ccf0b93fb5.png\" width=\"1000\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "```scala\n",
    "// Create two DataFrames\n",
    "val df1 = spark.createDataFrame(Seq(\n",
    "  (1, \"Alice\"),\n",
    "  (2, \"Bob\"),\n",
    "  (3, \"Charlie\")\n",
    ")).toDF(\"id\", \"name\")\n",
    "\n",
    "val df2 = spark.createDataFrame(Seq(\n",
    "  (1, \"Engineering\"),\n",
    "  (2, \"Finance\"),\n",
    "  (3, \"Marketing\")\n",
    ")).toDF(\"id\", \"department\")\n",
    "\n",
    "// Perform inner join\n",
    "val joinedDF = df1.join(df2, Seq(\"id\"), \"inner\")\n",
    "// can also be \"outer\", \"left\", \"right\", \"left_outer\", \"right_outer\", \"left_semi\", \"left_anti\"\n",
    "```\n",
    "\n",
    "Spark stack:\n",
    "<img alt=\"picture 1\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/767e579dc6f96f27cfda5046e3825d9c4db14cc77127f5416898850e98f9fd6c.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "1. Spark Core : basic functionality and API of Spark, including RDDs and transformations, and components for task scheduling, memory management, fault recovery, etc\n",
    "2. Spark SQL : provides support for structured and semi-structured data, including DataFrames, Datasets, and SQL queries\n",
    "    - allows using Hive variant of SQL called HiveQL, intermixing SQL queries with Spark programs in Java, Scala, Python, and R\n",
    "3. MLlib : provides machine learning algorithms and utilities for classification, regression, clustering, collaborative filtering, etc\n",
    "    - lower level primitives for ML algorithms like gradient descent optimization algorithms, linear algebra, etc\n",
    "4. Spark Streaming : enables scalable, high-throughput, fault-tolerant stream processing of live data streams\n",
    "    - can ingest data from various sources like Kafka, Flume, Kinesis, etc\n",
    "    - can process data using complex algorithms expressed with high-level functions like map, reduce, join, window, etc and common graph processing algorithms like PageRank, triangle counting, etc\n",
    "4. GraphX : provides an API for graphs and graph-parallel computation, including graph algorithms like PageRank, connected components, etc\n",
    "\n",
    "Transformations can be broadly classified into two categories:\n",
    "1. Narrow transformations: each input partition contributes to at most one output partition, eg. `map`, `filter`, `flatMap`, `mapPartitions`, `mapPartitionsWithIndex`, `sample`, `union`, `intersection`, `distinct`, `groupByKey`, `reduceByKey`, `join`, `cogroup`, `cartesian`\n",
    "2. Wide transformations: each input partition contributes to multiple output partitions, eg. `groupByKey`, `reduceByKey`, `join`, `cogroup`, `cartesian`, `sortByKey`\n",
    "\n",
    "Spark SQL example\n",
    "```scala\n",
    "val res = spark.sql(\"create database temp\")\n",
    "val res = spark.sql(\"show databases\").show()\n",
    "spark.sql(\"CREATE TABLE employee(id INT, name STRING, age INT) ROW FORMAT DELIMITED FIELDS \n",
    "TERMINATED BY ',' LINES TERMINATED BY '\\n'\")\n",
    "val res = spark.sql(\"show tables\")\n",
    "res.show()\n",
    "val res = spark.sql(\"desc employee\")\n",
    "res.show()\n",
    "spark.sql(\"LOAD DATA INPATH '/employee.csv' INTO TABLE employee\")\n",
    "val res = spark.sql(\"FROM employee SELECT id, name, age where age > 40\")\n",
    "res.show()\n",
    "\n",
    "val DF = spark.read.option(\"header\",true).csv(\"bank.csv\") // read CSV file into a DF\n",
    "DF.show() // Examine the DF\n",
    "DF.createOrReplaceTempView(\"BANK\") // Create a Table named BANK from DF\n",
    "spark.sql(\"desc BANK\").show() // Display schema\n",
    "spark.sql(\"SELECT age, job, balance FROM BANK\").show(5) // SQL select qury\n",
    "spark.sql(\"SELECT age, job, balance FROM BANK\").where(\"job == ‘admin.’\").show(10)\n",
    "// SORT by age \n",
    "spark.sql(\"\"\" SELECT age, job, balance FROM BANK WHERE job in ('admin.','services') order by age\"\"\").show(10)\n",
    "// SQL GROUP BY clause\n",
    "spark.sql(\"\"\" SELECT job, count(*) as count FROM BANK GROUP BY job\"\"\").show()\n",
    "//SQL JOIN\n",
    "val joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\")\n",
    "```\n",
    "\n",
    "### Spark Runtime architecture\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/cbc682ec248518f3f44d6b994283423f96d69668243c17ffcb2235b926593930.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "1. Driver: manages the execution of the Spark application, including creating the SparkContext, distributing the code to the executors, and collecting the results\n",
    "    It contains the :\n",
    "    - DAGScheduler: computes a DAG of stages for each job and submits them to the TaskScheduler\n",
    "        - also breaks the DAG into stages of tasks for submission to the TaskScheduler\n",
    "    - TaskScheduler: responsible for scheduling tasks on the cluster, managing task execution, and handling failures\n",
    "    - RDD graph compiler: optimizes the logical execution plan generated by the DAGScheduler\n",
    "    - SchedulerBackend: interface to the cluster manager, responsible for launching executors and scheduling tasks\n",
    "    - BlockManager: manages data storage for RDDs, caching, and shuffle data\n",
    "2. SparkContext: represents the connection to a Spark cluster, used to create RDDs, broadcast variables, and accumulators\n",
    "3. Executors: run the tasks and store the data for the application, managed by the driver\n",
    "    - provides in-memory storage for RDDs that are cached by user programs\n",
    "4. Tasks: individual units of work that are sent to the executors for execution\n",
    "5. Cluster Manager: manages the resources of the cluster and allocates them to the Spark application\n",
    "    - can be standalone, Mesos, YARN, Kubernetes, etc\n",
    "\n",
    "`spark-submit` command:\n",
    "```bash\n",
    "spark-submit --class <main-class> --master <master-url> --deploy-mode <deploy-mode> --conf <key=value> <application-jar> [application-arguments]\n",
    "```\n",
    "- it submits a Spark application to the cluster for execution with the specified configuration\n",
    "- `--class`: main class of the application (eg. `org.apache.spark.examples.SparkPi`)\n",
    "- `--master`: URL of the cluster manager (eg. `spark://host:port`, `mesos://host:port`, `yarn`, `local`)\n",
    "- `--deploy-mode`: deployment mode for the application (eg. `client`, `cluster`)\n",
    "- `--conf`: configuration properties for the application (eg. `spark.executor.memory=4g`)\n",
    "- `application-jar`: JAR file containing the application code and dependencies (eg. `myApp.jar`)\n",
    "- `application-arguments`: arguments passed to the application (eg. `arg1 arg2`)\n",
    "\n",
    "Shuffle operations:\n",
    "- operations that require data to be shuffled across the network, such as `groupByKey`, `reduceByKey`, `join`, `sortByKey`, etc\n",
    "- can be expensive in terms of network and disk I/O, and can impact the performance of the application\n",
    "- Spark optimizes shuffle operations through various mechanisms like pipelining, combiners, partitioning, and spill to disk\n",
    "\n",
    "Broadcast variables:\n",
    "- used to efficiently distribute large read-only data to all tasks in a Spark application\n",
    "- broadcast variables are cached in serialized form and distributed to all executors\n",
    "- can be used to reduce the cost of shipping large objects to tasks, especially when they are used multiple times across multiple tasks\n",
    "- can be created using the `broadcast` method on the SparkContext\n",
    "\n",
    "Accumulators:\n",
    "- used to aggregate values from tasks and return the aggregated result to the driver\n",
    "- accumulators are write-only variables that can be updated by tasks and read by the driver\n",
    "- can be used for tasks like counting the number of occurrences of an event, summing values, etc\n",
    "- can be created using the `accumulator` method on the SparkContext\n",
    "\n",
    "RDD Persistence:\n",
    "- RDDs can be persisted in memory or disk to avoid recomputation of the same data\n",
    "- persistence levels: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `OFF_HEAP`\n",
    "    - `MEMORY_ONLY`: store RDD partitions in memory\n",
    "    - `MEMORY_AND_DISK`: store RDD partitions in memory, spill to disk if memory is full\n",
    "    - `MEMORY_ONLY_SER`: store RDD partitions in memory as serialized Java objects\n",
    "    - `MEMORY_AND_DISK_SER`: store RDD partitions in memory as serialized Java objects, spill to disk if memory is full\n",
    "    - `DISK_ONLY`: store RDD partitions on disk\n",
    "    - `OFF_HEAP`: store RDD partitions off-heap in serialized form\n",
    "- can be persisted using the `persist` method on the RDD\n",
    "- Spark automatically unpersists RDDs that are no longer used to free up memory using LRU eviction policy\n",
    "    - can manually unpersist RDDs using the `unpersist` method\n",
    "\n",
    "### MLlib\n",
    "- scalable machine learning library built on top of Spark that provides a wide range of machine learning algorithms and utilities\n",
    "- key features:\n",
    "    - distributed algorithms: provides distributed implementations of popular machine learning algorithms\n",
    "    - high-level APIs: supports high-level APIs for building machine learning pipelines\n",
    "    - integration with Spark SQL: integrates with Spark SQL for data preprocessing and feature engineering\n",
    "    - model persistence: supports model persistence for saving and loading machine learning models\n",
    "    - hyperparameter tuning: provides utilities for hyperparameter tuning and model selection\n",
    "- tasks supported by MLlib:\n",
    "    - feature transformations: supports feature transformations like vectorization, normalization, and encoding\n",
    "    - classification: provides algorithms for classification tasks like logistic regression, decision trees, random forests, etc\n",
    "    - regression: provides algorithms for regression tasks like linear regression, decision trees, random forests, survival regression, etc\n",
    "    - clustering: provides algorithms for clustering tasks like k-means, Gaussian mixture models, frequent itemsets, association rules, sequential pattern mining\n",
    "    - collaborative filtering: provides algorithms for collaborative filtering tasks like alternating least squares, matrix factorization, etc\n",
    "    - dimensionality reduction: provides algorithms for dimensionality reduction tasks like singular value decomposition, principal component analysis, etc\n",
    "    - evaluation metrics: provides utilities for evaluating machine learning models using metrics like accuracy, precision, recall, F1 score, etc\n",
    "- primary API is the dataframe-based API in `spark.ml` package, which provides higher-level APIs built on top of DataFrames\n",
    "\n",
    "Recommendation system:\n",
    "1. Collaborative filtering: based on the idea that users who have agreed in the past will agree in the future\n",
    "    - user-based collaborative filtering: recommend items by finding similar users based on their ratings\n",
    "    - item-based collaborative filtering: recommend items by finding similar items based on user ratings\n",
    "2. Matrix factorization: factorize the user-item interaction matrix into two lower-dimensional matrices\n",
    "    - alternating least squares (ALS): popular algorithm for matrix factorization in collaborative filtering\n",
    "    - ALS is implemented in MLlib as `ALS` algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "- extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams\n",
    "- key features:\n",
    "    - micro-batch processing: processes data in small, user-defined batches, enabling low-latency processing\n",
    "    - fault tolerance: provides fault tolerance through checkpointing and write-ahead logs\n",
    "    - window operations: supports windowed computations over data streams\n",
    "    - integration with Spark Core: integrates seamlessly with Spark Core, allowing users to combine batch and streaming processing\n",
    "    - support for various data sources: supports various data sources like Kafka, Flume, Kinesis, etc\n",
    "    - high-level APIs: provides high-level APIs for stream processing, including transformations and output operations\n",
    "    - exactly-once semantics: supports exactly-once semantics for end-to-end fault-tolerant stream processing\n",
    "    - integration with MLlib: integrates with MLlib for real-time machine learning and analytics\n",
    "\n",
    "Continuous Operator Model:\n",
    "- traditional model, there are a set of worker nodes, each of which run one or more continuous operators\n",
    "- each continuous operator processes the streaming data one record at a time and forwards the records to other operators in the pipeline\n",
    "- there are \"source\" operators for receiving data from ingestion systems, and \"sink\" operators that output to downstream systems\n",
    "\n",
    "Spark is designed to:\n",
    "- fast failure and straggler recovery: recover quickly from failures and stragglers to provide results in real time\n",
    "- load balancing: dynamically adapt the resource allocation based on the workload\n",
    "- unification of batch, streaming, and interactive workloads: combine batch, streaming, and interactive queries\n",
    "- advanced analytics like machine learning and SQL queries: support complex workloads like continuously learning and updating data models, or querying the \"latest\" view of streaming data with SQL queries\n",
    "- to solve these challenges, Spark Streaming uses a new architecture called Discretized Streams (DStreams)\n",
    "\n",
    "Discretized Streams (DStreams):\n",
    "- basic abstraction provided by Spark Streaming, represents a continuous stream of data\n",
    "    - allows for the processing of live data streams in a fault-tolerant manner, recovering fast from failures and dynamic scheduling of batches leading to better load balancing\n",
    "- recievers accept data in parallel and buffer it in the memory of Spark's worker nodes, then the latency-optimized Spark engine runs short tasks to process the batches and output the results to other systems\n",
    "- unlike the traditional continuous operator model, where the computation is statically allocated to a node, Spark tasks are assigned dynamically to the workers based on the locality of the data and available resources\n",
    "- internally, a DStream is represented as a sequence of RDDs\n",
    "- can be created from various input sources like Kafka, Flume, Kinesis, etc\n",
    "- supports various transformations like `map`, `filter`, `reduceByKey`, `join`, `window`, etc\n",
    "\n",
    "Basic Streaming Transformations:\n",
    "- Stateless transformations: (these transformations do not depend on previous data)\n",
    "    - `map`: transforms one record to one record\n",
    "    - `filter`: filters out records that don't satisfy a predicate\n",
    "    - `flatMap`: transforms one record to zero or more records\n",
    "- Stateful transformations: (these transformations depend on previous data)\n",
    "    - `aggregation`: combines all records to produce a single value\n",
    "    - `group-and-aggregate`: extracts a grouping key from the record and computes a separate aggregated value for each key\n",
    "    - `join`: joins same-keyed records from several streams\n",
    "    - `sort`: sorts the records observed in the stream\n",
    "\n",
    "Transformations on DStreams:\n",
    "- `map(func)`: return a new DStream by passing each element of the source DStream through a function `func`\n",
    "- `flatMap(func)`: similar to `map`, but each input item can be mapped to 0 or more output items\n",
    "- `filter(func)`: return a new DStream by selecting only the records of the source DStream on which `func` returns true\n",
    "- `repartition(numPartitions)`: changes the level of parallelism in this DStream by creating more or fewer partitions\n",
    "- `union(otherStream)`: return a new DStream that contains the union of the elements in the source DStream and otherDStream\n",
    "- `count()`: return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream\n",
    "- `reduce(func)`: return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function `func` (which takes two arguments and returns one)\n",
    "- `countByValue()`: when called on a DStream of elements of type K, return a new DStream of `(K, Long)` pairs where the value of each key is its frequency in each RDD of the source DStream\n",
    "- `reduceByKey(func, [numTasks])`: when called on a DStream of `(K, V)` pairs, return a new DStream of `(K, V)` pairs where the values for each key are aggregated using the given reduce function\n",
    "- `join(otherStream, [numTasks])`: when called on two DStreams of `(K, V)` and `(K, W)` pairs, return a new DStream of `(K, (V, W))` pairs with all pairs of elements for each key\n",
    "- `cogroup(otherStream, [numTasks])`: when called on a DStream of `(K, V)` and `(K, W)` pairs, return a new DStream of `(K, Seq[V], Seq[W])` tuples\n",
    "- `transform(func)`: return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream\n",
    "- `updateStateByKey(func)`: return a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key\n",
    "\n",
    "Window operations:\n",
    "- apply transformations over a sliding window of data, the window slides over a source DSream, and the source RDD's that fall within the window are combined and operated upon\n",
    "- must specify the window length and sliding interval (eg. window length of 3 RDDs and sliding interval of 2 RDDs means the window slides every 2 RDDs and contains the last 3 RDDs)\n",
    "- functions:\n",
    "    - `window(windowLength, slideInterval)`: return a new DStream which is computed based on windowed batches of the source DStream\n",
    "    - `countByWindow(windowLength, slideInterval)`: return a sliding window count of elements in the stream\n",
    "    - `reduceByWindow(func, windowLength, slideInterval)`: return a new single-element stream, created by aggregating elements in the stream over a sliding interval using `func`\n",
    "    - `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])`: when called on a DStream of `(K, V)` pairs, return a new DStream of `(K, V)` pairs where the values for each key are aggregated using the given reduce function\n",
    "    - `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`: a more efficient version of the above `reduceByKeyAndWindow()` where the reduce value of each window is calculated incrementally using the reduce values of the previous window\n",
    "    - `countByValueAndWindow(windowLength, slideInterval, [numTasks])`: when called on a DStream of `(K, V)` pairs, return a new DStream of `(K, Long)` pairs where the value of each key is its frequency within a sliding window\n",
    "    \n",
    "Structured Streaming:\n",
    "- treats a live data stream as a table that is being continuously appended, \n",
    "- express streaming computation as standard batch-like query as on a static table, each query runs as an incremental query on the unbounded input table\n",
    "- does not materialize the entire table, reads the latest available data from the streaming data source, processes data incrementally to update the result, and discards the source data after processing\n",
    "- Spark is responsible for updating the Result Table when there is new data\n",
    "- output modes:\n",
    "    - `append`: only the new rows in the streaming DataFrame/Dataset will be written to the sink\n",
    "    - `complete`: all the rows in the streaming DataFrame/Dataset will be written to the sink every time there are some updates\n",
    "    - `update`: only the rows that were updated in the streaming DataFrame/Dataset will be written to the sink every time there are some updates\n",
    "\n",
    "| Info | Spark | Kafka |\n",
    "| --- | --- | --- |\n",
    "| Description | General purpose distributed processing system used for big data workloads. It uses in-memory caching and optimized query execution to provide fast analytic queries against data of any size. | Distributed Streaming platform that allows developers to create applications that continuously produce and consume data streams. |\n",
    "| Processing Model | Batch processing and streaming | Event Streaming |\n",
    "| Throughput & Latency | High throughput, low latency | High throughput, low latency |\n",
    "| Scalability | Horizontally scalable as a distributed system, though scaling is expensive due to RAM requirement | Horizontally scalable to support a growing number of users and use cases, meaning that it can handle an increasing amount of data by adding more nodes & partitions to the system |\n",
    "| Fault Tolerance | Fault tolerant by nature (RDDs). In case of node failure, RDDs are automatically recreated. | Fault-tolerant as it replicates data across multiple servers and can automatically recover from node failures |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "- 4 types:\n",
    "    1. Tumbling window: non-overlapping, fixed-size windows\n",
    "    2. Sliding window: overlapping, fixed-size windows\n",
    "        - window slides by a fixed interval reevaluted only when the contents of the window change, every time a new record arrives\n",
    "        - eg. calculate the sum of the last 5 elements in a stream every 2 elements (or moving average of 5 elements)\n",
    "    3. Session window: windows based on session boundaries\n",
    "        - window is re-evaluated based on the arrival of new data and the session boundaries, every time there is activity in the session, the boundary is extended\n",
    "        - eg. calculate the total time spent by a user on a website in a session\n",
    "    4. Hopping window: fixed-size windows that hop by a fixed interval\n",
    "        - window is re-evaluated on fixed time intervals irrespective of the data stream arrival\n",
    "        - can be used to calculate metrics like hourly, daily, weekly, etc\n",
    "\n",
    "<img alt=\"picture 3\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/2426dd54973a39d86e1c0df1f2fad404da016265f75306b0eed0547003ea3b18.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Exam answers\n",
    "\n",
    "HBase vs Cassandra:\n",
    "- Hadoop vs HBase - limitation that HBase addresses is its lack of real-time, random read and write access to data. Hadoop's primary storage system, HDFS, is optimized for batch processing, which makes it inefficient for real-time queries and updates.\n",
    "- Updates and deletions in HBases are implemented through a concept called \"Write-Ahead Logging (WAL).\" When a write operation occurs, the new data is first written to a WAL file. Then, the data is written to a MemStore, an in-memory data structure. Periodically, MemStore data is flushed to disk as an HFile, which is an immutable, sorted file. Deletions are implemented by writing a special marker called a \"delete marker\" to the WAL and MemStore. During compaction, HBase removes these delete markers and any corresponding data from the HFiles, effectively deleting the row.\n",
    "- Updates and deletions in Cassandra are implemented through a process called \"tombstone insertion.\" When a row is updated or deleted, a special marker called a \"tombstone\" is written for that particular row key. Tombstones are used during compaction to indicate that a particular row is deleted. During compaction, Cassandra merges SSTables and removes any rows marked with tombstones, effectively deleting the row.\n",
    "\n",
    "Flume config file to ingest data from a web server log file and write it to HDFS in real-time, only when 1000 or more entries are made to the webserver log\n",
    "```bash\n",
    "# Define the agent name, source, channel, and sink\n",
    "agent.sources = source1\n",
    "agent.channels = channel1\n",
    "agent.sinks = sink1\n",
    "\n",
    "# Configure the source\n",
    "agent.sources.source1.type = exec\n",
    "agent.sources.source1.command = tail -F /path/to/webserver/log/file\n",
    "\n",
    "# Configure the channel\n",
    "agent.channels.channel1.type = memory\n",
    "agent.channels.channel1.capacity = 10000\n",
    "agent.channels.channel1.transactionCapacity = 1000\n",
    "\n",
    "# Configure the sink\n",
    "agent.sinks.sink1.type = hdfs\n",
    "agent.sinks.sink1.hdfs.path = hdfs://<namenode>:<port>/path/to/hdfs/folder\n",
    "agent.sinks.sink1.hdfs.filePrefix = logs-\n",
    "agent.sinks.sink1.hdfs.fileSuffix = .log\n",
    "agent.sinks.sink1.hdfs.rollInterval = 0\n",
    "agent.sinks.sink1.hdfs.rollSize = 0\n",
    "agent.sinks.sink1.hdfs.rollCount = 1000\n",
    "agent.sinks.sink1.hdfs.batchSize = 1000\n",
    "agent.sinks.sink1.hdfs.txnEventMax = 1000\n",
    "\n",
    "# Bind the source and sink to the channel\n",
    "agent.sources.source1.channels = channel1\n",
    "agent.sinks.sink1.channel = channel1\n",
    "```\n",
    "\n",
    "Carefully go through scala Code1 and scala Code2 give below and answer the questions given below them.  \n",
    "```scala\n",
    "val lines = new Array[String](2) \n",
    "lines(0) = \"Hello world\" \n",
    "lines(1) = \"How are you world\" \n",
    "val stringRDD=sc.parallelize(lines,1) \n",
    "val wordsRDD = stringRDD.flatMap(x => x.split(\" \")) \n",
    "val wordRDD = wordsRDD.collect()\n",
    "```\n",
    "```scala\n",
    "val lines = new Array[String](2) \n",
    "lines(0) = \"Hello world\" \n",
    "lines(1) = \"How are you world\" \n",
    "val stringRDD=sc.parallelize(lines,1) \n",
    "val wordsRDD = stringRDD.flatMap(x => x.split(\" \")) \n",
    "val filtRDD = wordsRDD.filter(x => x.startsWith(\"H\")) \n",
    "val wordRDD = wordsRDD.collect() \n",
    "```\n",
    "\n",
    "1. Will there be any difference in the execution time for Code1 and Code2 ? \n",
    "- Additional filter operation in Code2, so Code2 will take more time to execute compared to Code1, but the difference may be negligible for small datasets\n",
    "2. Modify Code1 to run as 2 tasks in parallel \n",
    "- `sc.parallelize(lines, 2)` to run the RDD creation in parallel\n",
    "3. Modify the Code2 to output only words having a length of 5 \n",
    "- `val filtRDD = wordsRDD.filter(x => x.length == 5)` to filter words with a length of 5\n",
    "4. Modify Code2 to output the number of unique words in the list.\n",
    "- `val uniqueWords = wordsRDD.distinct().count()` to get the count of unique words\n",
    "\n",
    "Concepts relating to Cassandra:\n",
    "- primary key design: crucial for data distribution and retrieval efficiency\n",
    "- composite primary key: allows for efficient distribution and retrieval of data based on multiple criteria\n",
    "- partition key: determines how data is distributed across nodes in the cluster\n",
    "- clustering key: determines the physical sorting order of data within a partition\n",
    "- data locality: storing related data together based on the primary key can improve data locality and reduce the need to query multiple nodes for related data\n",
    "- query optimization: understanding query patterns and access patterns is essential for designing an effective primary key structure that minimizes the need for data scans and improves query performance\n",
    "- trade-offs: balancing data distribution, data locality, and query performance based on the specific requirements and access patterns of the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Diagrams\n",
    "\n",
    "### Hadoop\n",
    "\n",
    "<img alt=\"picture 12\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/c8d13a26353d5a6c9d27fcdf1a9248edadf083645aeeee53330acd17c52f7135.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### HDFS\n",
    "\n",
    "<img alt=\"picture 11\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/96d8065c92beac6ec11fc327ce744524d8e2a15895954e5006786160366bbac3.png\" width=\"500\" />\n",
    "<img alt=\"picture 13\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/324c076408e8129db78d87ec8ceaaffb695494ef780439e5dec42a0b6995316b.png\" width=\"500\" />  \n",
    "\n",
    "### YARN\n",
    "\n",
    "<img alt=\"picture 14\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/24853241be11497c90cf30b4885c9bb76b7244ad8f816e2df2c8e8d392c5dea7.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "YARN scheduling policies:\n",
    "- FIFO: first-in, first-out scheduling policy, where jobs are executed in the order they are submitted\n",
    "- Capacity: allows for multiple queues, each with a guaranteed capacity, and jobs are scheduled based on the available capacity in each queue\n",
    "- Fair: dynamically allocates resources to jobs based on the demand, ensuring that all jobs get a fair share of the cluster resources\n",
    "\n",
    "### HBase\n",
    "\n",
    "<img alt=\"picture 16\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/c2c102a530317488c2b5d49fb2fd6a7ff8865ff19464c03f77b7b34b84815527.png\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Pig\n",
    "\n",
    "<img alt=\"picture 17\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/d825829d1afd71e83ce17514b47474db1305923375501199f32e8c4360940106.png\" width=\"300\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Hive\n",
    "\n",
    "<img alt=\"picture 18\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/ceddd053d34825eecef17433e30fd6546088091eff24329b6126ede9e8a21aeb.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Sqoop\n",
    "\n",
    "<img alt=\"picture 19\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/b3a13b09e3cf3b2ab71fa1d27acff4dd19b87a49316d3ce1fe3a88e34b3fefbe.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Flume\n",
    "\n",
    "<img alt=\"picture 20\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/07307b3ab41d9a193d341c27262bbd49102c63c57bed72742ca6f699f9d19a0a.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Oozie\n",
    "\n",
    "<img alt=\"picture 21\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8a74d673a3e0a5f65dfe63f8d45c4bfe5e8fc1318a8b37b2005d548d4036f880.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### MongoDB\n",
    "<img alt=\"picture 22\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/becbf7882812def4c5bfea4359575b7ed611b7a4bc813ff4eaa3d477427f9586.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Cassandra\n",
    "\n",
    "<img alt=\"picture 23\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/ca810848243d3c4b95177562ff27a7763f7f16c08bff770b8ccee58c8a33a10e.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">  \n",
    "\n",
    "### Spark\n",
    "<img alt=\"picture 24\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/165f411f2ed28f5b894819b57bc4f58dc4aebf98a14e9194a18cb50f9d98d8b4.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### Kafka\n",
    "\n",
    "<img alt=\"picture 25\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/b28bd84242c4fb13329a1bf75fe5f1a649a649bf6cdee0dcca0108df27406f07.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
