{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1\n",
    "\n",
    "### What is Machine Learning?\n",
    "\n",
    "Program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\n",
    "To have a learning problem, we must identify:\n",
    "- class of tasks T\n",
    "- performance measure P\n",
    "- source of experience E\n",
    "\n",
    "Traditional programming vs Machine Learning\n",
    "- Traditional programming: (data) + (program) = (output)\n",
    "- Machine Learning: (data) + (output) = (program)\n",
    "\n",
    "#### Table with learning tasks, performance measures and experience sources\n",
    "\n",
    "| Task | Performance Measure | Experience Source |\n",
    "| --- | --- | --- |\n",
    "| Email spam filter | Accuracy of the filter | User marks emails as spam/not spam |\n",
    "| Handwritten digit recognition | Accuracy of the classifier | User provides examples of digits |\n",
    "| Self-driving car | Safety and efficiency of the car | User drives the car |\n",
    "| Playing checkers | % of games won against opponent | Games played against itself |\n",
    "\n",
    "#### When do we use / not use Machine Learning?\n",
    "Used when:\n",
    "- lots of hand-tuning, long lists of rules, or hard to define rules\n",
    "- complex / fluctuating environment\n",
    "- expert knowledge does not exist, or is difficult to obtain\n",
    "- models based on huge amount of data, must be customized to each individual\n",
    "\n",
    "Not used when:\n",
    "- simple, static environment, well-defined rules\n",
    "- no uncertainty in the environment\n",
    "- expert knowledge is available\n",
    "\n",
    "### Machine Learning Process\n",
    "\n",
    "| Step | Description |\n",
    "| --- | --- |\n",
    "| 1. Define the Problem     | Clearly define the problem statement, including the goal and the target variable(s).<br> Identify the available resources, constraints, and relevant stakeholders.<br> Understand the domain knowledge and business context to ensure the problem's relevance. |\n",
    "| 2. Data Collection        | Determine the data requirements based on the problem definition.<br> Identify potential data sources and acquire the necessary datasets.<br> Ensure data quality by performing data validation, cleaning, and handling missing values or outliers. |\n",
    "| 3. Data Exploration       | Perform statistical analysis, such as summary statistics and data distributions.<br> Visualize the data through plots, histograms, scatterplots, or heatmaps.<br> Identify correlations, patterns, and outliers within the dataset.<br> Conduct feature correlation analysis to understand relationships between variables. |\n",
    "| 4. Feature Engineering    | Select relevant features based on domain knowledge and exploration.<br> Handle categorical variables through techniques like one-hot encoding or ordinal encoding.<br> Scale numerical features to a common range or apply normalization techniques.<br> Create new features by transforming or combining existing ones (e.g., feature interactions, polynomial features). |\n",
    "| 5. Model Selection        | Identify the problem type (classification, regression, clustering, etc.).<br> Consider the characteristics of the dataset (e.g., size, dimensionality) and the assumptions of different algorithms.<br> Evaluate various algorithms and choose the one that best suits the problem and data. |\n",
    "| 6. Model Training         | Split the data into training and testing sets (e.g., using random sampling or time-based splitting).<br> Apply the chosen algorithm to the training data and optimize its hyperparameters.<br> Evaluate the model's performance on the testing set using appropriate metrics.<br> Repeat the training process with different algorithms or parameter settings if necessary. |\n",
    "| 7. Model Evaluation       | Calculate evaluation metrics such as accuracy, precision, recall, F1 score, or mean squared error.<br> Perform cross-validation or holdout validation to estimate the model's performance on unseen data.<br> Analyze the model's strengths, weaknesses, and potential biases.<br> Consider business requirements and domain-specific metrics for a comprehensive evaluation. |\n",
    "| 8. Model Optimization     | Fine-tune the model's hyperparameters through techniques like grid search, random search, or Bayesian optimization.<br> Regularize the model to prevent overfitting using techniques like L1/L2 regularization or dropout.<br> Explore ensemble methods, such as bagging or boosting, to improve model performance.<br> Use feature selection techniques to remove irrelevant or redundant features. |\n",
    "| 9. Model Deployment       | Prepare the model for deployment by saving its trained parameters and associated preprocessing steps.<br> Integrate the model into an application, system, or cloud infrastructure.<br> Design and implement an API for making predictions using the deployed model.<br> Ensure the model's scalability, robustness, and security in a production environment. |\n",
    "| 10. Monitoring and Maintenance | Continuously monitor the model's performance in real-world scenarios.<br> Collect feedback and track performance metrics to detect any degradation or concept drift.<br> Retrain the model periodically with new data to keep it up-to-date and maintain its accuracy.<br> Conduct regular model audits and updates as needed. |\n",
    "| 11. Iteration and Improvement | Regularly revisit and refine the model as new insights are gained, data quality improves, or new techniques emerge.<br> Incorporate feedback from stakeholders and address any limitations or shortcomings.<br> Continuously experiment with new algorithms or approaches to improve the model's performance and adapt to evolving requirements. |\n",
    "\n",
    "#### Types of learning:\n",
    "- Supervised (inductive) learning\n",
    "    - given training data, desired outputs (labels)\n",
    "    - learn a function that maps inputs to outputs\n",
    "    - types:\n",
    "        - classification (predict class or category, discrete value)\n",
    "            - binary classification (2 classes)\n",
    "            - multi-class classification (more than 2 classes)\n",
    "        - regression (predict continuous value)\n",
    "- Unsupervised (deductive) learning\n",
    "    - given training data, no desired outputs\n",
    "    - learn a function that describes hidden structure from unlabeled data\n",
    "- Semi-supervised learning\n",
    "    - given training data, some desired outputs\n",
    "    - learn a function that maps inputs to outputs\n",
    "- Reinforcement learning\n",
    "    - rewards from sequence of actions\n",
    "    - learn a function that maximizes a reward signal\n",
    "\n",
    "High level, general comparison table:\n",
    "\n",
    "|                       | Supervised Learning               | Unsupervised Learning          | Semi-Supervised Learning             | Reinforcement Learning                    |\n",
    "|-----------------------|-----------------------------------|--------------------------------|--------------------------------------|------------------------------------------|\n",
    "| Data                  | Labelled                          | Unlabelled                     | Mix of Labelled and Unlabelled       | Depends on State and Reward              |\n",
    "| Task                  | Prediction                        | Pattern Recognition            | Prediction                           | Sequential Decision Making               |\n",
    "| Example Algorithms    | Linear Regression, SVM, Neural Networks | Clustering, K-Means, PCA | Self-Training, Multi-View Training   | Q-Learning, SARSA, DQN                    |\n",
    "| Feedback              | Direct                            | None                           | Partial                              | Reward-based                              |\n",
    "| Goal                  | Minimize Error on Given Labels    | Discover Hidden Structure      | Better Generalization Accuracy       | Maximize Cumulative Reward                |\n",
    "| Typical Use Case      | Image Recognition, Email Spam Detection | Customer Segmentation, Anomaly Detection | Web Content Classification, Bioinformatics | Game AI, Robot Navigation, Real-time Decisions |\n",
    "| Training Efficiency   | High (due to direct feedback)     | Medium (no feedback)           | Varies (depends on labeled/unlabeled ratio) | Typically slow, trial and error-based      |\n",
    "| Complexity of Problem | Low-Medium                        | High                           | Medium-High                          | High                                      |\n",
    "| Real-time Adaptation  | Not Typically                     | Not Typically                  | Not Typically                        | Yes, using online learning                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Systems Concepts\n",
    "\n",
    "## Types and Storage of Data\n",
    "\n",
    "### Types of Data\n",
    "- Structured data\n",
    "    - data that has a defined length, format, and schema\n",
    "    - stored in relational databases, CRUD operations on records, ACID semantics\n",
    "    - examples: numbers, dates, strings\n",
    "- Unstructured data\n",
    "    - data that has no defined format or structure\n",
    "    - examples: text, images, audio, video\n",
    "- Semi-structured data\n",
    "    - data that has a defined structure, but not a defined schema\n",
    "    - attributes for every record could be different\n",
    "    - examples: XML, JSON\n",
    "\n",
    "3 Vs of Big Data\n",
    "- Volume (amount of data, terabytes, petabytes)\n",
    "- Velocity (speed of data generation, real-time, near real-time, batch, streaming)\n",
    "- Variety (types of data, structured, unstructured, semi-structured)\n",
    "- Veracity (uncertainty of data, trustworthiness, quality, accuracy, completeness) (4th V, newer)\n",
    "\n",
    "Issues with RDBMS:\n",
    "- Bigdata doesn't always need strong ACID semantics (esp systems of engagement)\n",
    "- Fixed schema is not sufficient, as application becomes popular more attributes need to be captured and DB modelling becomes an issue\n",
    "- Very wide de-normalized attribute sets\n",
    "- Data layout formats - column or row major - depends on use case\n",
    "- Expensive to retain and query long term data - need low cost solution\n",
    "\n",
    "Characteristics of Big Data Systems:\n",
    "- Application need not bother about common issues like sharding, replication\n",
    "    - devs focus on application logic rather than data management\n",
    "- Easier to model with flexible schema\n",
    "    - not necessary every record has same set of attributes\n",
    "- If possible, treat data as immutable\n",
    "    - keep adding timestamped versions of data values\n",
    "    - avoid human errors by not destroying a good copy\n",
    "- Built as distributed and incrementally scalable systems\n",
    "    - add new nodes to scale as in a Hadoop cluster\n",
    "- Options to have cheaper long term data retention\n",
    "    - long term data reads can have more latency and can be less expensive to store on commodity hardware, e.g. Hadoop file system (HDFS)\n",
    "- Generalized programming models that work close to the data\n",
    "    - e.g. Hadoop map-reduce that runs tasks on data nodes\n",
    "\n",
    "Challenges in Big Data Systems:\n",
    "- Latency issues in algorithms and data storage working with large data sets\n",
    "- Basic design considerations of Distributed and Parallel systems - reliability, availability, consistency\n",
    "- What data to keep and for how long - depends on analysis use case\n",
    "- Cleaning / Curation of data\n",
    "- Overall orchestration involving large volumes of data\n",
    "- Choose the right technologies from many options, including open source, to build the Big Data System for the use cases\n",
    "- Programming models for analysis\n",
    "- Scale out for high volume, search and analytics\n",
    "- Cloud is the cost effective way long term - but need to host Big Data outside the Enterprise\n",
    "- Data privacy and governance\n",
    "- Skilled coordinated teams to build/maintain Big Data Systems and analyse data\n",
    "\n",
    "Types of scalability:\n",
    "1. Vertical scaling (scaling up)\n",
    "    - increase the resources of a single node, demand architecture-aware algorithm design\n",
    "    - processing on x TB of data takes time t, then processing on (n*x) TB of data takes equal, less or much less than (n*t)\n",
    "    - e.g. more powerful CPU, more memory, \n",
    "2. Horizontal scaling (scaling out)\n",
    "    - increase the number of nodes, distribute the processing and storage tasks in parallel\n",
    "    - processing on x TB of data takes time t, then processing on (p*x) TB of data takes t or slightly more than t\n",
    "    - e.g. parallelization of jobs at several levels: distributing separate tasks onto separate threads on the same CPU, distributing separate tasks onto separate CPUs on the same computer, distributing separate tasks onto separate computers\n",
    "3. Elastic scaling (scaling up and down)\n",
    "    - dynamic provisioning based on computational need\n",
    "    - cloud computing\n",
    "    - on-demand service, resource pooling, scalability, accountability, broad network access\n",
    "\n",
    "Types of big data systems:\n",
    "1. batch processing of big data sources at rest\n",
    "    - building ML models, statistical aggregates\n",
    "2. real-time processing of big data in motion\n",
    "    - fraud detection from real-time financial transaction data\n",
    "3. interactive exploration with ad-hoc queries\n",
    "\n",
    "#### Big data architecture style\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/5e126d8f3906955d4fa2864535921f0b36e80537bc9f6c72908e9dd31a8a7683.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- big data solutions typically involve one or more of the following types of workload:\n",
    "    - batch processing of big data sources at rest\n",
    "    - real-time processing of big data in motion\n",
    "    - interactive exploration of big data\n",
    "    - predictive analytics and machine learning\n",
    "- components are usually: data sources, data storage, batch processing, real-time message ingestion, stream processing, analytical data store, analysis and reporting, orchestration, etc\n",
    "- benefits : choice in technology, performance through parallelism, scalability, interoperability with existing solutions\n",
    "- challenges : complexity, lack of standards, lack of skills, etc\n",
    "- look at Lambda architecture, Kappa architecture\n",
    "\n",
    "### Locality of reference\n",
    "\n",
    "Levels of storage:\n",
    "- computational data is stored in primary memory aka memory\n",
    "- persistent data is stored in secondary memory aka storage\n",
    "- remote data access from another computer's memory or storage is done over network\n",
    "\n",
    "Types:\n",
    "1. Temporal locality\n",
    "    - recently accessed data is likely to be accessed again\n",
    "    - e.g. loop iterations, function calls\n",
    "2. Spatial locality\n",
    "    - data near recently accessed data is likely to be accessed again\n",
    "    - e.g. sequential access, array traversal\n",
    "    - this is why columnar storage is better than row storage, because it is more likely that columns will be accessed together  for searching, filtering, etc\n",
    "\n",
    "#### Cache performance\n",
    "\n",
    "- cache hit: data requested by processor found in cache\n",
    "- cache miss: data requested by processor not found in cache, must be retrieved from main memory\n",
    "- cache hit ratio: fraction of memory accesses found in cache, $h = \\frac{hits}{hits + misses}$\n",
    "- average access time of any memory access: $t_{avg} = h*t_{cache} + (1-h)*t_{memory}$\n",
    "    - $t_{cache}$ - access time of cache\n",
    "    - $t_{memory}$ - access time of main memory\n",
    "- time required to access main memory block = $\\text{block\\_size} * t_{memory}$\n",
    "- time required to update cache block = $\\text{cache\\_block\\_size} * t_{cache}$\n",
    "\n",
    "Distributed Cache in Hadoop is a mechanism that allows copying small read-only files from HDFS to the local disks of the worker nodes, where they are accessible by the MapReduce tasks. These files are called localized and are tracked by the NodeManager. The files are deleted when they are not used by any task or when the cache size exceeds a limit. The cache size can be configured by a property.\n",
    "\n",
    "### Storage for Big Data\n",
    "\n",
    "RDBMS decline for Big Data due to:\n",
    "1. Scalability: RDBMS scale vertically; NoSQL scales horizontally for better cost-effective expansion.\n",
    "2. Schema flexibility: NoSQL adapts to dynamic data structures, unlike RDBMS with fixed schemas.\n",
    "3. Data variety: NoSQL handles diverse data types effectively compared to RDBMS.\n",
    "4. Query performance: NoSQL databases often outperform RDBMS for certain queries, crucial for Big Data analytics.\n",
    "5. Cost considerations: NoSQL's horizontal scaling on commodity hardware is more budget-friendly than vertical scaling of RDBMS.\n",
    "6. Concurrency and transaction overhead: NoSQL relaxes transaction constraints, prioritizing performance and scalability for Big Data use cases.\n",
    "\n",
    "Database Sharding:\n",
    "- Horizontal partitioning into shards for scalability.\n",
    "- Shards have dedicated hardware.\n",
    "- Enhances performance via workload distribution.\n",
    "- Enables parallel processing and better query performance.\n",
    "- Addresses vertical scaling limitations.\n",
    "- Handles increased data volumes and user loads effectively.\n",
    "- Sharding Types:\n",
    "1. Range: Divides data based on value ranges.\n",
    "2. Hash: Distributes data evenly using a hashing algorithm.\n",
    "3. Directory-Based: Uses a lookup directory for flexible shard assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Parallel and Distributed Systems\n",
    "\n",
    "| Parallel System | Distributed System |\n",
    "| --- | --- |\n",
    "| Computer system with several processing units attached to it | Independent, autonomous systems connected in a network accomplishing specific tasks |\n",
    "| A common shared memory can be directly accessed by every processing unit in a network | Coordination is possible between connected computers with own memory and CPU |\n",
    "| Tight coupling of processing resources that are used for solving single, complex problem | Loose coupling of computers connected in network, providing access to data and remotely located resources |\n",
    "| Programs may demand fine grain parallelism | Programs have coarse grain parallelism |\n",
    "\n",
    "\n",
    "#### Speedup calculations on parallel systems\n",
    "\n",
    "$$ \\text{Execution time after improvement} = \\frac{\\text{Execution time affected by improvement}}{\\text{Amount of improvement}} + \\text{Execution time unaffected} $$\n",
    "Let's say $f$ is the fraction of the code that is infinitely parallelizable, and N is the number of processors. Then, \n",
    "1. Amdahl’s Law\n",
    "    A rule stating that the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used.\n",
    "    $$ \\text{Speedup} = \\frac{\\text{Execution time single processor}}{\\text{Execution time on N parallel processors}} = \\frac{T(1-f) + Tf}{T(1-f) + \\frac{Tf}{N}} = \\frac{(1-f) + f}{(1-f) + \\frac{f}{N}} = \\frac{1}{(1-f) + \\frac{f}{N}} $$\n",
    "    It is used when the workload is fixed, and the number of processors is increased.\n",
    "2. Gustafson’s Law\n",
    "    A rule stating that the speedup with 1 processors for a given workload $W$, should be compared with with the speedup with N processors for a workload of size $W(N)$, where $W(N) = (1 - f)W + fNW$ (Parallelizable work can increase $N$ times)\n",
    "    $$ \\text{Speedup} = \\frac{T \\times W(N)}{T \\times W} = \\frac{W(N)}{W} = \\frac{(1 - f)W + fNW}{W} = 1 - f + fN $$\n",
    "    It is used when the workload size is increased proportionally to the number of processors.\n",
    "\n",
    "#### Memory access models\n",
    "- Shared memory\n",
    "    - multiple processors share a single memory space\n",
    "    - processors communicate by reading and writing to the same memory location\n",
    "    - e.g. OpenMP, Pthreads\n",
    "- Distributed memory\n",
    "    - each processor has its own private memory\n",
    "    - processors communicate by sending messages to each other\n",
    "    - e.g. MPI, PVM\n",
    "\n",
    "#### Shared Memory vs Message Passing:\n",
    "\n",
    "Shared Memory:\n",
    "- Tasks on different processors access a common address space.\n",
    "- Easier for programmers to conceptualize.\n",
    "- Single logical address space mapped onto physical memory.\n",
    "- Implemented as threads in a processor.\n",
    "- Options:\n",
    "    - Send Sync/Async, Blocking/Non-blocking.\n",
    "    - Receive Sync/Async, Blocking/Non-blocking.\n",
    "    - Handling complexities in terms of programming and wait times.\n",
    "  \n",
    "Distributed Memory (Message Passing):\n",
    "- Tasks access data from separate, isolated address spaces.\n",
    "- Communicate via sending/receiving messages.\n",
    "- Requires explicit communication for data exchange.\n",
    "- Harder programming abstraction compared to shared memory.\n",
    "- Data moved across virtual memories.\n",
    "- Harder for programmers due to explicit communication.\n",
    "\n",
    "#### Data access strategies - Replication, Partitioning, Messaging\n",
    "\n",
    "1. Partition \n",
    "    - Strategy: Partition data – typically, equally – to the nodes of the (distributed) system\n",
    "    - Cost: Network access and merge cost when query needs to go across partitions\n",
    "    - Advantage(s): Works well if task/algorithm is (mostly) data parallel, Works well when there is Locality of Reference within a partition\n",
    "    - Concerns: Merge across data fetched from multiple partitions, Partition balancing, Row vs Columnar layouts - what improves locality of reference ?\n",
    "2. Replication\n",
    "    - Strategy: Replicate all data across nodes of the (distributed) system\n",
    "    - Cost: Higher storage cost\n",
    "    - Advantage(s): All data accessed from local disk: no (runtime) communication on the network, High performance with parallel access, Fail over across replicas\n",
    "    - Concerns: Keep replicas in sync — various consistency models between readers and writers\n",
    "3. (Dynamic) Communication\n",
    "    - Strategy: Communicate (at runtime) only the data that is required\n",
    "    - Cost: High network cost for loosely coupled systems and data set to be exchanged is large\n",
    "    - Advantage(s): Minimal communication cost when only a small portion of the data is actually required by each node\n",
    "    - Concerns: Highly available and performant network, Fairly independent parallel data processing\n",
    "4. Networked Storage\n",
    "    - Common Storage on the Network:\n",
    "        - Storage Area Network (for raw access – i.e. disk block access)\n",
    "        - Network Attached Storage (for file access)\n",
    "    - Common Storage on the Cloud:\n",
    "        - Use Storage as a Service\n",
    "        - e.g. Amazon S3\n",
    "\n",
    "#### Computer clusters\n",
    "- type of distributed system that consists of a collection of inter-connected stand-alone computers working together as a single, integrated computing resource\n",
    "- examples:\n",
    "    - High Availability Clusters\n",
    "        - ServiceGuard, Lifekeeper, Failsafe, heartbeat, HACMP, failover clusters\n",
    "    - High Performance Clusters\n",
    "        - Beowulf; 1000 nodes; parallel programs; MPI\n",
    "    - Database Clusters\n",
    "        - Oracle Parallel Server (OPS)\n",
    "    - Storage Clusters\n",
    "        - Cluster filesystems; same view of data from each node\n",
    "- goals:\n",
    "    - continuous availability\n",
    "    - data integrity\n",
    "    - linear scalability\n",
    "    - open access\n",
    "    - parallelism in processing\n",
    "    - distributed systems management\n",
    "- built with high peforance, commodity hardware, high availability, and open source software\n",
    "\n",
    "#### Cloud computing\n",
    "- on-demand availability of computer system resources, especially data storage and computing power, without direct active management by the user\n",
    "- cluster is a building block for a datacenter, which is a building block for a cloud service\n",
    "- motivation to use clusters:\n",
    "    - rate of obsolescence of computers is high\n",
    "    - solution: build a cluster of commodity workstations\n",
    "    - scale-out clusters with commodity workstations as nodes are suitable for software environments that are resilient\n",
    "    - on the other hand, (public) cloud infrastructure is typically built as clusters of servers due to higher reliability of individual servers\n",
    "- typical cluster components:\n",
    "    - processor and memory\n",
    "    - network stack\n",
    "    - local storage\n",
    "    - OS and runtimes\n",
    "- split brain: caused by failure of heartbeat network connection(s), two halves of a cluster keep running, recovery options: allow cluster half with majority number of nodes to survive, force cluster with minority number of nodes to shut down\n",
    "- cluster middleware: single system image infrastructure, cluster services for availability, redundancy, fault-tolerance, recovery from failures\n",
    "- execution time on clusters, depends on : distribued scheduling, local on node scheduling, communication, synchronization, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability and Availability\n",
    "\n",
    "#### Metrics for reliability\n",
    "1. Mean Time To Failure (MTTF)\n",
    "    - average time between failures\n",
    "    - $MTTF = \\frac{\\text{Total hours of operation}}{\\text{Number of failures}} = \\frac{1}{\\text{Failure rate}}$\n",
    "2. Failure Rate\n",
    "    - number of failures per unit time\n",
    "    - $Failure rate = \\frac{1}{MTTF}$\n",
    "3. Mean Time To Repair (MTTR)\n",
    "    - average time to repair a failed component\n",
    "    - $MTTR = \\frac{\\text{Total hours for maintenance}}{\\text{Total number of repairs}}$\n",
    "4. Mean Time To Diagnose (MTTD)\n",
    "5. Mean Time Between Failures (MTBF)\n",
    "    - average time between failures\n",
    "    - $MTBF = MTTD + MTTR + MTTF$\n",
    "\n",
    "#### Metrics for availability\n",
    "1. Availability\n",
    "    - $Availability = \\frac{\\text{Time system is UP and accessible}}{\\text{Total time observed}} = \\frac{MTTF}{MTBF}$\n",
    "    - system is highly available when MTTF is high and MTTR is low\n",
    "\n",
    "#### Metrics for system with multiple components\n",
    "For a system with multiple components, the combined availability of the system is:\n",
    "1. Serial assembly of components\n",
    "    - failure of any component results in system failure\n",
    "    - $A_c = A_a \\times A_b$, where $A_a$ is the availability of component A and $A_b$ is the availability of component B\n",
    "    - failure rate of C = failure rate of A + failure rate of B = $\\frac{1}{MTTF_a} + \\frac{1}{MTTF_b}$\n",
    "    - $MTTF_c = \\frac{1}{\\frac{1}{MTTF_a} + \\frac{1}{MTTF_b}}$\n",
    "2. Parallel assembly of components\n",
    "    - failure of all components results in system failure\n",
    "    - $A_c = 1 - (1 - A_a)(1 - A_b)$\n",
    "    - $MTTF_c = MTTF_a + MTTF_b$\n",
    "\n",
    "#### Fault tolerance configurations\n",
    "\n",
    "| Configuration                 | Failover Time        | Active Component  | Replication Type                   |\n",
    "|-------------------------------|----------------------|-------------------|-------------------------------------|\n",
    "| Active-Active (load balanced)  | No failover time     | All components    | Bidirectional replication         |\n",
    "| Active-Passive (hot standby)   | Few seconds          | One active (passive up to date)        | Unidirectional replication        |\n",
    "| Warm standby                  | Few minutes          | One active (passive not fully up to date)         | Unidirectional replication with delay |\n",
    "| Cold standby                  | Few hours            | One active (passive not up-to-date, not running)        | Replication from secondary backup  |\n",
    "\n",
    "Different topologies:\n",
    "1. N+1 : N active nodes, 1 passive node \n",
    "2. N+M : N active nodes, M passive nodes\n",
    "3. N to 1 : N active nodes, 1 temporary passive node which returns services to active nodes after failure\n",
    "4. N to N : Any node failure is handled by distributing the load to other nodes\n",
    "\n",
    "Recovery:\n",
    "1. Diagnostic : Using heartbeat messages, the system detects the failure of a node\n",
    "2. Backward recovery : The system rolls back the transactions that were in progress at failure from the last checkpoint\n",
    "3. Forward recovery : The system re-executes the transactions that were in progress at failure from diagnosis data\n",
    "\n",
    "### Big Data Analytics\n",
    "Types of analytics:\n",
    "1. Descriptive (what happened)\n",
    "    - objective: summarize, interpret historical data for insights into past events\n",
    "    - methodology: involves data aggregation, summarization, visualization\n",
    "    - example: creating charts to illustrate trends in monthly website traffic\n",
    "2. Diagnostic (why did it happen)\n",
    "    - objective: identify reasons behind past events or trends\n",
    "    - methodology: investigates patterns, anomalies in data to understand root causes\n",
    "    - example: analyzing decrease in customer satisfaction scores for contributing factor\n",
    "3. Predictive (what will happen)\n",
    "    - objective: forecast future outcomes using historical data and patterns\n",
    "    - methodology: utilizes statistical models, machine learning algorithms for predictions\n",
    "    - example: predicting next quarter sales based on previous trends\n",
    "4. Prescriptive (how can we make it happen)\n",
    "    - objective: recommend actions to optimize future outcomes based on analysis\n",
    "    - methodology: combines predictive models with optimization techniques for suggestions\n",
    "    - example: recommending pricing adjustments for products based on predicted market trends\n",
    "\n",
    "Different aspects of big data analytics:\n",
    "1. Working with datasets of huge volume, velocity, variety beyond the capabilities of traditional data processing applications\n",
    "2. Processing data in parallel across multiple nodes in a distributed system\n",
    "3. Using specialized tools and techniques for data storage, processing, and analysis\n",
    "4. Use principles of locality to optimize performance and minimize network traffic\n",
    "5. Use of specialized programming models and languages for distributed computing\n",
    "6. Better faster decisions in real-time\n",
    "7. Richer faster insights from data of customers, products, operations, etc\n",
    "\n",
    "#### Big data analytics lifecycle\n",
    "1. Business Case Evaluation\n",
    "    \n",
    "2. Data Identification\n",
    "3. Data Acquisition & Filtering\n",
    "4. Data Extraction\n",
    "5. Data Validation & Cleansing\n",
    "6. Data Aggregation & Representation\n",
    "7. Data Analysis\n",
    "8. Data Visualization\n",
    "9. Utilization of Analysis Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "- open source framework for distributed storage and processing of large datasets\n",
    "- key components:\n",
    "    - HDFS (Hadoop Distributed File System)\n",
    "    - MapReduce\n",
    "    - YARN (Yet Another Resource Negotiator)\n",
    "- distributed storage: HDFS, stores data across multiple nodes for scalability and fault tolerance\n",
    "- distributed processing: MapReduce, allows parallel processing of vast datasets across a cluster of computers\n",
    "- scalability: scales horizontally by adding more nodes to the cluster to accommodate growing data volumes\n",
    "- fault tolerance: data redundancy and automatic recovery mechanisms ensure reliability in the event of hardware or software failures\n",
    "- ecosystem: offers a rich ecosystem with additional tools like:\n",
    "    - data ingestion : \n",
    "        - Sqoop: transfers data between Hadoop and relational databases, from RDBMS to HDFS, populating live tables in Hive and HBase\n",
    "        - Flume: collects, aggregates, and moves large amounts of streaming data into HDFS, from web servers, log files, etc\n",
    "    - data processing : \n",
    "        - MapReduce: distributed processing framework for batch processing of large datasets, supports Java, Python, C++, etc\n",
    "        - Spark: in-memory data processing engine, faster than MapReduce, supports multiple programming languages\n",
    "    - data analysis : \n",
    "        - Hive: data warehouse infrastructure, provides SQL-like query language called HiveQL, supports MapReduce and Spark\n",
    "        - Pig: data flow language and execution framework, supports MapReduce and Tez\n",
    "        - Impala: SQL query engine, supports low-latency queries on Hadoop datasets, supports HiveQL and SQL\n",
    "- programming language agnostic: supports various programming languages, allowing developers to use the language of their choice for writing MapReduce jobs\n",
    "- data locality: optimizes performance by processing data on the same node where it is stored, reducing data transfer overhead\n",
    "- use cases: \n",
    "    - widely used for batch processing, large-scale data analytics, and handling unstructured or semi-structured data\n",
    "    - Hadoop is a cornerstone in the big data landscape, providing a cost-effective and scalable solution for managing and analyzing massive datasets\n",
    "\n",
    "#### HDFS Architecture\n",
    "\n",
    "#### YARN Architecture\n",
    "\n",
    "#### Modes of operation\n",
    "1. Local (Standalone) Mode\n",
    "    - default mode, runs on a single node, no HDFS, no YARN\n",
    "    - used for debugging purposes\n",
    "2. Pseudo-Distributed Mode\n",
    "    - runs on a single node, HDFS, YARN\n",
    "    - all the daemons will be running as a separate Java process on separate JVMs\n",
    "    - used for development purposes\n",
    "3. Fully-Distributed Mode\n",
    "    - runs on clusters with multiple nodes, HDFS, YARN\n",
    "    - few of the nodes run master daemons like NameNode, ResourceManager, etc\n",
    "    - rest of the nodes run slave daemons like DataNode, NodeManager, etc\n",
    "    - all the daemons will be running as a separate Java process on separate JVMs\n",
    "    - used for production purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP Theorem\n",
    "- Brewer's conjecture: a distributed system cannot simultaneously provide all three of the following guarantees:\n",
    "    - Consistency: every read receives the most recent write or an error\n",
    "    - Availability: every request receives a (non-error) response, without the guarantee that it contains the most recent write\n",
    "    - Partition tolerance: the system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes\n",
    "- Different design choices for distributed systems:\n",
    "    - CA: All RDBMS, single data center, strong consistency, no network partition\n",
    "    - CP: HDFS, MongoDB, Redis, single data center, strong consistency, network partition\n",
    "    - AP: Cassandra, CouchDB, DynamoDB, multiple data centers, eventual consistency, network partition\n",
    "\n",
    "#### ACID properties\n",
    "- Atomicity: all or nothing, transaction is either fully completed or not at all\n",
    "- Consistency: transaction must bring the database from one valid state to another\n",
    "- Isolation: concurrent transactions do not interfere with each other\n",
    "- Durability: once a transaction is committed, it will remain so\n",
    "\n",
    "#### BASE properties\n",
    "A database design that sacrifices consistency for availability and partition tolerance\n",
    "- Basically Available: system guarantees availability (will always return a response) but not consistency (may return stale data)\n",
    "- Soft state: state of the system may be inconsistent, thus results might change over time even without input (as data is updated asynchronously)\n",
    "- Eventual consistency: system will become consistent over time, given that the system doesn't receive input during that time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB vs Cassandra\n",
    "\n",
    "| Feature                    | MongoDB                         | Cassandra                       |\n",
    "|----------------------------|---------------------------------|---------------------------------|\n",
    "| **Data Model**             | Document-based (BSON format)    | Wide-column store               |\n",
    "| **Query Language**         | MongoDB Query Language (MQL)     | CQL (Cassandra Query Language)  |\n",
    "| **Schema**                 | Dynamic schema (schema-less)     | Schema-agnostic                |\n",
    "| **Consistency Model**      | Eventual consistency            | Tunable consistency (can be adjusted per query) |\n",
    "| **Scaling**                | Horizontal scaling              | Linearly scalable               |\n",
    "| **Indexing**               | Rich indexing options            | Primary and secondary indexes   |\n",
    "| **Transactions**           | Supports multi-document transactions | Limited support for transactions |\n",
    "| **Joins**                  | Supports joins (with limitations) | No support for traditional joins |\n",
    "| **Data Distribution**      | Sharding for horizontal scaling | Automatic data distribution across nodes |\n",
    "| **Use Case**               | General-purpose, diverse use cases | Time-series data, write-intensive applications |\n",
    "| **ACID Compliance**        | Supports ACID transactions (in certain configurations) | ACID compliance with tunable consistency |\n",
    "\n",
    "#### Common MongoDB commands:\n",
    "1. show databases: `show databases`\n",
    "2. switch database: `use <database_name>`\n",
    "3. show collections: `show collections`\n",
    "4. insert document: `db.<collection_name>.insert({ key: value })`\n",
    "5. find documents: `db.<collection_name>.find()`\n",
    "6. query with criteria: `db.<collection_name>.find({ key: value })`\n",
    "7. update document: `db.<collection_name>.update({ key: value }, { $set: { new_key: new_value } })`\n",
    "8. delete document: `db.<collection_name>.remove({ key: value })`\n",
    "9. aggregate: `db.<collection_name>.aggregate([ ... ])`\n",
    "10. create index: `db.<collection_name>.createIndex({ key: 1 })`\n",
    "11. drop index: `db.<collection_name>.dropIndex(\"index_name\")`\n",
    "12. count documents: `db.<collection_name>.count()`\n",
    "13. limit results: `db.<collection_name>.find().limit(5)`\n",
    "14. sort results: `db.<collection_name>.find().sort({ key: 1 })`\n",
    "15. projection (select fields): `db.<collection_name>.find({}, { key: 1, _id: 0 })`\n",
    "16. bulk write operations: `db.<collection_name>.bulkWrite([ ... ])`\n",
    "17. show help: `db.<collection_name>.help()`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
