{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Systems Concepts\n",
    "\n",
    "## Types and Storage of Data\n",
    "\n",
    "### Types of Data\n",
    "- Structured data\n",
    "    - data that has a defined length, format, and schema\n",
    "    - stored in relational databases, CRUD operations on records, ACID semantics\n",
    "    - examples: numbers, dates, strings\n",
    "- Unstructured data\n",
    "    - data that has no defined format or structure\n",
    "    - examples: text, images, audio, video\n",
    "- Semi-structured data\n",
    "    - data that has a defined structure, but not a defined schema\n",
    "    - attributes for every record could be different\n",
    "    - examples: XML, JSON\n",
    "\n",
    "3 Vs of Big Data\n",
    "- Volume (amount of data, terabytes, petabytes)\n",
    "- Velocity (speed of data generation, real-time, near real-time, batch, streaming)\n",
    "- Variety (types of data, structured, unstructured, semi-structured)\n",
    "- Veracity (uncertainty of data, trustworthiness, quality, accuracy, completeness) (4th V, newer)\n",
    "\n",
    "Issues with RDBMS:\n",
    "- Bigdata doesn't always need strong ACID semantics (esp systems of engagement)\n",
    "- Fixed schema is not sufficient, as application becomes popular more attributes need to be captured and DB modelling becomes an issue\n",
    "- Very wide de-normalized attribute sets\n",
    "- Data layout formats - column or row major - depends on use case\n",
    "- Expensive to retain and query long term data - need low cost solution\n",
    "\n",
    "Characteristics of Big Data Systems:\n",
    "- Application need not bother about common issues like sharding, replication\n",
    "    - devs focus on application logic rather than data management\n",
    "- Easier to model with flexible schema\n",
    "    - not necessary every record has same set of attributes\n",
    "- If possible, treat data as immutable\n",
    "    - keep adding timestamped versions of data values\n",
    "    - avoid human errors by not destroying a good copy\n",
    "- Built as distributed and incrementally scalable systems\n",
    "    - add new nodes to scale as in a Hadoop cluster\n",
    "- Options to have cheaper long term data retention\n",
    "    - long term data reads can have more latency and can be less expensive to store on commodity hardware, e.g. Hadoop file system (HDFS)\n",
    "- Generalized programming models that work close to the data\n",
    "    - e.g. Hadoop map-reduce that runs tasks on data nodes\n",
    "\n",
    "Challenges in Big Data Systems:\n",
    "- Latency issues in algorithms and data storage working with large data sets\n",
    "- Basic design considerations of Distributed and Parallel systems - reliability, availability, consistency\n",
    "- What data to keep and for how long - depends on analysis use case\n",
    "- Cleaning / Curation of data\n",
    "- Overall orchestration involving large volumes of data\n",
    "- Choose the right technologies from many options, including open source, to build the Big Data System for the use cases\n",
    "- Programming models for analysis\n",
    "- Scale out for high volume, search and analytics\n",
    "- Cloud is the cost effective way long term - but need to host Big Data outside the Enterprise\n",
    "- Data privacy and governance\n",
    "- Skilled coordinated teams to build/maintain Big Data Systems and analyse data\n",
    "\n",
    "Types of scalability:\n",
    "1. Vertical scaling (scaling up)\n",
    "    - increase the resources of a single node, demand architecture-aware algorithm design\n",
    "    - processing on x TB of data takes time t, then processing on (n*x) TB of data takes equal, less or much less than (n*t)\n",
    "    - e.g. more powerful CPU, more memory, \n",
    "2. Horizontal scaling (scaling out)\n",
    "    - increase the number of nodes, distribute the processing and storage tasks in parallel\n",
    "    - processing on x TB of data takes time t, then processing on (p*x) TB of data takes t or slightly more than t\n",
    "    - e.g. parallelization of jobs at several levels: distributing separate tasks onto separate threads on the same CPU, distributing separate tasks onto separate CPUs on the same computer, distributing separate tasks onto separate computers\n",
    "3. Elastic scaling (scaling up and down)\n",
    "    - dynamic provisioning based on computational need\n",
    "    - cloud computing\n",
    "    - on-demand service, resource pooling, scalability, accountability, broad network access\n",
    "\n",
    "Types of big data systems:\n",
    "1. batch processing of big data sources at rest\n",
    "    - building ML models, statistical aggregates\n",
    "2. real-time processing of big data in motion\n",
    "    - fraud detection from real-time financial transaction data\n",
    "3. interactive exploration with ad-hoc queries\n",
    "\n",
    "#### Big data architecture style\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/5e126d8f3906955d4fa2864535921f0b36e80537bc9f6c72908e9dd31a8a7683.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- big data solutions typically involve one or more of the following types of workload:\n",
    "    - batch processing of big data sources at rest\n",
    "    - real-time processing of big data in motion\n",
    "    - interactive exploration of big data\n",
    "    - predictive analytics and machine learning\n",
    "- components are usually: data sources, data storage, batch processing, real-time message ingestion, stream processing, analytical data store, analysis and reporting, orchestration, etc\n",
    "- benefits : choice in technology, performance through parallelism, scalability, interoperability with existing solutions\n",
    "- challenges : complexity, lack of standards, lack of skills, etc\n",
    "- look at Lambda architecture, Kappa architecture\n",
    "\n",
    "### Locality of reference\n",
    "\n",
    "Levels of storage:\n",
    "- computational data is stored in primary memory aka memory\n",
    "- persistent data is stored in secondary memory aka storage\n",
    "- remote data access from another computer's memory or storage is done over network\n",
    "\n",
    "Types:\n",
    "1. Temporal locality\n",
    "    - recently accessed data is likely to be accessed again\n",
    "    - e.g. loop iterations, function calls\n",
    "2. Spatial locality\n",
    "    - data near recently accessed data is likely to be accessed again\n",
    "    - e.g. sequential access, array traversal\n",
    "    - this is why columnar storage is better than row storage, because it is more likely that columns will be accessed together  for searching, filtering, etc\n",
    "\n",
    "#### Cache performance\n",
    "\n",
    "- cache hit: data requested by processor found in cache\n",
    "- cache miss: data requested by processor not found in cache, must be retrieved from main memory\n",
    "- cache hit ratio: fraction of memory accesses found in cache, $h = \\frac{hits}{hits + misses}$\n",
    "- average access time of any memory access: $t_{avg} = h*t_{cache} + (1-h)*t_{memory}$\n",
    "    - $t_{cache}$ - access time of cache\n",
    "    - $t_{memory}$ - access time of main memory\n",
    "- time required to access main memory block = $\\text{block\\_size} * t_{memory}$\n",
    "- time required to update cache block = $\\text{cache\\_block\\_size} * t_{cache}$\n",
    "\n",
    "Distributed Cache in Hadoop is a mechanism that allows copying small read-only files from HDFS to the local disks of the worker nodes, where they are accessible by the MapReduce tasks. These files are called localized and are tracked by the NodeManager. The files are deleted when they are not used by any task or when the cache size exceeds a limit. The cache size can be configured by a property.\n",
    "\n",
    "### Storage for Big Data\n",
    "\n",
    "RDBMS decline for Big Data due to:\n",
    "1. Scalability: RDBMS scale vertically; NoSQL scales horizontally for better cost-effective expansion.\n",
    "2. Schema flexibility: NoSQL adapts to dynamic data structures, unlike RDBMS with fixed schemas.\n",
    "3. Data variety: NoSQL handles diverse data types effectively compared to RDBMS.\n",
    "4. Query performance: NoSQL databases often outperform RDBMS for certain queries, crucial for Big Data analytics.\n",
    "5. Cost considerations: NoSQL's horizontal scaling on commodity hardware is more budget-friendly than vertical scaling of RDBMS.\n",
    "6. Concurrency and transaction overhead: NoSQL relaxes transaction constraints, prioritizing performance and scalability for Big Data use cases.\n",
    "\n",
    "Database Sharding:\n",
    "- Horizontal partitioning into shards for scalability.\n",
    "- Shards have dedicated hardware.\n",
    "- Enhances performance via workload distribution.\n",
    "- Enables parallel processing and better query performance.\n",
    "- Addresses vertical scaling limitations.\n",
    "- Handles increased data volumes and user loads effectively.\n",
    "- Sharding Types:\n",
    "1. Range: Divides data based on value ranges.\n",
    "2. Hash: Distributes data evenly using a hashing algorithm.\n",
    "3. Directory-Based: Uses a lookup directory for flexible shard assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Parallel and Distributed Systems\n",
    "\n",
    "| Parallel System | Distributed System |\n",
    "| --- | --- |\n",
    "| Computer system with several processing units attached to it | Independent, autonomous systems connected in a network accomplishing specific tasks |\n",
    "| A common shared memory can be directly accessed by every processing unit in a network | Coordination is possible between connected computers with own memory and CPU |\n",
    "| Tight coupling of processing resources that are used for solving single, complex problem | Loose coupling of computers connected in network, providing access to data and remotely located resources |\n",
    "| Programs may demand fine grain parallelism | Programs have coarse grain parallelism |\n",
    "\n",
    "\n",
    "#### Speedup calculations on parallel systems\n",
    "\n",
    "$$ \\text{Execution time after improvement} = \\frac{\\text{Execution time affected by improvement}}{\\text{Amount of improvement}} + \\text{Execution time unaffected} $$\n",
    "Let's say $f$ is the fraction of the code that is infinitely parallelizable, and N is the number of processors. Then, \n",
    "1. Amdahl’s Law\n",
    "    A rule stating that the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used.\n",
    "    $$ \\text{Speedup} = \\frac{\\text{Execution time single processor}}{\\text{Execution time on N parallel processors}} = \\frac{T(1-f) + Tf}{T(1-f) + \\frac{Tf}{N}} = \\frac{(1-f) + f}{(1-f) + \\frac{f}{N}} = \\frac{1}{(1-f) + \\frac{f}{N}} $$\n",
    "    It is used when the workload is fixed, and the number of processors is increased.\n",
    "2. Gustafson’s Law\n",
    "    A rule stating that the speedup with 1 processors for a given workload $W$, should be compared with with the speedup with N processors for a workload of size $W(N)$, where $W(N) = (1 - f)W + fNW$ (Parallelizable work can increase $N$ times)\n",
    "    $$ \\text{Speedup} = \\frac{T \\times W(N)}{T \\times W} = \\frac{W(N)}{W} = \\frac{(1 - f)W + fNW}{W} = 1 - f + fN $$\n",
    "    It is used when the workload size is increased proportionally to the number of processors.\n",
    "\n",
    "#### Memory access models\n",
    "- Shared memory\n",
    "    - multiple processors share a single memory space\n",
    "    - processors communicate by reading and writing to the same memory location\n",
    "    - e.g. OpenMP, Pthreads\n",
    "- Distributed memory\n",
    "    - each processor has its own private memory\n",
    "    - processors communicate by sending messages to each other\n",
    "    - e.g. MPI, PVM\n",
    "\n",
    "#### Shared Memory vs Message Passing:\n",
    "\n",
    "Shared Memory:\n",
    "- Tasks on different processors access a common address space.\n",
    "- Easier for programmers to conceptualize.\n",
    "- Single logical address space mapped onto physical memory.\n",
    "- Implemented as threads in a processor.\n",
    "- Options:\n",
    "    - Send Sync/Async, Blocking/Non-blocking.\n",
    "    - Receive Sync/Async, Blocking/Non-blocking.\n",
    "    - Handling complexities in terms of programming and wait times.\n",
    "  \n",
    "Distributed Memory (Message Passing):\n",
    "- Tasks access data from separate, isolated address spaces.\n",
    "- Communicate via sending/receiving messages.\n",
    "- Requires explicit communication for data exchange.\n",
    "- Harder programming abstraction compared to shared memory.\n",
    "- Data moved across virtual memories.\n",
    "- Harder for programmers due to explicit communication.\n",
    "\n",
    "#### Data access strategies - Replication, Partitioning, Messaging\n",
    "\n",
    "1. Partition \n",
    "    - Strategy: Partition data – typically, equally – to the nodes of the (distributed) system\n",
    "    - Cost: Network access and merge cost when query needs to go across partitions\n",
    "    - Advantage(s): Works well if task/algorithm is (mostly) data parallel, Works well when there is Locality of Reference within a partition\n",
    "    - Concerns: Merge across data fetched from multiple partitions, Partition balancing, Row vs Columnar layouts - what improves locality of reference ?\n",
    "2. Replication\n",
    "    - Strategy: Replicate all data across nodes of the (distributed) system\n",
    "    - Cost: Higher storage cost\n",
    "    - Advantage(s): All data accessed from local disk: no (runtime) communication on the network, High performance with parallel access, Fail over across replicas\n",
    "    - Concerns: Keep replicas in sync — various consistency models between readers and writers\n",
    "3. (Dynamic) Communication\n",
    "    - Strategy: Communicate (at runtime) only the data that is required\n",
    "    - Cost: High network cost for loosely coupled systems and data set to be exchanged is large\n",
    "    - Advantage(s): Minimal communication cost when only a small portion of the data is actually required by each node\n",
    "    - Concerns: Highly available and performant network, Fairly independent parallel data processing\n",
    "4. Networked Storage\n",
    "    - Common Storage on the Network:\n",
    "        - Storage Area Network (for raw access – i.e. disk block access)\n",
    "        - Network Attached Storage (for file access)\n",
    "    - Common Storage on the Cloud:\n",
    "        - Use Storage as a Service\n",
    "        - e.g. Amazon S3\n",
    "\n",
    "#### Computer clusters\n",
    "- type of distributed system that consists of a collection of inter-connected stand-alone computers working together as a single, integrated computing resource\n",
    "- examples:\n",
    "    - High Availability Clusters\n",
    "        - ServiceGuard, Lifekeeper, Failsafe, heartbeat, HACMP, failover clusters\n",
    "    - High Performance Clusters\n",
    "        - Beowulf; 1000 nodes; parallel programs; MPI\n",
    "    - Database Clusters\n",
    "        - Oracle Parallel Server (OPS)\n",
    "    - Storage Clusters\n",
    "        - Cluster filesystems; same view of data from each node\n",
    "- goals:\n",
    "    - continuous availability\n",
    "    - data integrity\n",
    "    - linear scalability\n",
    "    - open access\n",
    "    - parallelism in processing\n",
    "    - distributed systems management\n",
    "- built with high peforance, commodity hardware, high availability, and open source software\n",
    "\n",
    "#### Cloud computing\n",
    "- on-demand availability of computer system resources, especially data storage and computing power, without direct active management by the user\n",
    "- cluster is a building block for a datacenter, which is a building block for a cloud service\n",
    "- motivation to use clusters:\n",
    "    - rate of obsolescence of computers is high\n",
    "    - solution: build a cluster of commodity workstations\n",
    "    - scale-out clusters with commodity workstations as nodes are suitable for software environments that are resilient\n",
    "    - on the other hand, (public) cloud infrastructure is typically built as clusters of servers due to higher reliability of individual servers\n",
    "- typical cluster components:\n",
    "    - processor and memory\n",
    "    - network stack\n",
    "    - local storage\n",
    "    - OS and runtimes\n",
    "- split brain: caused by failure of heartbeat network connection(s), two halves of a cluster keep running, recovery options: allow cluster half with majority number of nodes to survive, force cluster with minority number of nodes to shut down\n",
    "- cluster middleware: single system image infrastructure, cluster services for availability, redundancy, fault-tolerance, recovery from failures\n",
    "- execution time on clusters, depends on : distribued scheduling, local on node scheduling, communication, synchronization, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability and Availability\n",
    "\n",
    "#### Metrics for reliability\n",
    "1. Mean Time To Failure (MTTF)\n",
    "    - average time between failures\n",
    "    - $MTTF = \\frac{\\text{Total hours of operation}}{\\text{Number of failures}} = \\frac{1}{\\text{Failure rate}}$\n",
    "2. Failure Rate\n",
    "    - number of failures per unit time\n",
    "    - $Failure rate = \\frac{1}{MTTF}$\n",
    "3. Mean Time To Repair (MTTR)\n",
    "    - average time to repair a failed component\n",
    "    - $MTTR = \\frac{\\text{Total hours for maintenance}}{\\text{Total number of repairs}}$\n",
    "4. Mean Time To Diagnose (MTTD)\n",
    "5. Mean Time Between Failures (MTBF)\n",
    "    - average time between failures\n",
    "    - $MTBF = MTTD + MTTR + MTTF$\n",
    "\n",
    "#### Metrics for availability\n",
    "1. Availability\n",
    "    - $Availability = \\frac{\\text{Time system is UP and accessible}}{\\text{Total time observed}} = \\frac{MTTF}{MTBF}$\n",
    "    - system is highly available when MTTF is high and MTTR is low\n",
    "\n",
    "#### Metrics for system with multiple components\n",
    "For a system with multiple components, the combined availability of the system is:\n",
    "1. Serial assembly of components\n",
    "    - failure of any component results in system failure\n",
    "    - $A_c = A_a \\times A_b$, where $A_a$ is the availability of component A and $A_b$ is the availability of component B\n",
    "    - failure rate of C = failure rate of A + failure rate of B = $\\frac{1}{MTTF_a} + \\frac{1}{MTTF_b}$\n",
    "    - $MTTF_c = \\frac{1}{\\frac{1}{MTTF_a} + \\frac{1}{MTTF_b}}$\n",
    "2. Parallel assembly of components\n",
    "    - failure of all components results in system failure\n",
    "    - $A_c = 1 - (1 - A_a)(1 - A_b)$\n",
    "    - $MTTF_c = MTTF_a + MTTF_b$\n",
    "\n",
    "#### Fault tolerance configurations\n",
    "\n",
    "| Configuration                 | Failover Time        | Active Component  | Replication Type                   |\n",
    "|-------------------------------|----------------------|-------------------|-------------------------------------|\n",
    "| Active-Active (load balanced)  | No failover time     | All components    | Bidirectional replication         |\n",
    "| Active-Passive (hot standby)   | Few seconds          | One active (passive up to date)        | Unidirectional replication        |\n",
    "| Warm standby                  | Few minutes          | One active (passive not fully up to date)         | Unidirectional replication with delay |\n",
    "| Cold standby                  | Few hours            | One active (passive not up-to-date, not running)        | Replication from secondary backup  |\n",
    "\n",
    "Different topologies:\n",
    "1. N+1 : N active nodes, 1 passive node \n",
    "2. N+M : N active nodes, M passive nodes\n",
    "3. N to 1 : N active nodes, 1 temporary passive node which returns services to active nodes after failure\n",
    "4. N to N : Any node failure is handled by distributing the load to other nodes\n",
    "\n",
    "Recovery:\n",
    "1. Diagnostic : Using heartbeat messages, the system detects the failure of a node\n",
    "2. Backward recovery : The system rolls back the transactions that were in progress at failure from the last checkpoint\n",
    "3. Forward recovery : The system re-executes the transactions that were in progress at failure from diagnosis data\n",
    "\n",
    "### Big Data Analytics\n",
    "Types of analytics:\n",
    "1. Descriptive (what happened)\n",
    "    - objective: summarize, interpret historical data for insights into past events\n",
    "    - methodology: involves data aggregation, summarization, visualization\n",
    "    - example: creating charts to illustrate trends in monthly website traffic\n",
    "2. Diagnostic (why did it happen)\n",
    "    - objective: identify reasons behind past events or trends\n",
    "    - methodology: investigates patterns, anomalies in data to understand root causes\n",
    "    - example: analyzing decrease in customer satisfaction scores for contributing factor\n",
    "3. Predictive (what will happen)\n",
    "    - objective: forecast future outcomes using historical data and patterns\n",
    "    - methodology: utilizes statistical models, machine learning algorithms for predictions\n",
    "    - example: predicting next quarter sales based on previous trends\n",
    "4. Prescriptive (how can we make it happen)\n",
    "    - objective: recommend actions to optimize future outcomes based on analysis\n",
    "    - methodology: combines predictive models with optimization techniques for suggestions\n",
    "    - example: recommending pricing adjustments for products based on predicted market trends\n",
    "\n",
    "Different aspects of big data analytics:\n",
    "1. Working with datasets of huge volume, velocity, variety beyond the capabilities of traditional data processing applications\n",
    "2. Processing data in parallel across multiple nodes in a distributed system\n",
    "3. Using specialized tools and techniques for data storage, processing, and analysis\n",
    "4. Use principles of locality to optimize performance and minimize network traffic\n",
    "5. Use of specialized programming models and languages for distributed computing\n",
    "6. Better faster decisions in real-time\n",
    "7. Richer faster insights from data of customers, products, operations, etc\n",
    "\n",
    "#### Big data analytics lifecycle\n",
    "1. Business Case Evaluation\n",
    "    \n",
    "2. Data Identification\n",
    "3. Data Acquisition & Filtering\n",
    "4. Data Extraction\n",
    "5. Data Validation & Cleansing\n",
    "6. Data Aggregation & Representation\n",
    "7. Data Analysis\n",
    "8. Data Visualization\n",
    "9. Utilization of Analysis Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop\n",
    "- open source framework for distributed storage and processing of large datasets\n",
    "- key components:\n",
    "    - HDFS (Hadoop Distributed File System)\n",
    "    - MapReduce\n",
    "    - YARN (Yet Another Resource Negotiator)\n",
    "\n",
    "<img alt=\"picture 0\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/ebd34154f5a8c257141e1647b0a8a5d7638bbef6f1e1dde8398ff3ce7677aab1.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- distributed storage: HDFS, stores data across multiple nodes for scalability and fault tolerance\n",
    "- distributed processing: MapReduce, allows parallel processing of vast datasets across a cluster of computers\n",
    "- scalability: scales horizontally by adding more nodes to the cluster to accommodate growing data volumes\n",
    "- fault tolerance: data redundancy and automatic recovery mechanisms ensure reliability in the event of hardware or software failures\n",
    "- ecosystem: offers a rich ecosystem with additional tools like:\n",
    "    - data ingestion : \n",
    "        - Sqoop: transfers data between Hadoop and relational databases, from RDBMS to HDFS, populating live tables in Hive and HBase\n",
    "        - Flume: collects, aggregates, and moves large amounts of streaming data into HDFS, from web servers, log files, etc\n",
    "    - data processing : \n",
    "        - MapReduce: distributed processing framework for batch processing of large datasets, supports Java, Python, C++, etc\n",
    "        - Spark: in-memory data processing engine, faster than MapReduce, supports multiple programming languages\n",
    "    - data analysis : \n",
    "        - Hive: data warehouse infrastructure, provides SQL-like query language called HiveQL, supports MapReduce and Spark\n",
    "        - Pig: data flow language and execution framework, supports MapReduce and Tez\n",
    "        - Impala: SQL query engine, supports low-latency queries on Hadoop datasets, supports HiveQL and SQL\n",
    "- programming language agnostic: supports various programming languages, allowing developers to use the language of their choice for writing MapReduce jobs\n",
    "- data locality: optimizes performance by processing data on the same node where it is stored, reducing data transfer overhead\n",
    "- use cases: \n",
    "    - widely used for batch processing, large-scale data analytics, and handling unstructured or semi-structured data\n",
    "    - Hadoop is a cornerstone in the big data landscape, providing a cost-effective and scalable solution for managing and analyzing massive datasets\n",
    "\n",
    "#### HDFS Architecture\n",
    "\n",
    "<img alt=\"picture 1\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/1bad92038b580b8fd5e36ac5de6de728a0a9fc8458b89f14e7016228cdb8df75.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Master slave architecture within a HDFS cluster\n",
    "- Master node with NameNode\n",
    "    - maintains namespace - filename to blocks and their replica mappings\n",
    "    - serves as arbitrator and doesn't handle actual data flow\n",
    "    - HDFS client app interacts with NameNode for metadata\n",
    "- Slave nodes with DataNode\n",
    "    - serves block read/write from clients\n",
    "    - serves create/delete/replicate requests from NameNode\n",
    "    - DataNodes interact with each other for pipeline reads and writes\n",
    "- NameNode functions:\n",
    "    - maintains and manages the file system namespace, with two files:\n",
    "        1. FsImage: contains mapping of blocks to file, hierarchy, file properties, permissions\n",
    "        2. EditLog: transaction log of changes to metadata in FsImage\n",
    "    - does not store any data, only metadata about files\n",
    "    - runs on master node while DataNodes run on slave nodes\n",
    "    - records each change that takes place to the metadata, e.g. if a file is deleted in HDFS, the NameNode will immediately record this in the EditLog\n",
    "    - receives periodic heartbeat and a block report from all the DataNodes in the cluster to ensure that the DataNodes are live\n",
    "    - ensures replication factor is maintained across DataNode failures\n",
    "    - in case of the DataNode failure, the NameNode chooses new DataNodes for new replicas, balance disk usage and manages the communication traffic to the DataNodes\n",
    "- DataNode functions:\n",
    "    - stores data in the local file system\n",
    "    - sends heartbeat messages to the NameNode periodically to confirm that it is alive\n",
    "    - sends block report to the NameNode periodically to report the list of blocks it is storing\n",
    "    - serves read/write requests from clients\n",
    "    - serves create/delete/replicate requests from NameNode\n",
    "    - in case of a block failure, the NameNode will choose a new DataNode to create a replica of the block\n",
    "- Secondary NameNode functions:\n",
    "    - performs periodic checkpoints of the namespace by merging the FsImage and EditLog\n",
    "    - downloads the FsImage and EditLog from the NameNode, merges them, and uploads the new FsImage back to the NameNode\n",
    "    - does not store any data, only metadata about files\n",
    "    - runs on a separate node from the NameNode\n",
    "    - performs regular checkpoints of the namespace by merging the FsImage and EditLog, hence called CheckpointNode\n",
    "    \n",
    "#### YARN Architecture\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8e7bb7d535fc51c60dcb642eb68e8e9ebc5c9688579c2011d20d3b059d71596d.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "YARN workflow:\n",
    "1. client program submits the application / job with specs to start AppMaster\n",
    "2. ResourceManager asks a NodeManager to start a container which can host the ApplicationMaster and then launches ApplicationMaster\n",
    "3. ApplicationMaster on start-up registers with ResourceManager. So now the client can contact the ApplicationMaster directly also for application specific details\n",
    "4. As the application executes, AppMaster negotiates resources in the form of containers via the resource request protocol involving the ResourceManager\n",
    "5. As a container is allocated successfully for an application, AppMaster works with the NodeManager on same or diff node to launch the container as per the container spec. The spec involves how the AppMaster can communicate with the container\n",
    "6. App specific code inside container provides runtime information to AppMaster for progress, status etc. via application-specific protocol\n",
    "7. Client that submitted the app / job can directly communicate with the AppMaster for progress, status updates. via the application specific protocol\n",
    "8. On completion of the app / job, AppMaster de-registers from ResourceManager and shuts down. So the containers allocated can be re-purposed.\n",
    "\n",
    "#### Modes of operation\n",
    "1. Local (Standalone) Mode\n",
    "    - default mode, runs on a single node, no HDFS, no YARN\n",
    "    - used for debugging purposes\n",
    "2. Pseudo-Distributed Mode\n",
    "    - runs on a single node, HDFS, YARN\n",
    "    - all the daemons will be running as a separate Java process on separate JVMs\n",
    "    - used for development purposes\n",
    "3. Fully-Distributed Mode\n",
    "    - runs on clusters with multiple nodes, HDFS, YARN\n",
    "    - few of the nodes run master daemons like NameNode, ResourceManager, etc\n",
    "    - rest of the nodes run slave daemons like DataNode, NodeManager, etc\n",
    "    - all the daemons will be running as a separate Java process on separate JVMs\n",
    "    - used for production purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP Theorem\n",
    "- Brewer's conjecture: a distributed system cannot simultaneously provide all three of the following guarantees:\n",
    "    - Consistency: every read receives the most recent write or an error\n",
    "    - Availability: every request receives a (non-error) response, without the guarantee that it contains the most recent write\n",
    "    - Partition tolerance: the system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes\n",
    "- Different design choices for distributed systems:\n",
    "    - CA: All RDBMS, single data center, strong consistency, no network partition\n",
    "    - CP: HDFS, MongoDB, Redis, single data center, strong consistency, network partition\n",
    "    - AP: Cassandra, CouchDB, DynamoDB, multiple data centers, eventual consistency, network partition\n",
    "\n",
    "#### ACID properties\n",
    "- Atomicity: all or nothing, transaction is either fully completed or not at all\n",
    "- Consistency: transaction must bring the database from one valid state to another\n",
    "- Isolation: concurrent transactions do not interfere with each other\n",
    "- Durability: once a transaction is committed, it will remain so\n",
    "\n",
    "#### BASE properties\n",
    "A database design that sacrifices consistency for availability and partition tolerance\n",
    "- Basically Available: system guarantees availability (will always return a response) but not consistency (may return stale data)\n",
    "- Soft state: state of the system may be inconsistent, thus results might change over time even without input (as data is updated asynchronously)\n",
    "- Eventual consistency: system will become consistent over time, given that the system doesn't receive input during that time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDB vs Cassandra\n",
    "\n",
    "| Feature                    | MongoDB                         | Cassandra                       |\n",
    "|----------------------------|---------------------------------|---------------------------------|\n",
    "| Data Model             | Document-based (BSON format)    | Wide-column store               |\n",
    "| Query Language         | MongoDB Query Language (MQL)     | CQL (Cassandra Query Language)  |\n",
    "| Schema                 | Dynamic schema (schema-less)     | Schema-agnostic                |\n",
    "| Consistency Model      | Eventual consistency            | Tunable consistency (can be adjusted per query) |\n",
    "| Scaling                | Horizontal scaling              | Linearly scalable               |\n",
    "| Indexing               | Rich indexing options            | Primary and secondary indexes   |\n",
    "| Transactions           | Supports multi-document transactions | Limited support for transactions |\n",
    "| Joins                  | Supports joins (with limitations) | No support for traditional joins |\n",
    "| Data Distribution      | Sharding for horizontal scaling | Automatic data distribution across nodes |\n",
    "| Use Case               | General-purpose, diverse use cases | Time-series data, write-intensive applications |\n",
    "| ACID Compliance        | Supports ACID transactions (in certain configurations) | ACID compliance with tunable consistency |\n",
    "\n",
    "### Common MongoDB commands:\n",
    "1. show databases: `show databases` / `show dbs`\n",
    "2. switch database: `use <database_name>`\n",
    "3. show collections: `show collections`\n",
    "4. create collection: `db.createCollection(\"<collection_name>\")`\n",
    "4. insert document: `db.<collection_name>.insert({ key: value })`\n",
    "5. find documents: `db.<collection_name>.find()`\n",
    "6. query with criteria: \n",
    "    - `db.<collection_name>.find({ key: value })`\n",
    "    - `db.<collection_name>.find({ key: value }, { key: 1, _id: 0 })` (projection)\n",
    "    - `db.<collection_name>.find({ key: { $gt: value } })` (greater than)\n",
    "    - `db.<collection_name>.find({ key1: value1, key2: value2 })` (AND)\n",
    "    - `db.<collection_name>.find({ $or: [{ key1: value1 }, { key2: value2 }] })` (OR)\n",
    "    - `db.<collection_name>.find({ key: { $in: [value1, value2] } })` (IN)\n",
    "7. update document: \n",
    "    - `db.<collection_name>.update({ key: value }, { $set: { new_key: new_value } })`\n",
    "    - `db.<collection_name>.update({ key: value }, { $set: { new_key: new_value } }, { upsert: true })`\n",
    "8. delete document: `db.<collection_name>.remove({ key: value })`\n",
    "9. aggregate: `db.<collection_name>.aggregate([ ... ])`\n",
    "10. create index: `db.<collection_name>.createIndex({ key: 1 })`\n",
    "11. drop index: `db.<collection_name>.dropIndex(\"index_name\")`\n",
    "12. count documents: `db.<collection_name>.count()`\n",
    "13. limit results: `db.<collection_name>.find().limit(5)`\n",
    "14. sort results: \n",
    "    - `db.<collection_name>.find().sort({ key: 1 })`\n",
    "    - `db.<collection_name>.find().sort({ key1: 1, key2: -1 })` (sort by key1 ascending, then by key2 descending)\n",
    "15. projection (select fields): `db.<collection_name>.find({}, { key: 1, _id: 0 })`\n",
    "16. bulk write operations: `db.<collection_name>.bulkWrite([ ... ])`\n",
    "17. show help: `db.<collection_name>.help()`\n",
    "\n",
    "#### Aggregation pipeline\n",
    "- framework for data aggregation modeled on the concept of data processing pipelines\n",
    "- documents enter a multi-stage pipeline that transforms the documents into an aggregated result\n",
    "- each stage transforms the documents as they pass through the pipeline\n",
    "- `db.<collection_name>.aggregate(pipeline, options)`\n",
    "- eg: \n",
    "    1. `$match` stage filters the documents by some criteria\n",
    "    2. `$group` stage groups the documents by some criteria\n",
    "    3. `$sort` stage sorts the documents by some criteria\n",
    "    4. `$project` stage selects some fields from the documents\n",
    "    5. `$limit` stage limits the number of documents to be returned\n",
    "    6. `$project` stage selects some fields from the documents\n",
    "\n",
    "\n",
    "Import data from CSV:\n",
    "\n",
    "`mongoimport --db <database_name> --collection <collection_name> --type csv --headerline --file <file_name>`\n",
    "\n",
    "Export data to CSV:\n",
    "\n",
    "`mongoexport --db <database_name> --collection <collection_name> --type csv --fields <field1,field2> --out <file_name>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of parallelism\n",
    "\n",
    "- Data Parallelism:\n",
    "  - Definition: Distribute data across multiple processing units or nodes; perform the same operation concurrently.\n",
    "  - Example: Distribute portions of a large dataset to multiple processors; each processor independently processes its assigned data.\n",
    "\n",
    "- Tree Parallelism:\n",
    "  - Definition: Organize parallel tasks hierarchically in a tree-like structure; tasks are divided into sub-tasks forming a tree structure.\n",
    "  - Example: Main task divided into sub-tasks; each sub-task further divided into more specific tasks for efficient resource utilization.\n",
    "\n",
    "- Task Parallelism:\n",
    "  - Definition: Break down a program into independent tasks or processes; execute tasks concurrently.\n",
    "  - Example: Execute different program functions or modules concurrently without relying on each other's output; common in parallel programming frameworks.\n",
    "\n",
    "- Request Parallelism:\n",
    "  - Definition: Divide a computation into stages in a pipeline; each stage represents a distinct operation.\n",
    "  - Example: Tasks in a pipeline architecture are handled by separate units; each stage operates on data concurrently. Used in scenarios with sequential dependence of operations.\n",
    "\n",
    "## Map Reduce \n",
    "- programming model for processing large datasets in parallel across a cluster of computers\n",
    "- MapReduce is a framework for processing data in parallel across a cluster of computers\n",
    "\n",
    "A MapReduce framework (or system) is usually composed of three operations (or steps):\n",
    "1. Map: each worker node applies the map function to the local data, and writes the output to a temporary storage. A master node ensures that only one copy of the redundant input data is processed.\n",
    "  `Map(k1,v1) → list(k2,v2)`\n",
    "2. Shuffle: worker nodes redistribute data based on the output keys (produced by the map function), such that all data belonging to one key is located on the same worker node.\n",
    "3. Reduce: worker nodes now process each group of output data, per key, in parallel.\n",
    "  `Reduce(k2, list (v2)) → list((k3, v3))`\n",
    "\n",
    "```python\n",
    "function map(String name, String document):\n",
    "    // name: document name\n",
    "    // document: document contents\n",
    "    for each word w in document:\n",
    "        emit (w, 1)\n",
    "\n",
    "function reduce(String word, Iterator partialCounts):\n",
    "    // word: a word\n",
    "    // partialCounts: a list of aggregated partial counts\n",
    "    sum = 0\n",
    "    for each pc in partialCounts:\n",
    "        sum += pc\n",
    "    emit (word, sum)\n",
    "```\n",
    "\n",
    "## Apache Spark\n",
    "- open-source, distributed computing system that provides a fast and general-purpose cluster-computing framework for big data processing\n",
    "- developed to address the limitations of the MapReduce model and offers a more flexible and efficient alternative\n",
    "- features:\n",
    "    - speed: performs in-memory processing, which significantly improves the processing speed compared to the traditional MapReduce model that relies heavily on disk-based storage\n",
    "    - ease of use: provides high-level APIs in Java, Scala, Python, and R, making it accessible to a broad audience\n",
    "    - versatility: supports a wide range of data processing tasks, including batch processing, interactive queries, streaming analytics, and machine learning\n",
    "    - in-memory processing: utilizes resilient distributed datasets (RDDs), an immutable distributed collection of objects, to store data in-memory across a cluster\n",
    "    - fault tolerance: provides fault tolerance through lineage information stored in RDDs\n",
    "    - data processing libraries: comes with built-in libraries for various data processing tasks, such as Spark SQL for structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time data processing\n",
    "    - ease of integration: can be easily integrated with other popular big data technologies, such as Apache Hadoop, Apache Hive, Apache HBase, and more\n",
    "    - lazy evaluation: uses lazy evaluation, meaning that transformations on RDDs are not executed immediately\n",
    "    - community support: has a large and active open-source community, which contributes to its development and provides support through forums, mailing lists, and documentation\n",
    "    - cluster manager integration: can run on various cluster managers, including Apache Mesos, Apache Hadoop YARN, and its standalone built-in cluster manager\n",
    "\n",
    "<img alt=\"picture 3\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/165f411f2ed28f5b894819b57bc4f58dc4aebf98a14e9194a18cb50f9d98d8b4.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "2 main abstractions:\n",
    "1. Resilient Distributed Dataset (RDD)\n",
    "    - immutable, distributed collection of objects\n",
    "    - partitioned across nodes in a cluster\n",
    "    - can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs\n",
    "    - can be cached in memory across machines, can be recomputed if lost due to node failure\n",
    "2. Directed Acyclic Graph (DAG)\n",
    "    - sequence of computations on data\n",
    "    - each node in the graph represents a RDD, each edge represents a transformation on the data\n",
    "    - transformations are lazy, only executed when an action is called, evaluated in parallel, fault-tolerant, can be recomputed if lost due to node failure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
