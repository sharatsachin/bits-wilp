{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Leaning\n",
    "\n",
    "## Fundamentals of Neural Network\n",
    "\n",
    "### Why Deep Learning?\n",
    "\n",
    "Limitations of linear models:\n",
    "- Linearity implies the weaker assumption of monotonicity (increse or decrease in feature must always cause an increase or decrease in model's output)\n",
    "- Linearity is not always plausible\n",
    "    - predicting health as a function of body temperature (temp above and below 37 deg C indicates greater risk)\n",
    "    - classifying images of cats and dogs (increasing the intensity of the pixel at location (13, 17) always increase or decrease the likelihood that the image depicts a dog)\n",
    "- We overcome this by using deep neural networks to learn both a representation via hidden layers and a linear predictor that acts upon that representation\n",
    "\n",
    "### Applications of Deep Learning\n",
    "\n",
    "| Area | Description |\n",
    "|------|-------------|\n",
    "| image recognition | identify and classify objects, patterns, or scenes in images |\n",
    "| natural language processing | understand, interpret, and generate human language |\n",
    "| speech recognition | convert spoken language into text or commands |\n",
    "| autonomous vehicles | enable self-driving capabilities for cars and drones |\n",
    "| healthcare | assist in disease diagnosis, medical imaging, and drug discovery |\n",
    "| finance | predict stock prices, fraud detection, and risk assessment |\n",
    "| recommender systems | personalize recommendations for products or content |\n",
    "| robotics | enhance perception and decision-making in robotic systems |\n",
    "| gaming | create realistic simulations and improve in-game AI |\n",
    "| cybersecurity | detect and prevent security threats and intrusions |\n",
    "| generative models | generate realistic images, videos, and text |\n",
    "| virtual assistants | power natural language interfaces and virtual helpers |\n",
    "| environmental monitoring | analyze data for climate modeling and environmental studies |\n",
    "\n",
    "### Perceptron and Perceptron learning algorithm\n",
    "\n",
    "<img alt=\"picture 4\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/3151c7650e8ee2f625afc8a34a9960a1ff70c1d03d7aebb89e7dd72a4d51bfb3.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- It is a binary classification algorithm that forms the foundation of neural networks (Frank Rosenblatt in 1957)\n",
    "- The algorithm takes input features, each multiplied by a corresponding weight, and produces an output through a threshold function\n",
    "- If the output exceeds a specified threshold, the perceptron classifies the input as one class; otherwise, it belongs to the other class\n",
    "- During training, the algorithm adjusts the weights based on misclassifications, attempting to minimize the error\n",
    "- The perceptron learning algorithm is limited to linearly separable problems and is a single-layer neural network, paving the way for more complex models in modern deep learning\n",
    "\n",
    "### Multilayer Perceptron (MLP)\n",
    "\n",
    "We overcome the limitations of linear models by incorporating hidden layers into our model. The hidden layers allow us to model non-linear relationships between our features and the output. The hidden layers are also called as representation layers as they learn a representation of the data that is used by the final layer to make the prediction.\n",
    "- Think of the first $L-1$ layers as learning a representation of the data, and the final layer as using that representation to make a linear prediction.\n",
    "- This is called a multilayer perceptron (MLP) or a feedforward neural network (FFNN).\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/5d52b7a5eb8f67b1ae9a5f73b0555decc85d27247eee9a33a219452acd4842af.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "Say we have a MLP with $L = 1$ hidden layer. The input layer has $d$ features, the hidden layer has $h$ units and the output layer has $q$ units. Then:\n",
    "- We denote by $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$ the weight matrix between the input layer and the hidden layer, and by $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}$ the weight matrix between the hidden layer and the output layer.\n",
    "- The bias vector for the hidden layer is denoted by $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$ and the bias vector for the output layer is denoted by $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}$.\n",
    "- Let the input matrix be $\\mathbf{X}$, hidden layer activations be $\\mathbf{H}$ and output layer activations be $\\mathbf{O}$. Then:\n",
    "$$ \n",
    "\\underbrace{\\overbrace{\\mathbf{X}}^{\\text{input}}}_{\\mathbb{R}^{n \\times d}} \n",
    "\\underbrace{\\xrightarrow{\\frac{\\mathbf{W}^{(1)}}{\\mathbf{b}^{(1)}}}}_{\\frac{\\mathbb{R}^{d \\times h}}{\\mathbb{R}^{1 \\times h}}}\n",
    "\\underbrace{\\overbrace{\\mathbf{H}}^{\\text{hidden}}}_{\\mathbb{R}^{n \\times h}} \n",
    "\\underbrace{\\xrightarrow{\\frac{\\mathbf{W}^{(2)}}{\\mathbf{b}^{(2)}}}}_{\\frac{\\mathbb{R}^{h \\times q}}{\\mathbb{R}^{1 \\times q}}}\n",
    "\\underbrace{\\overbrace{\\mathbf{O}}^{\\text{output}}}_{\\mathbb{R}^{n \\times q}} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{H} &= \\sigma (\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}) \\\\\n",
    "\\mathbf{O} &= \\mathbf{H} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}\n",
    "\\end{align*}\n",
    "$$ where $\\sigma$ is the activation function, which is applied row-wise (i.e. one example at a time).\n",
    "\n",
    "To build more general models, we can stack multiple hidden layers on top of each other. This is called a deep neural network, e.g. $\\mathbf{H}^{(1)} = \\sigma_1 (\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})$ and $\\mathbf{H}^{(2)} = \\sigma_2 (\\mathbf{H}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)})$ thus yeilding more expressive models.\n",
    "\n",
    "#### Activation functions\n",
    "\n",
    "- Sigmoid function: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$ $$\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$$\n",
    "    - Squashes the input into the range $[0, 1]$\n",
    "    - Suffers from the vanishing gradient problem\n",
    "\n",
    "- Tanh function: $$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$ $$\\tanh'(x) = 1 - \\tanh^2(x)$$\n",
    "    - Squashes the input into the range $[-1, 1]$\n",
    "    - Suffers from the vanishing gradient problem\n",
    "\n",
    "- ReLU function: $$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "    - Does not saturate in the positive region\n",
    "    - It is much more amenable to optimization via gradient descent, because its derivative is a piecewise function (unlike sigmoid/tanh which have a very small derivative in the saturating region)\n",
    "    - Computationally efficient\n",
    "    - Converges much faster than sigmoid/tanh in practice\n",
    "    - Suffers from the dying ReLU problem\n",
    "\n",
    "\n",
    "<img alt=\"picture 3\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/34c396cb48938c8cec0e03fb25587cdddb9afef23e2f52467ed29912bd04bbaf.png\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "### MLP as classifiers and Universal approximators\n",
    "\n",
    "There are results that show that MLPs are universal approximators, i.e. they can approximate any function. This is true for MLPs with a single hidden layer. However, in practice, we use MLPs with multiple hidden layers as they are more expressive.\n",
    "- We can approximate most functions much more compactly with a deep MLP than a wide MLP.\n",
    "\n",
    "### Issue of Depth and Width\n",
    "\n",
    "- Depth: The number of hidden layers in the network\n",
    "- Width: The number of units in each hidden layer\n",
    "- The number of parameters in a single hidden layer MLP is $(d \\times h + h) + (h \\times q + q)$.\n",
    "- The number of parameters in a deep MLP with $L$ hidden layers is $(d \\times h_1) + (h_1 \\times h_2) + \\dots + (h_{L-1} \\times h_L) + (h_L \\times q) + (h_1 + h_2 + \\dots + h_L + q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
