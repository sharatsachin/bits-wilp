{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing and Analytics\n",
    "\n",
    "## Big Data Systems\n",
    "\n",
    "### Reliable, Scalaable and Maintainable Applications\n",
    "\n",
    "#### Data Intensive Applications\n",
    "- deals with huge amount of data, complex and fast moving data\n",
    "- built with several building blocks (databases, caches, indexes, batch and stream processing systems, etc.)\n",
    "- 3 main concerns: \n",
    "    1. reliability : fault tolerance, fault recovery, monitoring, alerting, etc.\n",
    "        - recover from hardware/software faults, human errors, etc.\n",
    "    2. scalability : horizontal scaling, load balancing, etc.\n",
    "    3. maintainability : operability, simplicity, evolvability, etc.\n",
    "\n",
    "#### Example web analytics pipeline\n",
    "- designing an application to track user visits on a website (schema : user_id, page_id, n_visits, etc.)\n",
    "- portal becomes popular, data volume increases, database writes become a bottleneck\n",
    "    - use intermediate queue to buffer writes (queue will hold messages, database will consume messages)\n",
    "- more traffic, more data, more writes, more database load\n",
    "    - use multiple databases (shard data by user_id, page_id, etc.)\n",
    "- as we move down, we need to handle more and more complexity - shards, queues, more complicated application logic, etc.\n",
    "    - need to handle failures, need to handle load balancing, etc. \n",
    "\n",
    "#### Big Data Systems\n",
    "- handle huge amounts of data, fast moving data, complex data\n",
    "- systems designed with distributed nature in mind, doesn't need to bother about common issues like sharding, replication, etc.\n",
    "- scalability achieved by horizontal scaling - just add new machines, devs need only focus on application logic\n",
    "- 3Vs of Big Data : Volume, Velocity, Variety\n",
    "- examples : \n",
    "    1. data sources : web logs, social media, sensors, etc.\n",
    "    2. data acquisition : Kafka, Flume, Spark Streaming, etc.\n",
    "    3. storage : HDFS, Cassandra, HBase, etc.\n",
    "    4. BI analysis : Spark, Hive, Pig, etc.\n",
    "    5. visualization : Tableau, Zeppelin, PowerBI, etc.\n",
    "- properties: fault tolerance, low latency, scalability, extensibility, maintainability, debuggability, etc.\n",
    "\n",
    "#### Data Model of Big Data Systems\n",
    "- fact based model : data is modeled as facts/events\n",
    "    - graph schema captures relationships between entities in the form of nodes, edges and properties\n",
    "    - nodes represent entities, edges represent relationships, properties represent attributes\n",
    "    <img alt=\"picture 0\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/57bd7d48fad8d4e07f588599101a241bcc50e73ee1bafb1c4c97e5800aa95e1f.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "- fact table : stores facts (events) in the form of rows\n",
    "- other columns in the fact table are foreign key references to dimension tables\n",
    "    <img alt=\"picture 1\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/e4e2363558893501a303520b0841a66bb4b755422ae4cbf17f27196ab334f389.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "#### Big data architecture style\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/5e126d8f3906955d4fa2864535921f0b36e80537bc9f6c72908e9dd31a8a7683.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- big data solutions typically involve one or more of the following types of workload:\n",
    "    - batch processing of big data sources at rest\n",
    "    - real-time processing of big data in motion\n",
    "    - interactive exploration of big data\n",
    "    - predictive analytics and machine learning\n",
    "- components are usually: data sources, data storage, batch processing, real-time message ingestion, stream processing, analytical data store, analysis and reporting, orchestration, etc\n",
    "- benefits : choice in technology, performance through parallelism, scalability, interoperability with existing solutions\n",
    "- challenges : complexity, lack of standards, lack of skills, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Systems\n",
    "\n",
    "### Real time systems, stream processing\n",
    "\n",
    "| Feature                      | Real-Time Data               | Near Real-Time Data             | Streaming Data               |\n",
    "| ---------------------------- | ---------------------------- | ------------------------------- | ---------------------------- |\n",
    "| **Latency Measurement**       | Micro to milliseconds        | Extended to seconds             | Constant, always accessible  |\n",
    "| **Use Case**                  | Immediate decision-making    | Operational intelligence, event-driven systems | Continuous acquisition and transmission of data |\n",
    "| **Storage**                   | Short-term, may use in-memory storage | Short to mid-term, may be stored for a relatively longer | Continuous flow, no funneling to long-term storage |\n",
    "| **Dependency on Response Time** | Critical                   | Important but with flexibility | No specific response time     |\n",
    "| **Examples**                  | Financial trading systems, IoT applications, real-time analytics | Monitoring and alerting systems | Astronomical observations, climate observation systems, earth-sensing satellites |\n",
    "| **Processing**                | Immediate processing upon creation or acquisition | Processing with acceptable latency for operational insights | Continuous tracking and analysis of data as it flows |\n",
    "\n",
    "### Stream processing vs batch processing\n",
    "\n",
    "| Feature                      | Batch Processing             | Stream Processing              |\n",
    "| ---------------------------- | ---------------------------- | ------------------------------- |\n",
    "| **Data Processing Model**    | Process data in fixed-size batches | Process data in real-time, small chunks or records |\n",
    "| **Latency**                  | Typically higher, minutes to hours | Very low, milliseconds to seconds |\n",
    "| **Use Case**                 | Suitable for scenarios where data is not time-sensitive, e.g., daily reports | Ideal for time-sensitive applications, e.g., real-time analytics, monitoring |\n",
    "| **Processing Approach**      | Data processed in isolated batches | Continuous and incremental processing of data |\n",
    "| **Storage**                  | Requires storage of large batches before processing | Minimal storage, as data is processed as it arrives |\n",
    "| **Scalability**              | Scales well for large volumes of data but may have higher infrastructure costs | Scales horizontally with ease, cost-effective for high-velocity data |\n",
    "| **Examples**                 | End-of-day financial reports, data warehousing | Real-time analytics, fraud detection, IoT data processing |\n",
    "| **Complexity**               | Typically less complex as it deals with fixed batches | More complex due to real-time nature and handling of streaming data |\n",
    "| **Analyses**                 | Complex analyses, e.g., machine learning, can be performed | Simple analyses, e.g., aggregations, and rolling metrics, can be performed |\n",
    "\n",
    "Why is stream processing important?\n",
    "- data is generated continuously, in huge volumes, at high velocity\n",
    "    - to do batch processing, we need to wait for data to accumulate, stop and restart processing, etc.\n",
    "    - in stream processing, data is processed as it arrives, so we can get real time insights gracefully and naturally\n",
    "    - you can detect patterns, inspect results, look at data from multiple streams, etc.\n",
    "- stream processing is a natural fit for time series and detecting patterns over time\n",
    "- may work with less capable hardware, as data is processed in small chunks, also enables approximate query processing via load shedding\n",
    "- stream processing is a natural fit for event driven architectures\n",
    "\n",
    "### Applications of stream processing\n",
    "- algorithmic trading, stock market analysis, fraud detection, smart patient monitoring, monitoring of IoT devices, production line monitoring, supply chain optimization, intrusion detection and surveillance, smart grids, traffic monitoring, sports analytics, contextual promotions and advertising, computer system and network monitoring, predictive maintenance, geospatial data processing\n",
    "- CEP (complex event processing) : processing of multiple events to infer higher level events, e.g., detecting a fraud transaction by combining multiple events like login, purchase, etc.\n",
    "- Stream analytics : processing of data in motion, e.g., aggregations, filtering, etc.\n",
    "    - usually windows are used to process data in batches, e.g., tumbling window, sliding window, etc.\n",
    "- Materialized views : pre-computed views of data, e.g., aggregations, etc.\n",
    "    - can be used to speed up queries, e.g., in OLAP systems\n",
    "    - can be used to speed up stream processing, e.g., in stream analytics\n",
    "\n",
    "### Streaming data sources\n",
    "- operational monitoring : monitoring of systems, e.g., CPU, memory, network, etc.\n",
    "- web analytics : tracking of user activity on websites, e.g., page views, clicks, etc.\n",
    "- online advertising : call made by modern ad exchanges to bid on ad slots to multiple advertisers in real time\n",
    "- social media : tracking of user activity on social media, e.g., tweets, likes, etc.\n",
    "- IoT : tracking of sensor data, e.g., temperature, humidity, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Streaming Data Architecture\n",
    "\n",
    "### Streaming data architecture\n",
    "- Often we want to deploy our models to make predictions on data 'as it arrives'\n",
    "- operating on streaming data presents a number of challenges\n",
    "    - rebuilding your model to reflect the changing world\n",
    "    - deploying your model that can run quickly and efficiently\n",
    "    - scalability and fault tolerance (systems stuff)\n",
    "- They are layered systems that rely on several loosely coupled systems\n",
    "    - Helps in achieving high availability, managing the system, maintaining the cost under control\n",
    "    - All subsystems / components can reside on individual physical servers or can be co hosted on the single or more than one servers\n",
    "    - Not all components to be present in every system\n",
    "- Components: \n",
    "    1. collection : takes responsibility of collecting the data from the source\n",
    "        - mostly TCP/IP over HTTP, now formats like Avro, Parquet, JSON are used\n",
    "        - happens at the edge, usually application specific, new servers integrated diretly with the streaming system\n",
    "    2. data flow : required intermediate layer that takes responsibility of accepting messages / events from collection layer and providing those messages / events to processing layer\n",
    "        - usually a message queue, e.g., Kafka, RabbitMQ, etc.\n",
    "    3. processing : takes responsibility of processing the messages / events and generating the output\n",
    "        - usually a stream processing framework, e.g., Spark Streaming, Flink, etc.\n",
    "        - relies on distributed processing of data, framework does the most of the heavy lifting of data partitioning, job scheduling, job managing\n",
    "    4. storage : takes responsibility of storing the output of the processing layer\n",
    "        - usually a NoSQL database, e.g., Cassandra, MongoDB, etc.\n",
    "    5. delivery : takes responsibility of delivering the output of the processing layer to the end user\n",
    "        - usually a web / mobile interface or a BI tool, e.g., Tableau, PowerBI, etc.\n",
    "        - can also be a raw file like SVG, PDF, etc. \n",
    "        - monitoring / alerting use cases, feeding data to downstream applications, etc.\n",
    "\n",
    "### Lambda architecture\n",
    "\n",
    "<img alt=\"picture 5\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/72d2beeaeeaa04308b7ccafd493a6e03cdd75c47dcf1c66727826dcceef24a05.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "<img alt=\"picture 6\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/1223d99cb90bcff11fa0fab78b1e50080efaf50792497cff9fcf61a3f5aa7c5c.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 20px\">\n",
    "\n",
    "\n",
    "- proposed by Nathan Marz (2011), combines batch and stream processing, a generic, scalable and fault tolerant data processing architecture\n",
    "- should be linearly scalable, scale out by adding more machines rather than up\n",
    "- critical feature : uses two separate data processing systems to handle different types of data processing workloads\n",
    "    - batch processing system : processes data in large batches and stores the results in a centralized data store, e.g., data warehouse, distributed file system, etc.\n",
    "    - stream processing system : processes data in real time as it arrives and stores the results in a distributed data store, e.g., message queue, NoSQL database, etc.\n",
    "- 4 layers: \n",
    "    1. data ingestion layer : collects and stores raw data from various sources, e.g., log files, sensors, message queues, APIs, etc.\n",
    "        - data is typically ingested in real time and fed to the batch layer and speed layer simultaneously\n",
    "    2. batch layer : responsible for processing historical data in large batches and storing the results in a centralized data store, e.g., data warehouse, distributed file system, etc.\n",
    "        - typically uses a batch processing framework, e.g., Hadoop, Spark, etc.\n",
    "        - designed to handle large volumes of data and provide a complete view of all data\n",
    "    3. speed layer : responsible for processing real time data as it arrives and storing the results in a distributed data store, e.g., message queue, NoSQL database, etc.\n",
    "        - typically uses a stream processing framework, e.g., Flink, Storm, etc.\n",
    "        - designed to handle high volume data streams and provide up to date views of the data\n",
    "    4. serving layer : responsible for serving query results to users in real time\n",
    "        - typically implemented as a layer on top of the batch and stream processing layers\n",
    "        - accessed through a query layer, which allows users to query the data using a query language, e.g., SQL, HiveQL, etc.\n",
    "        - designed to provide fast and reliable access to query results, regardless of whether the data is being accessed from the batch or stream processing layers\n",
    "        - typically uses a distributed data store, e.g., NoSQL database, distributed cache, etc.\n",
    "- advantages : \n",
    "    - scalability : designed to handle large volumes of data and scale horizontally to meet the needs of the business\n",
    "    - fault tolerance : designed to be fault tolerant, with multiple layers and systems working together to ensure that data is processed and stored reliably\n",
    "    - flexibility : designed to handle a wide range of data processing workloads, from historical batch processing to streaming architecture\n",
    "- disadvantages :\n",
    "    - complexity : designed to be complex, uses multiple layers and systems to process and store data\n",
    "    - errors and data discrepancies : with doubled implementations of different workflows, you may run into a problem of different results from batch and stream processing engines\n",
    "    - architecture lock-in : may be super hard to reorganize or migrate existing data stored in the Lambda architecture\n",
    "- use cases :\n",
    "    - handling large volumes of data and providing low-latency query results, e.g., dashboards and reporting\n",
    "    - batch processing tasks, e.g., data cleansing, transformation, and aggregation\n",
    "    - stream processing tasks, e.g., event processing, machine learning models, anomaly detection, and fraud detection\n",
    "    - building data lakes, e.g., centralized repositories that store structured and unstructured data at rest\n",
    "\n",
    "### Kappa architecture\n",
    "\n",
    "<img alt=\"picture 3\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/0908ffe3ce19857b78b9684c8eb50acd9882451d681dfcbc346e517b2c6e14d5.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "<img alt=\"picture 7\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/62326926883a15f1de431103d4df58f52eba4580a527aff23020f4096edbec30.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 20px\">\n",
    "\n",
    "- proposed by Jay Kreps (2014), an evolution of Lambda architecture, uses a single data processing system to handle both batch processing and stream processing workloads\n",
    "- treats everything as streams, allows it to provide a more streamlined and simplified data processing pipeline while still providing fast and reliable access to query results\n",
    "- there is only the speed layer (stream layer)\n",
    "- 2 components : \n",
    "    1. ingestion component : same as Lambda architecture\n",
    "    2. processing component : responsible for processing the data as it arrives and storing the results in a distributed data store, e.g., message queue, NoSQL database, etc.\n",
    "        - typically implemented using a stream processing framework, e.g., Flink, Storm, etc.\n",
    "        - designed to handle high volume data streams and provide fast and reliable access to query results\n",
    "        - there is no seperate serving layer, instead, the stream processing layer is responsible for serving query results to users in real time\n",
    "- advantages :\n",
    "    - simplicity and streamlined pipeline : uses a single data processing system to handle both batch processing and stream processing workloads, which makes it simpler to set up and maintain compared to Lambda architecture\n",
    "    - enables high-throughput big data processing of historical data : although it may feel that it is not designed for these set of problems, Kappa architecture can support these use cases with grace, enabling reprocessing directly from our stream processing job\n",
    "    - ease of migrations and reorganizations : as there is only stream processing pipeline, you can perform migrations and reorganizations with new data streams created from the canonical data store\n",
    "    - tiered storage : tiered storage is a method of storing data in different storage tiers, based on the access patterns and performance requirements of the data\n",
    "        - in Kappa architecture, tiered storage is not a core concept, however, it is possible to use tiered storage in conjunction with Kappa architecture, as a way to optimize storage costs and performance\n",
    "        - for example, businesses may choose to store historical data in a lower-cost fault tolerant distributed storage tier, such as object storage, while storing real-time data in a more performant storage tier, such as a distributed cache or a NoSQL database\n",
    "        - tiered storage Kappa architecture makes it a cost-efficient and elastic data processing technique without the need for a traditional data lake\n",
    "- disadvantages :\n",
    "    - complexity : although Kappa architecture is simpler than Lambda architecture, it can still be complex to set up and maintain, especially for businesses that are not familiar with stream processing frameworks\n",
    "    - costly infrastructure with scalability issues : storing big data in an event streaming platform can be costly, to make it more cost efficient, you may want to use data lake approach from your cloud provider (like AWS S3 or GCP Google Cloud Storage), another common approach for big data architecture is building a 'streaming data lake' with Apache Kafka as a streaming layer and object storage to enable long-term data storage\n",
    "- use cases:\n",
    "    - real-time data processing, e.g., continuous data pipelines, real-time data processing, machine learning models and real-time data analytics, IoT systems, etc.\n",
    "    - building data lakes, e.g., centralized repositories that store structured and unstructured data at rest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Configuration and Coordination in Distributed Systems\n",
    "\n",
    "### Distributed systems\n",
    "- collection of independent computers that appears to its users as a single coherent system\n",
    "- computers coordinate their actions by passing messages over a network (nodes together form a cluster)\n",
    "- nodes need to share metadata and state information to coordinate their actions (e.g., leader election, location of data, etc.)\n",
    "    - this is difficult, leads to incorrect results, inconsistent state, etc.\n",
    "    - needs a system-wide service that correctly and reliably implements distributed configuration and coordination\n",
    "- challenges : \n",
    "    - unreliable network : network partitions, message loss, etc.\n",
    "        - latency issues, bandwidth changes, lost connections, etc.\n",
    "        - split brain problem : loss of connectivity between nodes, nodes may form multiple clusters\n",
    "            - some amount of state is innacessible to some nodes, nodes may diverge in their views of the system\n",
    "            - we should disallow writes to the state until the network partition is resolved, allow one partition to remain functional while degrading the capabilities of the other partition\n",
    "    - unreliable nodes : node failures, node restarts, etc.\n",
    "    - clock synchronization : clocks on different nodes may be out of sync, lead to drifts in time and disordering of events\n",
    "    - consistency in application state : nodes may have different views of the application state (use Paxos, multi-Paxos, Raft algorithms to solve this)\n",
    "        - these are difficult to implement\n",
    "\n",
    "#### Data delivery semantics\n",
    "Data Delivery Semantics, governing data transfer, includes three types:\n",
    "1. At Most Once (AMO): The data is delivered at most once. It may not be delivered at all.\n",
    "    - eg: sending a notification; no guarantee of receipt, and missed notifications are not that important\n",
    "2. At Least Once (ALO): The data is delivered at least once. It may be delivered multiple times.\n",
    "    - eg: email delivery; continuous sending until acknowledged, allowing duplicates\n",
    "3. Exactly Once (EO): The data is delivered exactly once. Deduplication may be required.\n",
    "    - eg: financial transactions; ensures no duplicates but involves more complex mechanisms\n",
    "\n",
    "### Apache Kafka [[Vid1]](https://youtu.be/B5j3uNBH8X4), [[Vid2]](https://youtu.be/jY02MB-sz8I)\n",
    "- [Kafka docs](https://kafka.apache.org/documentation/#gettingStarted)\n",
    "- distributed streaming platform, used for building real time data pipelines and streaming applications\n",
    "- Kafka is a distributed system, it needs to coordinate its nodes to ensure that they are all in agreement\n",
    "- uses ZooKeeper to manage its cluster, store metadata, and perform leader election\n",
    "    - Cluster management : ZooKeeper is used to manage the Kafka cluster, including configuration, topic metadata, broker metadata, etc.\n",
    "    - Failure detection and recovery, leader election\n",
    "    - Store ACLs for authorization\n",
    "\n",
    "<img alt=\"picture 8\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8c6d01dd21e6edaf3e614f69553b607fdcf751a25326b086d27dd704527c5bd9.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Architecture:\n",
    "    1. Producer : responsible for publishing data to Kafka cluster\n",
    "        - Can be written in any language, native: Java, C/C++, Python, Go, .NET, etc\n",
    "    2. Consumer : responsible for subscribing to topics and processing the data published to them\n",
    "        - new inflowing messages are automatically retrieved\n",
    "        - consumer offset, keeps track of the last message read, is stored in special Kafka topic\n",
    "    3. Cluster : collection of nodes that together form a Kafka cluster\n",
    "        - nodes are called brokers, each broker is identified by a unique integer ID\n",
    "- Producers and consumers are completely decoupled:\n",
    "    - slow consumers/producers don't affect each other, add more consumers/producers to scale, failures of consumers/producers don't affect system\n",
    "- Topics: streams of related messages in Kafka, is a logical representation, categorizes messages into groups\n",
    "    - developers can define any number of topics\n",
    "    - topics are partitioned, each partition is an ordered, immutable sequence of messages\n",
    "        - partitions are distributed across brokers, each partition is replicated across multiple brokers for fault tolerance\n",
    "    - producers <-> topics are N:N relation, same with consumers <-> topics\n",
    "\n",
    "<img alt=\"picture 9\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8b5680ad6c9e3e97d745b8928d567ee1f36a3817c22b02207b94a700faf96dae.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Kafka data record : header, key, value,  timestamp\n",
    "    - header : contains metadata about the record, e.g., topic, partition, offset, etc.\n",
    "    - key : optional, used for partitioning, if key is null, messages are round-robin distributed to partitions\n",
    "    - value : actual data, can be any format, e.g., JSON, Avro, etc.\n",
    "    - timestamp\n",
    "- Broker replication:\n",
    "    - each partition has one broker acting as leader and multiple brokers acting as followers\n",
    "    - leader handles all read and write requests for the partition, followers replicate the leader\n",
    "    - if leader fails, one of the followers is elected as the new leader\n",
    "    - if a follower fails, it is removed from the ISR (in-sync replica) and replaced by a new follower\n",
    "- Load Balancing and Semantic Partitioning\n",
    "    - Producers use a partitioning strategy (defined by producer) to determine which partition to publish messages to\n",
    "        - default strategy : hash(key) % n_partitions\n",
    "            - messages with the same key are always published to the same partition\n",
    "        - no key : round-robin\n",
    "        - custom partitioner is also allowed\n",
    "- Estimating number of partitions:\n",
    "    - $N = \\max(\\frac{T_t}{T_p}, \\frac{T_t}{T_c})$\n",
    "        - $T_t$ : throughput of the system (processing speed of the slowest component)\n",
    "        - $T_p$ : throughput of producer writing onto single partition\n",
    "        - $T_c$ : throughput of a consumer reading from single partition\n",
    "\n",
    "Apache Kafka Commands:\n",
    "1. start kafka server : `bin/kafka-server-start.sh config/server.properties`\n",
    "2. create a topic : `bin/kafka-topics.sh --create --topic <topic-name> --bootstrap-server <bootstrap-server> --partitions <num-partitions> --replication-factor <replication-factor>`\n",
    "3. list topics : `bin/kafka-topics.sh --list --bootstrap-server <bootstrap-server>`\n",
    "4. produce messages : `bin/kafka-console-producer.sh --topic <topic-name> --bootstrap-server <bootstrap-server>`\n",
    "5. consume messages : `bin/kafka-console-consumer.sh --topic <topic-name> --bootstrap-server <bootstrap-server> --from-beginning`\n",
    "6. describe consumer groups : `bin/kafka-consumer-groups.sh --describe --group <group-id> --bootstrap-server <bootstrap-server>`\n",
    "7. check offsets : `bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic <topic-name> --group <group-id> --bootstrap-server <bootstrap-server>`        \n",
    "\n",
    "Apache Kafka APIs:\n",
    "1. Producer API : used to publish messages to Kafka topics\n",
    "2. Consumer API : used to subscribe to Kafka topics and process messages\n",
    "3. Streams API : used to process streams of data and produce output streams, effectively transforming input streams into output streams\n",
    "    - highly scalable and fault-tolerant, can be used to build real-time applications, stateful and stateless processing, etc.\n",
    "    - event time processing, windowing, joins, aggregations, etc.\n",
    "    - does not take much configuration, can be run on a single machine or a cluster\n",
    "    - employs one record at a time processing, to achieve low latency and high throughput\n",
    "4. Connector API : used to connect Kafka topics to external systems, e.g., databases, message queues, etc.\n",
    "\n",
    "Stream processing topologies:\n",
    "- A stream is an unbounded, continuously updating data set, ordered, replayable, fault-tolerant sequence of immutable data records\n",
    "- A stream processing application defines its computational logic through one or more processor topologies, a graph of stream processors connected by streams\n",
    "- A stream processor is a node in the processor topology, represents a processing step to transform data in streams\n",
    "    - receives one input record at a time from its upstream processors\n",
    "    - applies its operation to it\n",
    "    - may subsequently produce one or more output records to its downstream processors\n",
    "- Kafka stream processors:\n",
    "    - Source Processor : does not have any upstream processors, produces an input stream to its topology from one or multiple Kafka topics\n",
    "    - Sink Processor : does not have downstream processors, sends any received records from its upstream processors to a specified Kafka topic\n",
    "- Processing in Kafka Streams:\n",
    "    - High-level Kafka Streams DSL : provides ready to use methods with functional style\n",
    "        - has already implemented methods ready to use\n",
    "        - composed of two main abstractions: KStream and KTable or GlobalKTable\n",
    "            - KStream : abstraction of record stream, provides many functional ways to manipulate stream data\n",
    "                - map, mapValue, flatMap, flatMapValues, filter, are some of the methods\n",
    "            - KTable or GlobalKTable : abstraction of a changelog stream, every data record is considered an Insert or Update\n",
    "                - existing row with the same key will be overwritten\n",
    "    - Low-level Processor API : provides flexibility to implement processing logic according to need\n",
    "        - extends AbstractProcessor, overrides process method, called once for every key-value pair\n",
    "        - provides client to access stream data, perform business logic, send result as downstream data\n",
    "        - trade-off is lines of code needed for specific scenarios\n",
    "\n",
    "### Apache Zookeeper [[Vid]](https://youtu.be/gZj16chk0Ss)\n",
    "- [ZooKeeper overview](https://zookeeper.apache.org/doc/r3.9.1/zookeeperOver.html)\n",
    "- distributed coordination service, used for building distributed systems, used with Hadoop, HBase, Kafka, etc\n",
    "- services:\n",
    "    - naming : provides a hierarchical namespace for nodes in cluster\n",
    "    - configuration management : stores and manages configuration information for systems\n",
    "    - cluster management : manages the membership of nodes in a cluster\n",
    "    - leader election : elects a leader among distributed nodes\n",
    "    - locking and synchronization : provides primitives for synchronization and coordination between distributed processes\n",
    "- benefits : synchronization, serialization (application runs consistently), atomicity, reliability, etc.\n",
    "- architecture:\n",
    "    - ensemble : a group of ZooKeeper servers that collectively manage the service (3 minimum)\n",
    "        - leaders are elected among the ensemble, followers replicate the leader\n",
    "    - quorum : majority of servers must agree on an operation for it to be committed\n",
    "    - client : connects to any server in the ensemble, sends heartbeat to server, if server doesn't respond, client connects to another server\n",
    "    - znode : the basic data structure in ZooKeeper, analogous to a file system node\n",
    "        - znodes are organized in a hierarchical tree-like structure\n",
    "        - each znode contains data and metadata (version number, ACL, etc.)\n",
    "        - znodes can be ephemeral (deleted when client disconnects) or persistent (deleted when explicitly deleted) or sequential (name of znode is appended with a monotonically increasing counter)\n",
    "\n",
    "<img alt=\"picture 10\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/2574be9ee8aa76e507988eb8754c5d9ec658d83ada1c13e2050ff13d69f3821e.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- operations : create, read, update, delete, watch\n",
    "    - watches : allows clients to be notified of changes to a znode\n",
    "- ACLs : access control lists, used to control access to znodes\n",
    "    - each znode has an ACL that specifies the permissions for that znode\n",
    "    - permissions : read, write, create, delete, admin\n",
    "    - ACLs are stored in ZooKeeper and managed by ZooKeeper itself\n",
    "- deployment:\n",
    "    - ensemble size : typically odd number (3, 5, 7) for fault tolerance\n",
    "    - network configuration : low-latency, reliable network required between ZooKeeper servers\n",
    "    - client deployment : clients connect to any server in the ensemble\n",
    "- split brain problem : loss of connectivity between nodes, nodes may form multiple clusters\n",
    "    - some amount of state is innacessible to some nodes, nodes may diverge in their views of the system\n",
    "    - we should disallow writes to the state until the network partition is resolved, allow one partition to remain functional while degrading the capabilities of the other partition\n",
    "\n",
    "Apache ZooKeeper Commands:\n",
    "1. start zookeeper : `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "2. connect to zookeeper cli : `bin/zookeeper-shell.sh <zookeeper-host>:<zookeeper-port>`\n",
    "3. create a znode : `create /<path> <data>`\n",
    "4. list znode contents : `ls /<path>`\n",
    "5. get znode data : `get /<path>`\n",
    "6. set znode data : `set /<path> <new-data>`\n",
    "7. delete znode : `delete /<path>`\n",
    "\n",
    "### Pub-sub messaging workflow\n",
    "- producers send messages to a topic at regular intervals\n",
    "- Kafka broker stores all messages in the partitions configured for that particular topic\n",
    "    - ensures the messages are equally shared between partitions\n",
    "- consumer subscribes to a specific topic, Kafka will provide the current offset of the topic to the consumer and also saves the offset in the Zookeeper ensemble\n",
    "- consumer will request Kafka at regular intervals for new messages\n",
    "- once Kafka receives messages from producers, it forwards these messages to the consumers\n",
    "- consumer will receive the message and process it\n",
    "- once the messages are processed, consumer will send an acknowledgement to the Kafka broker\n",
    "- once Kafka receives an acknowledgement, it changes offset to new value and updates it in Zookeeper\n",
    "    - since offsets are maintained in Zookeeper, consumer can read next message correctly even during server outrages\n",
    "- this above flow will repeat until consumer stops the request\n",
    "- consumer has option to rewind/skip to desired offset of a topic at any time and read all subsequent messages\n",
    "\n",
    "### Message queues vs Pub-sub systems\n",
    "- message queues : messages are stored in a queue, each message is delivered to exactly one consumer\n",
    "    - consumers can be grouped into consumer groups, each message is delivered to one consumer in each consumer group\n",
    "    - consumers can acknowledge messages, once a message is acknowledged, it is deleted from the queue\n",
    "    - if a consumer fails to acknowledge a message before a timeout, the message is redelivered to another consumer\n",
    "    - examples : RabbitMQ, ActiveMQ, etc\n",
    "- pub-sub systems : messages are published to a topic, each message is delivered to all consumers subscribed to that topic\n",
    "    - message will only be deleted if it's consumed by all subscribers to the category\n",
    "    - examples : Kafka, Google Cloud Pub/Sub, etc have retention policy that ensures messages stay in the queue for a specified amount of time, even after they are consumed by all subscribers\n",
    "\n",
    "### Streaming Architectures\n",
    "\n",
    "Random streaming architectures I found\n",
    "\n",
    "[Link1](https://www.simform.com/blog/stream-processing/)\n",
    "<img alt=\"picture 11\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/132f86c07b23b9b8e108558f9e27bf6159a9c86ca033cfa15341b247cb2ebe62.png\" width=\"700\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "[Link2](https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/what-is-a-modern-streaming-data-architecture.html)\n",
    "<img alt=\"picture 12\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/4318bba6aca69a8f6dce99565fb3d697ee78c24378ddc9c8ff3d8b1dba700aa4.png\" width=\"700\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "[Link3](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/data/stream-processing-stream-analytics)\n",
    "<img alt=\"picture 13\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/cd7def52aa38a303f47ef8c8f15a484d61f92de42f90e3e4b8a83f7ce2049750.png\" width=\"700\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing Framework Features\n",
    "\n",
    "State Management\n",
    "- where to maintain state in stream processing systems?\n",
    "    - in-memory : for example, sum(sales) for last one hour\n",
    "        - data will be flushed out once one hour window expires\n",
    "        - potential of losing all the data if the stream processor node fails\n",
    "    - persistent storage : for example, needs to join data from two different streams and data being produced at different rate\n",
    "\n",
    "Fault Tolerance\n",
    "- point of failures in stream processing data flow\n",
    "    - incoming stream data, network carrying data, stream processor, connection to output sink, output destination, streaming manager, application driver\n",
    "- in case of data loss, different approaches can be used\n",
    "    - replication and coordination : replicate the state of computation on multiple nodes, in case of failures, streaming manager interacts with replicas\n",
    "        - can predefine number of simultaneous failures, K-fault tolerant where k is number of simultaneous failures\n",
    "    - state machine approach : streaming job is replicated on multiple nodes, replicas are coordinated by sending same input in same order to all\n",
    "        - allows for quick failover, resulting into little disruption\n",
    "    - rollback recovery approach : stream processor periodically packages the state of computation into checkpoint, copies checkpoint to different nodes\n",
    "        - in case of failure, stream manager fetches the computation from last saved checkpoint and reschedules it\n",
    "    \n",
    "### Timing Concepts\n",
    "- Event Time vs Stream Time\n",
    "    - event time is time at which the event occurs\n",
    "    - stream time is time at which event enters the streaming system\n",
    "    - stream time will lag a bit due to several reasons\n",
    "    - for example, if we are monitoring the traffic on the road and a person jumps the signal, then\n",
    "        - event time is the time at which the person jumped the signal\n",
    "        - stream time is the time at which the event information reached to streaming platform for processing like snapping the fine on the user and then sending him the alert on the mobile\n",
    "- Time Skew\n",
    "    - stream time is often lagging behind the event time\n",
    "    - the difference between these two times is \"time skew\"\n",
    "    - variance can be significant sometimes due to various issues\n",
    "    - applications needs to take into consideration whether they depend on the event time or stream time\n",
    "\n",
    "### Windowing\n",
    "- Windowing is a technique used in stream processing to divide the stream of data into finite segments\n",
    "- policies:\n",
    "    - trigger policy : rules for determining when the code should be executed\n",
    "    - eviction policy : rules for determining when the data should be evicted from the window\n",
    "- types:\n",
    "    - sliding window : window slides over the data stream\n",
    "        - window length specifies the eviction policy (time duration for which data is available for processing, eg. if 2 seconds, data older than 2 seconds will be evicted)\n",
    "        - sliding interval specifies the trigger policy (time duration after which code will be triggered eg. if 1 second, code will be executed every second)\n",
    "    - tumbling window : window is fixed and data is processed in chunks\n",
    "        - trigger and eviction policies are executed when window is full\n",
    "        - no time constraints for window to be filled\n",
    "        - types:\n",
    "            - count based tumbling : trigger and eviction policy set to 2 seconds, when 2 events are accumulated in window, trigger will be fired, code will be executed, window will be drained\n",
    "            - temporal based tumbling : based on time, if the eviction and trigger policy set to 2 seconds, when 2 seconds are over, trigger will be fired, code will be executed, window will be drained\n",
    "\n",
    "### Stream Joins\n",
    "- stream-stream join : event streams from two or more sources are joined in real time\n",
    "    - sources can be same or different\n",
    "    - for example, location coordinates of mobile devices of users is received in real time and can be joined with the traffic updates available in real time around those locations to suggest an alternative route in case of congestion\n",
    "    - at least one stream must be bounded (have a window attached) to perform join\n",
    "        ```sql\n",
    "        Select x, y \n",
    "        From S1 as s1 join S2 as s2 \n",
    "        on s1.id=s2.id \n",
    "        insert into JoinedStream\n",
    "        ```\n",
    "    - one window join : one stream is bounded by a window, events retained in the window are matched against events coming in the second stream\n",
    "        - only events in the second stream will trigger an output\n",
    "    - two window join : a two-window query will retain events coming from both the streams in the window\n",
    "        - a new event coming in will either trigger a match and an output\n",
    "- stream-table join : streaming event and data stored in persistent storage like database is joined together\n",
    "    - usually information from the database is fetched to add more value to the streaming data\n",
    "    - for example, users location is tracked near a mall in real time along with identity of user and sourced as stream, profile data of user is fetched from database, his/her interests are observed, based on the interests and outlets located in the malls, discount coupons can be sent on the users devices\n",
    "    - lookup in database can be time consuming in case of too many records of users are available, local copy of users data can be loaded in stream processor\n",
    "\n",
    "### Approaches to Stream Processing\n",
    "Two approaches:\n",
    "1. Microbatching (Bulk Synchronous Processing)\n",
    "    - gist of Bulk Synchronous Processing (BSP)\n",
    "        - split distribution of asynchronous work\n",
    "        - each executor receives the chunk(s) of work and works separately until the second element comes in\n",
    "        - a particular resource is tasked with keeping track of progress of computation\n",
    "        - between these scheduled steps, all executors on the cluster are doing the same thing\n",
    "    - synchronous barrier, coming in at fixed intervals\n",
    "        - frequency at which further rounds of processing are scheduled id dictated by time period\n",
    "        - implemented at small, fixed intervals that better approximate the real time motion of data processing\n",
    "        - function-passing style – asynchronously pass safe functions to data\n",
    "        - functions are passed around the scheduling process that describe processing to be done on data\n",
    "        - data is already on various executors, delivered directly to resources\n",
    "2. One-Record-at-a-Time Processing\n",
    "    - works based on pipelining\n",
    "    - analyses the whole computation as described by user-specified functions and deploys it as a pipeline using the resources of the cluster\n",
    "    - flow the data through various resources, following prescribed pipeline\n",
    "    - each step of computation is materialized at some place in cluster at any given point\n",
    "    - Apache Filnk, Storm and IBM Streams follows this style\n",
    "    - latency\n",
    "        - for microbatching system is batch interval + processing time\n",
    "        - for one-record-at-a-time system is only processing time as it react as it meets the event of interest\n",
    "\n",
    "Bringing Microbatch and One-record-at-time Together\n",
    "- marriage is implemented in systems like Flink and Naiad\n",
    "- Spark Structured Streaming\n",
    "    - backed up by microbatching but does not reveal the batch interval at API level\n",
    "    - allows for processing that is independent of fixed batch interval\n",
    "    - execution model mixes microbatching with a dynamic batch interval\n",
    "    - trigger the execution of next batch as early as possible\n",
    "    - new batch should be started as soon as the previous one has been processed\n",
    "\n",
    "Trade-Offs\n",
    "- despite high latency, microbathcing offers significant advantages\n",
    "    - able to adapt at the synchronization barrier boundaries\n",
    "        - might represent the task of recovering from failure\n",
    "        - give opportunity to add or remove executor nodes, possibility to grow or shrink resources depending upon cluster load\n",
    "    - have easier time providing strong consistency\n",
    "        - batch determinations – beginning and end of batch of data – are deterministic and recorded\n",
    "        - any kind of computation can be redone and produce same results the second time\n",
    "    - perform efficient optimizations\n",
    "        - data available as a set can provide ideas on the way to compute on data\n",
    "        - allows an efficient way of specifying programming both batch processing and streaming data\n",
    "        - even for mere instances, looks like data at rest\n",
    "\n",
    "### Streaming in Apache Spark\n",
    "- 2 APIs : Spark Streaming and Structured Streaming\n",
    "- Spark Streaming\n",
    "    - first streaming engine based on distributed capabilities of Spark\n",
    "    - based on simple but powerful premise\n",
    "        - apply Spark’s distributed computing capabilities to stream processing by transforming a continuous stream of data into discrete data collections\n",
    "        - micro batching\n",
    "    - uses same functional programming paradigm as Spark core but introduces a new abstraction\n",
    "    - discretized streams – exposes a programming model to operate on data in stream\n",
    "    - Spark RDD abstraction permits creation of programs that treat distributed data as a collection\n",
    "        - allows applying data processing logic in form of transformation of distributed dataset\n",
    "    - main task is to take data from stream, package it down into small batches and provide them to Spark for further processing\n",
    "    - DSream abstraction\n",
    "        - Spark streaming relies on the much more fundamental Spark abstraction of RDD\n",
    "        - introduces a new concept : The Discretized Stream or DStream\n",
    "        - DStream represents a stream in terms of discrete block of data that in turn are represented as RDDs over time\n",
    "        - primarily an execution model that when combined with functional programming model, provides a complete framework to develop and execute streaming applications\n",
    "        - DStreams as a Programming model\n",
    "            - provides functional programming APIs consistent with RDD APIs and augmented with stream specific functions to deal with\n",
    "                - aggregations\n",
    "                - time based operations\n",
    "                - stateful computations\n",
    "            - in Spark Streaming, we\n",
    "                - consume a stream by creating Dstream form one of the native implementations or the connectors available\n",
    "                    - from SocketInputStream or Kafka / Twitter / Kinesis connector\n",
    "                - implement application logic using functions provided by Dstream API\n",
    "                    - such as counting the elements or grouping the elements\n",
    "                - uses transformations – no execution happens unless output operation is not called\n",
    "                - execute output operations to yield out the results\n",
    "            - DStream programming model consists of functional composition of transformations over the stream payload, materialized by one or more output operations and recurrently executed by Spark engine\n",
    "        - DStreams as an Execution model\n",
    "            - in programming model, its descried how data is transformed from original form to intended result as a series of lazy evaluations\n",
    "            - Spark streaming engine is responsible for taking that chain of transformations and turning it into an actual execution plan\n",
    "                - happens by receiving data from input stream(s), collecting that into batches and feeding to spark in timely manner\n",
    "            - the measure of time to wait for data in batch interval\n",
    "                - central unit of time\n",
    "                - short amount of time, ranging from 200ms to 1 min, depending app’s latency requirement\n",
    "            - at each batch interval, the data corresponding to the previous interval is sent to Spark for processing while new data is received\n",
    "                - process is repeated as long as Streaming job is active and healthy\n",
    "            - DStream model dictates that a continuous stream of data is discretized into micro batches using a regular time interval\n",
    "- Structured Streaming\n",
    "    - stream processor built on top of Spark SQL abstractions\n",
    "    - extends Dataset and DataFrame APIs with streaming capabilities\n",
    "    - adopts schema oriented transformation model – structured part in name\n",
    "    - inherits optimizations implemented in Spark SQL\n",
    "    - introduced in 2017 with Spark 2.2 release\n",
    "    - still evolving fast with each new version of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketching Algorithms\n",
    "- Sketching algorithms are used to summarize data streams in a small amount of space \n",
    "- Sketching algorithms are used to estimate various statistics of the data stream, such as count, sum, average, variance, etc.\n",
    "- Examples of sketching algorithms include Count-Min Sketch, HyperLogLog, and Bloom Filter\n",
    "\n",
    "#### Bloom Filter\n",
    "- Bloom filter is a probabilistic data structure used to test whether an element is a member of a set Link : https://llimllib.github.io/bloomfilter-tutorial/\n",
    "- Designed to tell you, rapidly and memory-efficiently, whether an element is present in a set - in other words, it tells you either \"possibly in set\" or \"definitely not in set\". Not - to say \"definitely in set\".\n",
    "- Can't remove elements\n",
    "- hash functions used in a Bloom filter should be independent and uniformly distributed\n",
    "    - like murmur, xxhash, fnv, HashMix, etc.\n",
    "- In a Bloom filter with k hashes, m bits in the filter, and n elements that have been inserted:\n",
    "    - The probability of a false positive is $$p = \\left(1 - \\left[1 - \\frac{1}{m}\\right]^{kn}\\right)^k \\approx (1 - e^{-kn/m})^k$$\n",
    "- So, to choose the size of a bloom filter, we:\n",
    "    - Choose a ballpark value for $n$\n",
    "    - Choose a value for $m$\n",
    "    - Calculate the optimal value of $k$\n",
    "    - Calculate the error rate for our chosen values of $m$ and $k$, and if it's too high, increase $m$ and recalculate $k$.\n",
    "- Both insertion and lookup are $O(k)$\n",
    "\n",
    "#### HyperLogLog\n",
    "- HyperLogLog is a probabilistic data structure used to estimate the cardinality of a set\n",
    "- HyperLogLog is used to estimate the number of distinct elements in a multiset\n",
    "- HyperLogLog is a memory-efficient way to estimate the number of distinct elements in a set\n",
    "- HyperLogLog uses a hash function to map elements to a fixed-size array of bits\n",
    "- HyperLogLog uses a technique called \"log-log\" counting to estimate the number of distinct elements in a set\n",
    "- Algorithm:\n",
    "    - Identify a unique ID for the data value\n",
    "    - Pass the ID through a hash function, it will result into a hashed value\n",
    "    - Hashed value is converted into a binary representation\n",
    "    - Need to determine the place which needs to be updated and with what value.\n",
    "        - Take the least 6 significant digits of binary number and convert it to a decimal value. That gives you the position where value needs to be updated\n",
    "        - Count the number of leading zeros in the binary number, add one to it and use that number as a value to be stored in the position identified earlier\n",
    "    - Repeat the process for all the data values\n",
    "    - To estimate the cardinality, use the harmonic mean of the values stored in the array\n",
    "\n",
    "#### Count-Min Sketch\n",
    "- is a probabilistic data structure used to estimate the frequency of elements in a data stream\n",
    "- uses a hash function to map elements to a fixed-size array of counters\n",
    "- create a table:\n",
    "    - number of rows = number of different values given by hash function\n",
    "    - number of columns = number of hash functions (each representing a different counter)\n",
    "- uses multiple hash functions to map elements to multiple counters\n",
    "    - loop through each element in the stream, hash it using multiple hash functions, increment the counter at the hashed index\n",
    "- uses the minimum value of the counters to estimate the frequency of that element in the stream\n",
    "\n",
    "#### Reservoir Sampling\n",
    "- Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number\n",
    "- Algorithm:\n",
    "    - Initialize an array of size k to store the sample\n",
    "    - Fill the array with the first k elements from the stream\n",
    "    - For each subsequent element in the stream, generate a random number between 0 and the number of elements seen so far\n",
    "    - If the random number is less than k:\n",
    "        - Insert the element into the sample\n",
    "        - Remove a random element from the sample\n",
    "\n",
    "#### Hot list detection\n",
    "- Hot list detection is a technique used to identify the most frequently occurring items in a data stream\n",
    "- Algorithm:\n",
    "    - input $S$ : a sequence of examples\n",
    "    - for each example $x$ in $S$:\n",
    "        - if $x$ is in the hot list:\n",
    "            - increment the count of $x$\n",
    "        - else \n",
    "            - if there is a element $y$ in the hot list with count 0:\n",
    "                - replace $y$ with $x$\n",
    "                - increment the count of $x$\n",
    "            - else\n",
    "                - decrement the count of each element in the hot list\n",
    "\n",
    "#### Decaying Window\n",
    "- Decaying window is a technique used to give more weight to recent data in a data stream\n",
    "- Here, the weight of the data decreases exponentially as it gets older\n",
    "- Idea:\n",
    "    - Attach weights to elements in sliding window\n",
    "    - Recent elements receive higher weights, with the older elements receive decaying weights\n",
    "    - For a new element:\n",
    "        - First reduce the weight of all the existing elements by a constant factor\n",
    "        - Assign the new element with a specific weight\n",
    "        - The aggregate sum of the decaying exponential weights can be calculated using the following formula\n",
    "    - Let stream is $x_1, x_2, x_3, \\ldots$ and taking sum of stream as follows:\n",
    "        - $S_t = \\sum_{i=0}^{t-1} \\lambda^{i} \\cdot x_i$\n",
    "        - where $\\lambda$ is the decay factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
