{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing and Analytics\n",
    "\n",
    "## Big Data Systems\n",
    "\n",
    "### Reliable, Scalaable and Maintainable Applications\n",
    "\n",
    "#### Data Intensive Applications\n",
    "- deals with huge amount of data, complex and fast moving data\n",
    "- built with several building blocks (databases, caches, indexes, batch and stream processing systems, etc.)\n",
    "- 3 main concerns: \n",
    "    1. reliability : fault tolerance, fault recovery, monitoring, alerting, etc.\n",
    "        - recover from hardware/software faults, human errors, etc.\n",
    "    2. scalability : horizontal scaling, load balancing, etc.\n",
    "    3. maintainability : operability, simplicity, evolvability, etc.\n",
    "\n",
    "#### Example web analytics pipeline\n",
    "- designing an application to track user visits on a website (schema : user_id, page_id, n_visits, etc.)\n",
    "- portal becomes popular, data volume increases, database writes become a bottleneck\n",
    "    - use intermediate queue to buffer writes (queue will hold messages, database will consume messages)\n",
    "- more traffic, more data, more writes, more database load\n",
    "    - use multiple databases (shard data by user_id, page_id, etc.)\n",
    "- as we move down, we need to handle more and more complexity - shards, queues, more complicated application logic, etc.\n",
    "    - need to handle failures, need to handle load balancing, etc. \n",
    "\n",
    "#### Big Data Systems\n",
    "- handle huge amounts of data, fast moving data, complex data\n",
    "- systems designed with distributed nature in mind, doesn't need to bother about common issues like sharding, replication, etc.\n",
    "- scalability achieved by horizontal scaling - just add new machines, devs need only focus on application logic\n",
    "- 3Vs of Big Data : Volume, Velocity, Variety\n",
    "- examples : \n",
    "    1. data sources : web logs, social media, sensors, etc.\n",
    "    2. data acquisition : Kafka, Flume, Spark Streaming, etc.\n",
    "    3. storage : HDFS, Cassandra, HBase, etc.\n",
    "    4. BI analysis : Spark, Hive, Pig, etc.\n",
    "    5. visualization : Tableau, Zeppelin, PowerBI, etc.\n",
    "- properties: fault tolerance, low latency, scalability, extensibility, maintainability, debuggability, etc.\n",
    "\n",
    "#### Data Model of Big Data Systems\n",
    "- fact based model : data is modeled as facts/events\n",
    "    - graph schema captures relationships between entities in the form of nodes, edges and properties\n",
    "    - nodes represent entities, edges represent relationships, properties represent attributes\n",
    "    <img alt=\"picture 0\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/57bd7d48fad8d4e07f588599101a241bcc50e73ee1bafb1c4c97e5800aa95e1f.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "- fact table : stores facts (events) in the form of rows\n",
    "- other columns in the fact table are foreign key references to dimension tables\n",
    "    <img alt=\"picture 1\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/e4e2363558893501a303520b0841a66bb4b755422ae4cbf17f27196ab334f389.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "#### Big data architecture style\n",
    "\n",
    "<img alt=\"picture 2\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/5e126d8f3906955d4fa2864535921f0b36e80537bc9f6c72908e9dd31a8a7683.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- big data solutions typically involve one or more of the following types of workload:\n",
    "    - batch processing of big data sources at rest\n",
    "    - real-time processing of big data in motion\n",
    "    - interactive exploration of big data\n",
    "    - predictive analytics and machine learning\n",
    "- components are usually: data sources, data storage, batch processing, real-time message ingestion, stream processing, analytical data store, analysis and reporting, orchestration, etc\n",
    "- benefits : choice in technology, performance through parallelism, scalability, interoperability with existing solutions\n",
    "- challenges : complexity, lack of standards, lack of skills, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Systems\n",
    "\n",
    "### Real time systems, stream processing\n",
    "\n",
    "| Feature                      | Real-Time Data               | Near Real-Time Data             | Streaming Data               |\n",
    "| ---------------------------- | ---------------------------- | ------------------------------- | ---------------------------- |\n",
    "| **Latency Measurement**       | Micro to milliseconds        | Extended to seconds             | Constant, always accessible  |\n",
    "| **Use Case**                  | Immediate decision-making    | Operational intelligence, event-driven systems | Continuous acquisition and transmission of data |\n",
    "| **Storage**                   | Short-term, may use in-memory storage | Short to mid-term, may be stored for a relatively longer | Continuous flow, no funneling to long-term storage |\n",
    "| **Dependency on Response Time** | Critical                   | Important but with flexibility | No specific response time     |\n",
    "| **Examples**                  | Financial trading systems, IoT applications, real-time analytics | Monitoring and alerting systems | Astronomical observations, climate observation systems, earth-sensing satellites |\n",
    "| **Processing**                | Immediate processing upon creation or acquisition | Processing with acceptable latency for operational insights | Continuous tracking and analysis of data as it flows |\n",
    "\n",
    "### Stream processing vs batch processing\n",
    "\n",
    "| Feature                      | Batch Processing             | Stream Processing              |\n",
    "| ---------------------------- | ---------------------------- | ------------------------------- |\n",
    "| **Data Processing Model**    | Process data in fixed-size batches | Process data in real-time, small chunks or records |\n",
    "| **Latency**                  | Typically higher, minutes to hours | Very low, milliseconds to seconds |\n",
    "| **Use Case**                 | Suitable for scenarios where data is not time-sensitive, e.g., daily reports | Ideal for time-sensitive applications, e.g., real-time analytics, monitoring |\n",
    "| **Processing Approach**      | Data processed in isolated batches | Continuous and incremental processing of data |\n",
    "| **Storage**                  | Requires storage of large batches before processing | Minimal storage, as data is processed as it arrives |\n",
    "| **Scalability**              | Scales well for large volumes of data but may have higher infrastructure costs | Scales horizontally with ease, cost-effective for high-velocity data |\n",
    "| **Examples**                 | End-of-day financial reports, data warehousing | Real-time analytics, fraud detection, IoT data processing |\n",
    "| **Complexity**               | Typically less complex as it deals with fixed batches | More complex due to real-time nature and handling of streaming data |\n",
    "| **Analyses**                 | Complex analyses, e.g., machine learning, can be performed | Simple analyses, e.g., aggregations, and rolling metrics, can be performed |\n",
    "\n",
    "Why is stream processing important?\n",
    "- data is generated continuously, in huge volumes, at high velocity\n",
    "    - to do batch processing, we need to wait for data to accumulate, stop and restart processing, etc.\n",
    "    - in stream processing, data is processed as it arrives, so we can get real time insights gracefully and naturally\n",
    "    - you can detect patterns, inspect results, look at data from multiple streams, etc.\n",
    "- stream processing is a natural fit for time series and detecting patterns over time\n",
    "- may work with less capable hardware, as data is processed in small chunks, also enables approximate query processing via load shedding\n",
    "- stream processing is a natural fit for event driven architectures\n",
    "\n",
    "### Applications of stream processing\n",
    "- algorithmic trading, stock market analysis, fraud detection, smart patient monitoring, monitoring of IoT devices, production line monitoring, supply chain optimization, intrusion detection and surveillance, smart grids, traffic monitoring, sports analytics, contextual promotions and advertising, computer system and network monitoring, predictive maintenance, geospatial data processing\n",
    "- CEP (complex event processing) : processing of multiple events to infer higher level events, e.g., detecting a fraud transaction by combining multiple events like login, purchase, etc.\n",
    "- Stream analytics : processing of data in motion, e.g., aggregations, filtering, etc.\n",
    "    - usually windows are used to process data in batches, e.g., tumbling window, sliding window, etc.\n",
    "- Materialized views : pre-computed views of data, e.g., aggregations, etc.\n",
    "    - can be used to speed up queries, e.g., in OLAP systems\n",
    "    - can be used to speed up stream processing, e.g., in stream analytics\n",
    "\n",
    "### Streaming data sources\n",
    "- operational monitoring : monitoring of systems, e.g., CPU, memory, network, etc.\n",
    "- web analytics : tracking of user activity on websites, e.g., page views, clicks, etc.\n",
    "- online advertising : call made by modern ad exchanges to bid on ad slots to multiple advertisers in real time\n",
    "- social media : tracking of user activity on social media, e.g., tweets, likes, etc.\n",
    "- IoT : tracking of sensor data, e.g., temperature, humidity, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Streaming Data Architecture\n",
    "\n",
    "### Streaming data architecture\n",
    "- Often we want to deploy our models to make predictions on data 'as it arrives'\n",
    "- operating on streaming data presents a number of challenges\n",
    "    - rebuilding your model to reflect the changing world\n",
    "    - deploying your model that can run quickly and efficiently\n",
    "    - scalability and fault tolerance (systems stuff)\n",
    "- They are layered systems that rely on several loosely coupled systems\n",
    "    - Helps in achieving high availability, managing the system, maintaining the cost under control\n",
    "    - All subsystems / components can reside on individual physical servers or can be co hosted on the single or more than one servers\n",
    "    - Not all components to be present in every system\n",
    "- Components: \n",
    "    1. collection : takes responsibility of collecting the data from the source\n",
    "        - mostly TCP/IP over HTTP, now formats like Avro, Parquet, JSON are used\n",
    "        - happens at the edge, usually application specific, new servers integrated diretly with the streaming system\n",
    "    2. data flow : required intermediate layer that takes responsibility of accepting messages / events from collection layer and providing those messages / events to processing layer\n",
    "        - usually a message queue, e.g., Kafka, RabbitMQ, etc.\n",
    "    3. processing : takes responsibility of processing the messages / events and generating the output\n",
    "        - usually a stream processing framework, e.g., Spark Streaming, Flink, etc.\n",
    "        - relies on distributed processing of data, framework does the most of the heavy lifting of data partitioning, job scheduling, job managing\n",
    "    4. storage : takes responsibility of storing the output of the processing layer\n",
    "        - usually a NoSQL database, e.g., Cassandra, MongoDB, etc.\n",
    "    5. delivery : takes responsibility of delivering the output of the processing layer to the end user\n",
    "        - usually a web / mobile interface or a BI tool, e.g., Tableau, PowerBI, etc.\n",
    "        - can also be a raw file like SVG, PDF, etc. \n",
    "        - monitoring / alerting use cases, feeding data to downstream applications, etc.\n",
    "\n",
    "### Lambda architecture\n",
    "\n",
    "<img alt=\"picture 5\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/72d2beeaeeaa04308b7ccafd493a6e03cdd75c47dcf1c66727826dcceef24a05.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "<img alt=\"picture 6\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/1223d99cb90bcff11fa0fab78b1e50080efaf50792497cff9fcf61a3f5aa7c5c.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 20px\">\n",
    "\n",
    "\n",
    "- proposed by Nathan Marz (2011), combines batch and stream processing, a generic, scalable and fault tolerant data processing architecture\n",
    "- should be linearly scalable, scale out by adding more machines rather than up\n",
    "- critical feature : uses two separate data processing systems to handle different types of data processing workloads\n",
    "    - batch processing system : processes data in large batches and stores the results in a centralized data store, e.g., data warehouse, distributed file system, etc.\n",
    "    - stream processing system : processes data in real time as it arrives and stores the results in a distributed data store, e.g., message queue, NoSQL database, etc.\n",
    "- 4 layers: \n",
    "    1. data ingestion layer : collects and stores raw data from various sources, e.g., log files, sensors, message queues, APIs, etc.\n",
    "        - data is typically ingested in real time and fed to the batch layer and speed layer simultaneously\n",
    "    2. batch layer : responsible for processing historical data in large batches and storing the results in a centralized data store, e.g., data warehouse, distributed file system, etc.\n",
    "        - typically uses a batch processing framework, e.g., Hadoop, Spark, etc.\n",
    "        - designed to handle large volumes of data and provide a complete view of all data\n",
    "    3. speed layer : responsible for processing real time data as it arrives and storing the results in a distributed data store, e.g., message queue, NoSQL database, etc.\n",
    "        - typically uses a stream processing framework, e.g., Flink, Storm, etc.\n",
    "        - designed to handle high volume data streams and provide up to date views of the data\n",
    "    4. serving layer : responsible for serving query results to users in real time\n",
    "        - typically implemented as a layer on top of the batch and stream processing layers\n",
    "        - accessed through a query layer, which allows users to query the data using a query language, e.g., SQL, HiveQL, etc.\n",
    "        - designed to provide fast and reliable access to query results, regardless of whether the data is being accessed from the batch or stream processing layers\n",
    "        - typically uses a distributed data store, e.g., NoSQL database, distributed cache, etc.\n",
    "- advantages : \n",
    "    - scalability : designed to handle large volumes of data and scale horizontally to meet the needs of the business\n",
    "    - fault tolerance : designed to be fault tolerant, with multiple layers and systems working together to ensure that data is processed and stored reliably\n",
    "    - flexibility : designed to handle a wide range of data processing workloads, from historical batch processing to streaming architecture\n",
    "- disadvantages :\n",
    "    - complexity : designed to be complex, uses multiple layers and systems to process and store data\n",
    "    - errors and data discrepancies : with doubled implementations of different workflows, you may run into a problem of different results from batch and stream processing engines\n",
    "    - architecture lock-in : may be super hard to reorganize or migrate existing data stored in the Lambda architecture\n",
    "- use cases :\n",
    "    - handling large volumes of data and providing low-latency query results, e.g., dashboards and reporting\n",
    "    - batch processing tasks, e.g., data cleansing, transformation, and aggregation\n",
    "    - stream processing tasks, e.g., event processing, machine learning models, anomaly detection, and fraud detection\n",
    "    - building data lakes, e.g., centralized repositories that store structured and unstructured data at rest\n",
    "\n",
    "### Kappa architecture\n",
    "\n",
    "<img alt=\"picture 3\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/0908ffe3ce19857b78b9684c8eb50acd9882451d681dfcbc346e517b2c6e14d5.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "<img alt=\"picture 7\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/62326926883a15f1de431103d4df58f52eba4580a527aff23020f4096edbec30.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto; padding-top: 20px\">\n",
    "\n",
    "- proposed by Jay Kreps (2014), an evolution of Lambda architecture, uses a single data processing system to handle both batch processing and stream processing workloads\n",
    "- treats everything as streams, allows it to provide a more streamlined and simplified data processing pipeline while still providing fast and reliable access to query results\n",
    "- there is only the speed layer (stream layer)\n",
    "- 2 components : \n",
    "    1. ingestion component : same as Lambda architecture\n",
    "    2. processing component : responsible for processing the data as it arrives and storing the results in a distributed data store, e.g., message queue, NoSQL database, etc.\n",
    "        - typically implemented using a stream processing framework, e.g., Flink, Storm, etc.\n",
    "        - designed to handle high volume data streams and provide fast and reliable access to query results\n",
    "        - there is no seperate serving layer, instead, the stream processing layer is responsible for serving query results to users in real time\n",
    "- advantages :\n",
    "    - simplicity and streamlined pipeline : uses a single data processing system to handle both batch processing and stream processing workloads, which makes it simpler to set up and maintain compared to Lambda architecture\n",
    "    - enables high-throughput big data processing of historical data : although it may feel that it is not designed for these set of problems, Kappa architecture can support these use cases with grace, enabling reprocessing directly from our stream processing job\n",
    "    - ease of migrations and reorganizations : as there is only stream processing pipeline, you can perform migrations and reorganizations with new data streams created from the canonical data store\n",
    "    - tiered storage : tiered storage is a method of storing data in different storage tiers, based on the access patterns and performance requirements of the data\n",
    "        - in Kappa architecture, tiered storage is not a core concept, however, it is possible to use tiered storage in conjunction with Kappa architecture, as a way to optimize storage costs and performance\n",
    "        - for example, businesses may choose to store historical data in a lower-cost fault tolerant distributed storage tier, such as object storage, while storing real-time data in a more performant storage tier, such as a distributed cache or a NoSQL database\n",
    "        - tiered storage Kappa architecture makes it a cost-efficient and elastic data processing technique without the need for a traditional data lake\n",
    "- disadvantages :\n",
    "    - complexity : although Kappa architecture is simpler than Lambda architecture, it can still be complex to set up and maintain, especially for businesses that are not familiar with stream processing frameworks\n",
    "    - costly infrastructure with scalability issues : storing big data in an event streaming platform can be costly, to make it more cost efficient, you may want to use data lake approach from your cloud provider (like AWS S3 or GCP Google Cloud Storage), another common approach for big data architecture is building a 'streaming data lake' with Apache Kafka as a streaming layer and object storage to enable long-term data storage\n",
    "- use cases:\n",
    "    - real-time data processing, e.g., continuous data pipelines, real-time data processing, machine learning models and real-time data analytics, IoT systems, etc.\n",
    "    - building data lakes, e.g., centralized repositories that store structured and unstructured data at rest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Configuration and Coordination in Distributed Systems\n",
    "\n",
    "### Distributed systems\n",
    "- collection of independent computers that appears to its users as a single coherent system\n",
    "- computers coordinate their actions by passing messages over a network (nodes together form a cluster)\n",
    "- nodes need to share metadata and state information to coordinate their actions (e.g., leader election, location of data, etc.)\n",
    "    - this is difficult, leads to incorrect results, inconsistent state, etc.\n",
    "    - needs a system-wide service that correctly and reliably implements distributed configuration and coordination\n",
    "- challenges : \n",
    "    - unreliable network : network partitions, message loss, etc.\n",
    "        - latency issues, bandwidth changes, lost connections, etc.\n",
    "        - split brain problem : loss of connectivity between nodes, nodes may form multiple clusters\n",
    "            - some amount of state is innacessible to some nodes, nodes may diverge in their views of the system\n",
    "            - we should disallow writes to the state until the network partition is resolved, allow one partition to remain functional while degrading the capabilities of the other partition\n",
    "    - unreliable nodes : node failures, node restarts, etc.\n",
    "    - clock synchronization : clocks on different nodes may be out of sync, lead to drifts in time and disordering of events\n",
    "    - consistency in application state : nodes may have different views of the application state (use Paxos, multi-Paxos, Raft algorithms to solve this)\n",
    "        - these are difficult to implement\n",
    "\n",
    "#### Data delivery semantics\n",
    "Data Delivery Semantics, governing data transfer, includes three types:\n",
    "1. At Most Once (AMO): The data is delivered at most once. It may not be delivered at all.\n",
    "    - eg: sending a notification; no guarantee of receipt, and missed notifications are not that important\n",
    "2. At Least Once (ALO): The data is delivered at least once. It may be delivered multiple times.\n",
    "    - eg: email delivery; continuous sending until acknowledged, allowing duplicates\n",
    "3. Exactly Once (EO): The data is delivered exactly once. Deduplication may be required.\n",
    "    - eg: financial transactions; ensures no duplicates but involves more complex mechanisms\n",
    "\n",
    "### Apache Kafka [[Vid1]](https://youtu.be/B5j3uNBH8X4), [[Vid2]](https://youtu.be/jY02MB-sz8I)\n",
    "- [Kafka docs](https://kafka.apache.org/documentation/#gettingStarted)\n",
    "- distributed streaming platform, used for building real time data pipelines and streaming applications\n",
    "- Kafka is a distributed system, it needs to coordinate its nodes to ensure that they are all in agreement\n",
    "- uses ZooKeeper to manage its cluster, store metadata, and perform leader election\n",
    "    - Cluster management : ZooKeeper is used to manage the Kafka cluster, including configuration, topic metadata, broker metadata, etc.\n",
    "    - Failure detection and recovery, leader election\n",
    "    - Store ACLs for authorization\n",
    "\n",
    "<img alt=\"picture 8\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8c6d01dd21e6edaf3e614f69553b607fdcf751a25326b086d27dd704527c5bd9.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Architecture:\n",
    "    1. Producer : responsible for publishing data to Kafka cluster\n",
    "        - Can be written in any language, native: Java, C/C++, Python, Go, .NET, etc\n",
    "    2. Consumer : responsible for subscribing to topics and processing the data published to them\n",
    "        - new inflowing messages are automatically retrieved\n",
    "        - consumer offset, keeps track of the last message read, is stored in special Kafka topic\n",
    "    3. Cluster : collection of nodes that together form a Kafka cluster\n",
    "        - nodes are called brokers, each broker is identified by a unique integer ID\n",
    "- Producers and consumers are completely decoupled:\n",
    "    - slow consumers/producers don't affect each other, add more consumers/producers to scale, failures of consumers/producers don't affect system\n",
    "- Topics: streams of related messages in Kafka, is a logical representation, categorizes messages into groups\n",
    "    - developers can define any number of topics\n",
    "    - topics are partitioned, each partition is an ordered, immutable sequence of messages\n",
    "        - partitions are distributed across brokers, each partition is replicated across multiple brokers for fault tolerance\n",
    "    - producers <-> topics are N:N relation, same with consumers <-> topics\n",
    "\n",
    "<img alt=\"picture 9\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/8b5680ad6c9e3e97d745b8928d567ee1f36a3817c22b02207b94a700faf96dae.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- Kafka data record : header, key, value,  timestamp\n",
    "    - header : contains metadata about the record, e.g., topic, partition, offset, etc.\n",
    "    - key : optional, used for partitioning, if key is null, messages are round-robin distributed to partitions\n",
    "    - value : actual data, can be any format, e.g., JSON, Avro, etc.\n",
    "    - timestamp\n",
    "- Broker replication:\n",
    "    - each partition has one broker acting as leader and multiple brokers acting as followers\n",
    "    - leader handles all read and write requests for the partition, followers replicate the leader\n",
    "    - if leader fails, one of the followers is elected as the new leader\n",
    "    - if a follower fails, it is removed from the ISR (in-sync replica) and replaced by a new follower\n",
    "- Load Balancing and Semantic Partitioning\n",
    "    - Producers use a partitioning strategy (defined by producer) to determine which partition to publish messages to\n",
    "        - default strategy : hash(key) % n_partitions\n",
    "            - messages with the same key are always published to the same partition\n",
    "        - no key : round-robin\n",
    "        - custom partitioner is also allowed\n",
    "- Estimating number of partitions:\n",
    "    - $N = \\max(\\frac{T_t}{T_p}, \\frac{T_t}{T_c})$\n",
    "        - $T_t$ : throughput of the system (processing speed of the slowest component)\n",
    "        - $T_p$ : throughput of producer writing onto single partition\n",
    "        - $T_c$ : throughput of a consumer reading from single partition\n",
    "\n",
    "Apache Kafka Commands:\n",
    "1. start kafka server : `bin/kafka-server-start.sh config/server.properties`\n",
    "2. create a topic : `bin/kafka-topics.sh --create --topic <topic-name> --bootstrap-server <bootstrap-server> --partitions <num-partitions> --replication-factor <replication-factor>`\n",
    "3. list topics : `bin/kafka-topics.sh --list --bootstrap-server <bootstrap-server>`\n",
    "4. produce messages : `bin/kafka-console-producer.sh --topic <topic-name> --bootstrap-server <bootstrap-server>`\n",
    "5. consume messages : `bin/kafka-console-consumer.sh --topic <topic-name> --bootstrap-server <bootstrap-server> --from-beginning`\n",
    "6. describe consumer groups : `bin/kafka-consumer-groups.sh --describe --group <group-id> --bootstrap-server <bootstrap-server>`\n",
    "7. check offsets : `bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic <topic-name> --group <group-id> --bootstrap-server <bootstrap-server>`        \n",
    "\n",
    "### Apache Zookeeper [[Vid]](https://youtu.be/gZj16chk0Ss)\n",
    "- [ZooKeeper overview](https://zookeeper.apache.org/doc/r3.9.1/zookeeperOver.html)\n",
    "- distributed coordination service, used for building distributed systems, used with Hadoop, HBase, Kafka, etc\n",
    "- services:\n",
    "    - naming : provides a hierarchical namespace for nodes in cluster\n",
    "    - configuration management : stores and manages configuration information for systems\n",
    "    - cluster management : manages the membership of nodes in a cluster\n",
    "    - leader election : elects a leader among distributed nodes\n",
    "    - locking and synchronization : provides primitives for synchronization and coordination between distributed processes\n",
    "- benefits : synchronization, serialization (application runs consistently), atomicity, reliability, etc.\n",
    "- architecture:\n",
    "    - ensemble : a group of ZooKeeper servers that collectively manage the service (3 minimum)\n",
    "        - leaders are elected among the ensemble, followers replicate the leader\n",
    "    - quorum : majority of servers must agree on an operation for it to be committed\n",
    "    - client : connects to any server in the ensemble, sends heartbeat to server, if server doesn't respond, client connects to another server\n",
    "    - znode : the basic data structure in ZooKeeper, analogous to a file system node\n",
    "        - znodes are organized in a hierarchical tree-like structure\n",
    "        - each znode contains data and metadata (version number, ACL, etc.)\n",
    "        - znodes can be ephemeral (deleted when client disconnects) or persistent (deleted when explicitly deleted) or sequential (name of znode is appended with a monotonically increasing counter)\n",
    "\n",
    "<img alt=\"picture 10\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/2574be9ee8aa76e507988eb8754c5d9ec658d83ada1c13e2050ff13d69f3821e.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "- operations : create, read, update, delete, watch\n",
    "    - watches : allows clients to be notified of changes to a znode\n",
    "- ACLs : access control lists, used to control access to znodes\n",
    "    - each znode has an ACL that specifies the permissions for that znode\n",
    "    - permissions : read, write, create, delete, admin\n",
    "    - ACLs are stored in ZooKeeper and managed by ZooKeeper itself\n",
    "- deployment:\n",
    "    - ensemble size : typically odd number (3, 5, 7) for fault tolerance\n",
    "    - network configuration : low-latency, reliable network required between ZooKeeper servers\n",
    "    - client deployment : clients connect to any server in the ensemble\n",
    "- split brain problem : loss of connectivity between nodes, nodes may form multiple clusters\n",
    "    - some amount of state is innacessible to some nodes, nodes may diverge in their views of the system\n",
    "    - we should disallow writes to the state until the network partition is resolved, allow one partition to remain functional while degrading the capabilities of the other partition\n",
    "\n",
    "Apache ZooKeeper Commands:\n",
    "1. start zookeeper : `bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "2. connect to zookeeper cli : `bin/zookeeper-shell.sh <zookeeper-host>:<zookeeper-port>`\n",
    "3. create a znode : `create /<path> <data>`\n",
    "4. list znode contents : `ls /<path>`\n",
    "5. get znode data : `get /<path>`\n",
    "6. set znode data : `set /<path> <new-data>`\n",
    "7. delete znode : `delete /<path>`\n",
    "\n",
    "### Pub-sub messaging workflow\n",
    "- producers send messages to a topic at regular intervals\n",
    "- Kafka broker stores all messages in the partitions configured for that particular topic\n",
    "    - ensures the messages are equally shared between partitions\n",
    "- consumer subscribes to a specific topic, Kafka will provide the current offset of the topic to the consumer and also saves the offset in the Zookeeper ensemble\n",
    "- consumer will request Kafka at regular intervals for new messages\n",
    "- once Kafka receives messages from producers, it forwards these messages to the consumers\n",
    "- consumer will receive the message and process it\n",
    "- once the messages are processed, consumer will send an acknowledgement to the Kafka broker\n",
    "- once Kafka receives an acknowledgement, it changes offset to new value and updates it in Zookeeper\n",
    "    - since offsets are maintained in Zookeeper, consumer can read next message correctly even during server outrages\n",
    "- this above flow will repeat until consumer stops the request\n",
    "- consumer has option to rewind/skip to desired offset of a topic at any time and read all subsequent messages\n",
    "\n",
    "### Message queues vs Pub-sub systems\n",
    "- message queues : messages are stored in a queue, each message is delivered to exactly one consumer\n",
    "    - consumers can be grouped into consumer groups, each message is delivered to one consumer in each consumer group\n",
    "    - consumers can acknowledge messages, once a message is acknowledged, it is deleted from the queue\n",
    "    - if a consumer fails to acknowledge a message before a timeout, the message is redelivered to another consumer\n",
    "    - examples : RabbitMQ, ActiveMQ, etc\n",
    "- pub-sub systems : messages are published to a topic, each message is delivered to all consumers subscribed to that topic\n",
    "    - message will only be deleted if it's consumed by all subscribers to the category\n",
    "    - examples : Kafka, Google Cloud Pub/Sub, etc have retention policy that ensures messages stay in the queue for a specified amount of time, even after they are consumed by all subscribers\n",
    "\n",
    "### Streaming Architectures\n",
    "\n",
    "Random streaming architectures I found\n",
    "\n",
    "[Link1](https://www.simform.com/blog/stream-processing/)\n",
    "<img alt=\"picture 11\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/132f86c07b23b9b8e108558f9e27bf6159a9c86ca033cfa15341b247cb2ebe62.png\" width=\"700\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "[Link2](https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/what-is-a-modern-streaming-data-architecture.html)\n",
    "<img alt=\"picture 12\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/4318bba6aca69a8f6dce99565fb3d697ee78c24378ddc9c8ff3d8b1dba700aa4.png\" width=\"700\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "[Link3](https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/data/stream-processing-stream-analytics)\n",
    "<img alt=\"picture 13\" src=\"https://cdn.jsdelivr.net/gh/sharatsachin/images-cdn@master/images/cd7def52aa38a303f47ef8c8f15a484d61f92de42f90e3e4b8a83f7ce2049750.png\" width=\"700\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
