{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, fpgrowth, fpmax, association_rules\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean\n",
    "    \n",
    "$${\\displaystyle {\\bar {x}}={\\frac {1}{n}}\\left(\\sum_{i=1}^{n}{x_{i}}\\right)={\\frac {x_{1}+x_{2}+\\cdots +x_{n}}{n}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median   \n",
    "\n",
    "if ${\\displaystyle n}$ is odd, $${\\displaystyle \\mathrm {median} (x)=x_{(n+1)/2}}$$\n",
    "if ${\\displaystyle n}$ is even, $${\\displaystyle \\mathrm {median} (x)={\\frac {x_{(n/2)}+x_{((n/2)+1)}}{2}}}$$\n",
    "also, \n",
    "$$mean − mode ≈ 3(mean − median)$$\n",
    "$$midrange = (min + max) / 2$$\n",
    "$$range = max− min$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IQR = Q3 − Q1\n",
    "\n",
    "Outliers are values falling at least 1.5 × IQR above the third quartile or below the first quartile.\n",
    "\n",
    "<div>\n",
    "<img src=\"\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len : 15, sorted : [20, 22, 30, 33, 33, 35, 35, 35, 35, 36, 40, 41, 42, 51, 54]\n",
      "Q1 : 33.0, Median : 35.0, Q3 : 40.5\n",
      "IQR : 7.5\n",
      "Minimum : 21.75, Maximum : 51.75\n"
     ]
    }
   ],
   "source": [
    "# data = np.array([30,36,47,50,52,52,56,60,63,70,70,110])\n",
    "data = np.array([20, 22, 30, 33, 33, 35, 35, 35, 35, 36, 40, 41, 42, 51, 54])\n",
    "print(f'Len : {len(data)}, sorted : {sorted(data)}')\n",
    "q1 = np.quantile(data, 0.25, method= 'midpoint')\n",
    "q2 = np.quantile(data, 0.5, method= 'midpoint')\n",
    "q3 = np.quantile(data, 0.75, method= 'midpoint')\n",
    "iqr = q3 - q1\n",
    "minimum = q1 - 1.5 * iqr\n",
    "maximum = q3 + 1.5 * iqr\n",
    "print(f'Q1 : {q1}, Median : {q2}, Q3 : {q3}')\n",
    "print(f'IQR : {iqr}')\n",
    "print(f'Minimum : {minimum}, Maximum : {maximum}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5−number summary\n",
    "\n",
    "[Minimum, Q1, Median, Q3, Maximum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/0WovyJS.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.imgur.com/0WovyJS.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "$$\\operatorname {Var} (X)=\\operatorname {E} \\left[(X-\\mu )^{2}\\right]$$\n",
    "$$\\operatorname {Var} (X)=\\operatorname {Cov} (X,X)$$\n",
    "$${\\displaystyle \\operatorname {Var} (X) = \\operatorname {E} \\left[X^{2}\\right]-\\operatorname {E} [X]^{2}}$$\n",
    "$${\\displaystyle \\operatorname {Var} (X)={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-\\mu )^{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.25"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard deviation\n",
    "\n",
    "$${\\displaystyle \\sigma = \\sqrt{\\operatorname {Var} (X)} =\\sqrt{{\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-\\mu )^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8722813232690143"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissimilarity matrix\n",
    "1. Nominal variables\n",
    "    \n",
    "    $${\\displaystyle d(i,j) = \\frac{n\\_variables - n\\_matches}{n\\_variables}}$$\n",
    "    \n",
    "2. Ordinal variables \n",
    "    \n",
    "    Map the range of all attributes to $[0, 1]$\n",
    "    \n",
    "    $${\\displaystyle d(i,j) = \\sqrt{(\\sum_f{(x_{if} - x_{jf})^{2}})}}$$\n",
    "    \n",
    "    where $x_i$ and $x_j$ are two scaled values of different rows for the same variable, summed up over all the ordinal variables \n",
    "    \n",
    "3. Numerical variables\n",
    "\n",
    "    1. Euclidean distance \n",
    "        \n",
    "        $${\\displaystyle d(i,j) = \\sqrt{(\\sum_f{(x_{if} - x_{jf})^{2}})}}$$\n",
    "        \n",
    "    2. Manhattan distance \n",
    "        \n",
    "        $${\\displaystyle d(i,j) = (\\sum_f{|x_{if} - x_{jf}|})}$$\n",
    "        \n",
    "    3. Minkowski distance \n",
    "        \n",
    "        $${\\displaystyle d(i,j) = (\\sum_f{|x_{if} - x_{jf}|^{p}})^{1/p}}$$\n",
    "\n",
    "        where $x_i$ and $x_j$ are two scaled values of different rows for the same variable, summed up over all the numerical variables\n",
    "        \n",
    "    4. Supremum distance, for $p = \\infin$\n",
    "        \n",
    "        $${\\displaystyle d(i,j) = max_f({|x_{if} - x_{jf}|})}$$\n",
    "        \n",
    "4. Binary variables\n",
    "    1. Symmetric\n",
    "        \n",
    "        $${\\displaystyle d(i,j) = {M_{01} + M_{10} \\over M_{00} + M_{01} + M_{10} + M_{11}}}$$\n",
    "        \n",
    "    2. Unsymmetric\n",
    "        \n",
    "        $${\\displaystyle d(i,j) = {M_{01} + M_{10} \\over M_{01} + M_{10} + M_{11}}}$$\n",
    "        \n",
    "        Jacard coeffecient, $${\\displaystyle J = 1 - d(i,j) = {M_{11} \\over M_{01} + M_{10} + M_{11}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/DkbRiVQ.png\" width=\"1000\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.imgur.com/DkbRiVQ.png', width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilarity matrix : \n",
      "[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "Dissimilarity matrix : \n",
      "[[0.  0.  0. ]\n",
      " [1.  0.  0. ]\n",
      " [0.5 0.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "# dissimilarity measure\n",
    "\n",
    "# for nominal data\n",
    "data_col = ['A', 'B', 'A']\n",
    "dissimilarity_matrix = np.zeros((len(data_col), len(data_col)), dtype=float)\n",
    "for i in range(len(data_col)):\n",
    "    for j in range(i):\n",
    "        if data_col[i] != data_col[j]:\n",
    "            dissimilarity_matrix[i][j] = 1\n",
    "print(f'Dissimilarity matrix : \\n{dissimilarity_matrix}')\n",
    "\n",
    "# for ordinal data / numeric data\n",
    "data_col = [0, 1, 0.5]\n",
    "dissimilarity_matrix = np.zeros((len(data_col), len(data_col)))\n",
    "# scale between 0 and 1 \n",
    "data_col = (data_col - np.min(data_col)) / (np.max(data_col) - np.min(data_col))\n",
    "dissimilarity_matrix = np.zeros((len(data_col), len(data_col)), dtype=float)\n",
    "for i in range(len(data_col)):\n",
    "    for j in range(i):\n",
    "        dissimilarity_matrix[i][j] = abs(data_col[i] - data_col[j])\n",
    "print(f'Dissimilarity matrix : \\n{dissimilarity_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "$${\\displaystyle S_{C}(A,B):=\\cos(\\theta )={\\mathbf {A} \\cdot \\mathbf {B}  \\over \\|\\mathbf {A} \\|\\|\\mathbf {B} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{A_{i}B_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{A_{i}^{2}}}}{\\sqrt {\\sum \\limits _{i=1}^{n}{B_{i}^{2}}}}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b = [15  0  6  0  2  0  0  2  0  0]\n",
      "a_norm = [25  0  9  0  4  0  0  4  0  0]\n",
      "b_norm = [9 0 4 0 1 1 0 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25, 6.48074069840786, 4.123105625617661, 0.9356014857063997)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([5, 0, 3, 0, 2, 0, 0, 2, 0,0])\n",
    "b = np.array([3, 0, 2, 0, 1, 1, 0, 1, 0,1])\n",
    "print(f\"a * b = {a * b}\")\n",
    "a_dot_b = np.dot(a, b)\n",
    "print(f\"a_norm = {a * a}\")\n",
    "a_norm = np.linalg.norm(a)\n",
    "print(f\"b_norm = {b * b}\")\n",
    "b_norm = np.linalg.norm(b)\n",
    "a_dot_b, a_norm, b_norm, a_dot_b / (a_norm * b_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "1. Min-max normalization\n",
    "    \n",
    "    $${\\displaystyle x^{\\prime}=\\frac{x-\\min (x)}{\\max (x)-\\min (x)}}$$\n",
    "    \n",
    "    $${\\displaystyle x^{\\prime}=a+\\frac{(x-\\min (x))(b-a)}{\\max (x)-\\min (x)}}$$\n",
    "    \n",
    "2. Z-score normalization\n",
    "    \n",
    "    $${\\displaystyle z=\\frac{x-\\mu}{\\sigma}}$$\n",
    "    \n",
    "3. Decimal normalization - normalize to range [−1, +1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min : 1, Max : 10, \n",
      "data_1 : [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ]\n",
      "Mean : 5.5, Std : 2.8722813232690143, \n",
      "data_2 : [-1.5666989  -1.21854359 -0.87038828 -0.52223297 -0.17407766  0.17407766\n",
      "  0.52223297  0.87038828  1.21854359  1.5666989 ]\n",
      "data_3 : [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "# min-max normalization\n",
    "data = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "data_1 = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "print(f'Min : {np.min(data)}, Max : {np.max(data)}, \\ndata_1 : {data_1}')\n",
    "\n",
    "# z-score normalization\n",
    "data_2 = (data - np.mean(data)) / np.std(data)\n",
    "print(f'Mean : {np.mean(data)}, Std : {np.std(data)}, \\ndata_2 : {data_2}')\n",
    "\n",
    "# decimal scaling normalization\n",
    "data_3 = data / 10\n",
    "print(f'data_3 : {data_3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning \n",
    "\n",
    "1. Equal-width binning\n",
    "    \n",
    "    $${\\displaystyle \\Delta = \\frac{max - min}{n}}$$\n",
    "    \n",
    "    $${\\displaystyle x^{\\prime} = \\frac{x - min}{\\Delta}}$$\n",
    "\n",
    "2. Equal-frequency binning\n",
    "    \n",
    "    $${\\displaystyle \\Delta = \\frac{N}{n}}$$\n",
    "    \n",
    "    $${\\displaystyle x^{\\prime} = \\frac{rank(x)}{\\Delta}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted data : [53 56 57 63 66 67 67 67 68 69 70 70 70 70 72 73 75 75 76 76 78 79 80 81]\n",
      "Min : 53, Max : 81, Range : 28, Count : 24, Width : 9.333333333333334, Depth : 8.0\n",
      "Equal width binning : [53.         62.33333333 71.66666667 81.        ]\n",
      "Bin 1 : [53 56 57]\n",
      "Bin 2 : [63 66 67 67 67 68 69 70 70 70 70]\n",
      "Bin 3 : [72 73 75 75 76 76 78 79 80 81]\n",
      "Equal frequency binning : [53.         67.66666667 73.66666667 81.        ]\n",
      "Bin 1 : [53 56 57 63 66 67 67 67]\n",
      "Bin 2 : [68 69 70 70 70 70 72 73]\n",
      "Bin 3 : [75 75 76 76 78 79 80 81]\n"
     ]
    }
   ],
   "source": [
    "# equal width binning and equal frequency binning\n",
    "\n",
    "data = np.array([70, 70, 72, 73, 75, 75, 76, 76, 78, 79, 80, 81, 53, 56, 57, 63, 66, 67, 67, 67, 68, 69, 70, 70])\n",
    "n_bins = 3\n",
    "\n",
    "# sort data\n",
    "data = np.sort(data)\n",
    "print(f'Sorted data : {data}')\n",
    "print(f'Min : {np.min(data)}, Max : {np.max(data)}, Range : {np.max(data) - np.min(data)}, Count : {len(data)}, Width : {(np.max(data) - np.min(data)) / n_bins}, Depth : {len(data) / n_bins}')\n",
    "\n",
    "# equal width binning\n",
    "data_1 = np.array([np.min(data) + (np.max(data) - np.min(data)) / n_bins * i for i in range(n_bins + 1)])\n",
    "print(f'Equal width binning : {data_1}')\n",
    "# data split into bins\n",
    "print(f'Bin 1 : {data[data < data_1[1]]}')\n",
    "print(f'Bin 2 : {data[(data >= data_1[1]) & (data < data_1[2])]}')\n",
    "print(f'Bin 3 : {data[data >= data_1[2]]}')\n",
    "\n",
    "# equal frequency binning\n",
    "data_2 = np.array([np.quantile(data, i / n_bins) for i in range(n_bins + 1)])\n",
    "print(f'Equal frequency binning : {data_2}')\n",
    "# data split into bins\n",
    "print(f'Bin 1 : {data[data < data_2[1]]}')\n",
    "print(f'Bin 2 : {data[(data >= data_2[1]) & (data < data_2[2])]}')\n",
    "print(f'Bin 3 : {data[data >= data_2[2]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson’s correlation coefficient\n",
    "\n",
    "For a sample, $${\\displaystyle r_{x y}=\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sqrt{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2} \\sqrt{\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mean : 14.5, y_mean : 18.5\n",
      "x - x_mean : [ 5.5 -4.5  8.5 -9.5]\n",
      "y - y_mean : [ 11.5 -13.5  10.5  -8.5]\n",
      "(x - x_mean) ** 2 : [30.25 20.25 72.25 90.25]\n",
      "(y - y_mean) ** 2 : [132.25 182.25 110.25  72.25]\n",
      "(x - x_mean) * (y - y_mean) : [63.25 60.75 89.25 80.75]\n",
      "numerator : 294.0\n",
      "denominator : 325.36287434186465\n",
      "pearson correlation : 0.9036064750617149\n"
     ]
    }
   ],
   "source": [
    "# calculating pearson correlation\n",
    "x = np.array([20, 10, 23, 5])\n",
    "y = np.array([30, 5, 29, 10])\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "print(f'x_mean : {x_mean}, y_mean : {y_mean}')\n",
    "# numerator\n",
    "print(f'x - x_mean : {x - x_mean}\\ny - y_mean : {y - y_mean}')\n",
    "print(f'(x - x_mean) ** 2 : {(x - x_mean) ** 2}\\n(y - y_mean) ** 2 : {(y - y_mean) ** 2}\\n(x - x_mean) * (y - y_mean) : {(x - x_mean) * (y - y_mean)}')\n",
    "numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "print(f'numerator : {numerator}')\n",
    "denominator = np.sqrt(np.sum((x - x_mean) ** 2)) * np.sqrt(np.sum((y - y_mean) ** 2))\n",
    "print(f'denominator : {denominator}')\n",
    "print(f'pearson correlation : {numerator / denominator}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\chi^2$  statistic\n",
    "\n",
    "The $\\chi^2$ statistic is a measure of the difference between expected and observed data in a statistical analysis. It is commonly used to test the null hypothesis that two categorical variables are independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>sum_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_col</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a   b   c  sum_row\n",
       "A        11   5   1       17\n",
       "B         8   6   8       22\n",
       "C         3  10  12       25\n",
       "sum_col  22  21  21       64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>sum_row</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>5.84375</td>\n",
       "      <td>5.578125</td>\n",
       "      <td>5.578125</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>7.56250</td>\n",
       "      <td>7.218750</td>\n",
       "      <td>7.218750</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>8.59375</td>\n",
       "      <td>8.203125</td>\n",
       "      <td>8.203125</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_col</th>\n",
       "      <td>22.00000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                a          b          c  sum_row\n",
       "A         5.84375   5.578125   5.578125       17\n",
       "B         7.56250   7.218750   7.218750       22\n",
       "C         8.59375   8.203125   8.203125       25\n",
       "sum_col  22.00000  21.000000  21.000000       64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>4.549632</td>\n",
       "      <td>0.059918</td>\n",
       "      <td>3.757397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.025310</td>\n",
       "      <td>0.205763</td>\n",
       "      <td>0.084551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>3.641023</td>\n",
       "      <td>0.393601</td>\n",
       "      <td>1.757411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c\n",
       "A  4.549632  0.059918  3.757397\n",
       "B  0.025310  0.205763  0.084551\n",
       "C  3.641023  0.393601  1.757411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-squared statistic : 14.474605180915342\n"
     ]
    }
   ],
   "source": [
    "# calculating chi-square statistic\n",
    "\n",
    "data = {'A': {'a': 11, 'b': 5, 'c': 1}, 'B': {'a': 8, 'b': 6, 'c': 8}, 'C': {'a': 3, 'b': 10, 'c': 12}}\n",
    "data = pd.DataFrame(data).T\n",
    "data['sum_row'] = data.sum(axis=1)\n",
    "data.loc['sum_col'] = data.sum(axis=0)\n",
    "display(data)\n",
    "\n",
    "print(\"Expected Values\")\n",
    "data1 = data.copy()\n",
    "# calculate expected value\n",
    "for i in data1.index[:-1]:\n",
    "    for j in data1.columns[:-1]:\n",
    "        data1.loc[i, j] = data1.loc['sum_col', j] * data1.loc[i, 'sum_row'] / data1.loc['sum_col', 'sum_row']\n",
    "display(data1)\n",
    "\n",
    "# calculate chi-square statistic\n",
    "data2 = data.copy().iloc[:-1, :-1]\n",
    "for i in data2.index:\n",
    "    for j in data2.columns:\n",
    "        data2.loc[i, j] = (data2.loc[i, j] - data1.loc[i, j]) ** 2 / data1.loc[i, j]\n",
    "display(data2)\n",
    "\n",
    "chi = data2.sum().sum()\n",
    "print(f'Chi-squared statistic : {chi}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree impurity measures\n",
    "\n",
    "$${\\displaystyle \\text{Entropy}(t) = -\\sum_{c=1}^{C} p(c|t) log_2(p(c|t))}$$\n",
    "\n",
    "$${\\displaystyle Gini(t) = 1 - \\sum_{c=1}^{C} [p(c|t)]^2}$$\n",
    "\n",
    "$$\\text{Misclassification error}(t) =  1 - \\max_c[p(c|t)]$$\n",
    "\n",
    "Where $t$ is the current node, $C$ is the number of classes, and $p(c|t)$ is the proportion of the samples that belong to class $c$ at node $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entropy\n",
      "Class : A, Count : 5, Probability : 0.357, Class Entropy : 0.531\n",
      "Class : B, Count : 9, Probability : 0.643, Class Entropy : 0.410\n",
      "Total Entropy : 0.940\n",
      "\n",
      "Gini Index\n",
      "Class : A, Count : 5, Probability : 0.357, Class Gini Index : 0.128\n",
      "Class : B, Count : 9, Probability : 0.643, Class Gini Index : 0.413\n",
      "Total Gini Index : 0.459\n",
      "\n",
      "Misclassification Error\n",
      "Misclassification Error : 0.357\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "# code for calculating entropy, gini index, and misclassification error\n",
    "\n",
    "# sample data\n",
    "data = {'A': 5, 'B': 9}\n",
    "\n",
    "# calculate entropy\n",
    "print('\\nEntropy')\n",
    "entropy = 0.0\n",
    "for c, i in data.items():\n",
    "    p = i/sum(data.values())\n",
    "    if p != 0:\n",
    "        class_entropy = -p*log2(p)\n",
    "    else:\n",
    "        class_entropy = 0\n",
    "    entropy += class_entropy\n",
    "    print(f'Class : {c}, Count : {i}, Probability : {p:.3f}, Class Entropy : {class_entropy:.3f}')\n",
    "print(f'Total Entropy : {entropy:.3f}')\n",
    "\n",
    "# calculate gini index\n",
    "print('\\nGini Index')\n",
    "gini = 1\n",
    "for c, i in data.items():\n",
    "    p = i/sum(data.values())\n",
    "    gini -= p**2\n",
    "    print(f'Class : {c}, Count : {i}, Probability : {p:.3f}, Class Gini Index : {p**2:.3f}')\n",
    "print(f'Total Gini Index : {gini:.3f}')\n",
    "\n",
    "# calculate misclassification error\n",
    "print('\\nMisclassification Error')\n",
    "misclassification = 1 - max(data.values())/sum(data.values())\n",
    "print(f'Misclassification Error : {misclassification:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entropy before split\n",
      "Class : A, Count : 9, Probability : 0.643, Class Entropy : 0.410\n",
      "Class : B, Count : 5, Probability : 0.357, Class Entropy : 0.531\n",
      "Total Entropy before split: 0.940\n",
      "\n",
      "Entropy after split\n",
      "For attribute value a1\n",
      "Class : A, Count : 3, Probability : 0.600, Class Entropy : 0.442\n",
      "Class : B, Count : 2, Probability : 0.400, Class Entropy : 0.529\n",
      "Attribute : a1, Entropy : 0.971\n",
      "For attribute value a2\n",
      "Class : A, Count : 4, Probability : 1.000, Class Entropy : -0.000\n",
      "Class : B, Count : 0, Probability : 0.000, Class Entropy : 0.000\n",
      "Attribute : a2, Entropy : 0.000\n",
      "For attribute value a3\n",
      "Class : A, Count : 2, Probability : 0.400, Class Entropy : 0.529\n",
      "Class : B, Count : 3, Probability : 0.600, Class Entropy : 0.442\n",
      "Attribute : a3, Entropy : 0.971\n",
      "Total Entropy after split: 0.694\n",
      "Information Gain : 0.247\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# code for calculating information gain\n",
    "\n",
    "data = {'a1': {'A': 3, 'B': 2}, 'a2': {'A': 4, 'B': 0}, 'a3': {'A': 2, 'B': 3}}\n",
    "summed_data = Counter()\n",
    "for k, v in data.items():\n",
    "    summed_data.update(v)\n",
    "summed_data = dict(summed_data)\n",
    "\n",
    "# calculate entropy\n",
    "print('\\nEntropy before split')\n",
    "entropy_bef = 0.0\n",
    "for c, i in summed_data.items():\n",
    "    p = i/sum(summed_data.values())\n",
    "    if p != 0:\n",
    "        class_entropy = -p*log2(p)\n",
    "    else:\n",
    "        class_entropy = 0\n",
    "    entropy_bef += class_entropy\n",
    "    print(f'Class : {c}, Count : {i}, Probability : {p:.3f}, Class Entropy : {class_entropy:.3f}')\n",
    "print(f'Total Entropy before split: {entropy_bef:.3f}')\n",
    "\n",
    "# calculate entropy after split\n",
    "print('\\nEntropy after split')\n",
    "entropy_af = 0.0\n",
    "for a, c in data.items():\n",
    "    print(f'For attribute value {a}')\n",
    "    entropy = 0.0\n",
    "    for c, i in c.items():\n",
    "        p = i/sum(data[a].values())\n",
    "        if p != 0:\n",
    "            class_entropy = -p*log2(p)\n",
    "        else:\n",
    "            class_entropy = 0\n",
    "        entropy += class_entropy\n",
    "        print(f'Class : {c}, Count : {i}, Probability : {p:.3f}, Class Entropy : {class_entropy:.3f}')\n",
    "\n",
    "    entropy_af += entropy * sum(data[a].values())/sum([sum(data[b].values()) for b in data])\n",
    "    print(f'Attribute : {a}, Entropy : {entropy:.3f}')\n",
    "print(f'Total Entropy after split: {entropy_af:.3f}')\n",
    "\n",
    "# calculate information gain\n",
    "print(f'Information Gain : {entropy_bef - entropy_af:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gini Index before split\n",
      "Class : A, Count : 9, Probability : 0.643, Class Gini Index : 0.413\n",
      "Class : B, Count : 5, Probability : 0.357, Class Gini Index : 0.128\n",
      "Total Gini Index before split: 0.459\n",
      "\n",
      "Gini Index after split\n",
      "For attribute value a1\n",
      "Class : A, Count : 3, Probability : 0.600, Class Gini Index : 0.360\n",
      "Class : B, Count : 2, Probability : 0.400, Class Gini Index : 0.160\n",
      "Attribute : a1, Gini Index : 0.480\n",
      "For attribute value a2\n",
      "Class : A, Count : 4, Probability : 1.000, Class Gini Index : 1.000\n",
      "Class : B, Count : 0, Probability : 0.000, Class Gini Index : 0.000\n",
      "Attribute : a2, Gini Index : 0.000\n",
      "For attribute value a3\n",
      "Class : A, Count : 2, Probability : 0.400, Class Gini Index : 0.160\n",
      "Class : B, Count : 3, Probability : 0.600, Class Gini Index : 0.360\n",
      "Attribute : a3, Gini Index : 0.480\n",
      "Total Gini Index after split: 0.343\n",
      "Gini Gain : 0.116\n"
     ]
    }
   ],
   "source": [
    "# code for calculating information gain\n",
    "\n",
    "data = {'a1': {'A': 3, 'B': 2}, 'a2': {'A': 4, 'B': 0}, 'a3': {'A': 2, 'B': 3}}\n",
    "summed_data = Counter()\n",
    "for k, v in data.items():\n",
    "    summed_data.update(v)\n",
    "summed_data = dict(summed_data)\n",
    "\n",
    "# calculate gini index\n",
    "print('\\nGini Index before split')\n",
    "gini_bef = 1\n",
    "for c, i in summed_data.items():\n",
    "    p = i/sum(summed_data.values())\n",
    "    gini_bef -= p**2\n",
    "    print(f'Class : {c}, Count : {i}, Probability : {p:.3f}, Class Gini Index : {p**2:.3f}')\n",
    "print(f'Total Gini Index before split: {gini_bef:.3f}')\n",
    "\n",
    "# calculate gini index after split\n",
    "print('\\nGini Index after split')\n",
    "gini_af = 0.0\n",
    "for a, c in data.items():\n",
    "    print(f'For attribute value {a}')\n",
    "    gini = 1\n",
    "    for c, i in c.items():\n",
    "        p = i/sum(data[a].values())\n",
    "        gini -= p**2\n",
    "        print(f'Class : {c}, Count : {i}, Probability : {p:.3f}, Class Gini Index : {p**2:.3f}')\n",
    "\n",
    "    gini_af += gini * sum(data[a].values())/sum([sum(data[b].values()) for b in data])\n",
    "    print(f'Attribute : {a}, Gini Index : {gini:.3f}')\n",
    "print(f'Total Gini Index after split: {gini_af:.3f}')\n",
    "\n",
    "# calculate gini gain\n",
    "print(f'Gini Gain : {gini_bef - gini_af:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.imgur.com/0Zn675n.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://i.imgur.com/0Zn675n.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Measures\n",
    "\n",
    "Accuracy = $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "Error Rate = $\\frac{FP + FN}{TP + TN + FP + FN} = 1 - Accuracy$\n",
    "\n",
    "Precision = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "Recall / Sensitivity / TPR = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "TNR / Specificity = $\\frac{TN}{TN + FP}$\n",
    "\n",
    "FPR / Fall-out / Type I error = $\\frac{FP}{FP + TN} = 1 - TNR$\n",
    "\n",
    "F1 Score = $\\frac{2 * Precision * Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.300, Error Rate : 0.700, Precision : 0.250, Recall : 0.200, TNR : 0.400, FPR : 0.600, F1 Score : 0.222\n"
     ]
    }
   ],
   "source": [
    "tp, tn, fp, fn = 10, 20, 30, 40\n",
    "accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "error_rate = 1 - accuracy\n",
    "precision = tp/(tp + fp)\n",
    "recall = tp/(tp + fn)\n",
    "tnr = tn/(tn + fp)\n",
    "fpr = fp/(fp + tn)\n",
    "f1_score = 2 * precision * recall/(precision + recall)\n",
    "print(f'Accuracy : {accuracy:.3f}, Error Rate : {error_rate:.3f}, Precision : {precision:.3f}, Recall : {recall:.3f}, TNR : {tnr:.3f}, FPR : {fpr:.3f}, F1 Score : {f1_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.25, 0.43, 0.53, 0.76, 0.85, 0.87, 0.93, 0.95, 1.95],\n",
       " [1.0, 0.8, 0.8, 0.6, 0.6, 0.4, 0.4, 0.2, 0.0],\n",
       " [1.0, 1.0, 0.8, 0.8, 0.6, 0.2, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "data = {'y_true': [1, 0, 1, 0, 0, 0, 1, 0, 1, 1], 'y_pred': [0.25, 0.43, 0.53, 0.76, 0.85, 0.85, 0.85, 0.87, 0.93, 0.95]}\n",
    "fpr, tpr, thresholds = roc_curve(data['y_true'], data['y_pred'], drop_intermediate=False)\n",
    "thresholds, tpr, fpr = thresholds.tolist()[::-1], tpr.tolist()[::-1], fpr.tolist()[::-1]\n",
    "thresholds, tpr, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "$${\\displaystyle \\text{tf-idf}(t,d)=\\text{tf}(t,d)\\cdot \\text{idf}(t)}$$\n",
    "\n",
    "where $t$ is a term and $d$ is a document. The tf-idf value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "Here, $$\\text{tf}(t,d) = 1 + log(1 + log(f_{t,d}))$$\n",
    "and $$\\text{idf}(t) = log(\\frac{1 + |d|}{|d_t|})$$\n",
    "\n",
    "where $f_{t,d}$ is the frequency of term $t$ in document $d$, and $\\sum_{t'} f_{t',d}$ is the total number of terms in document $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rules\n",
    "\n",
    "$$\\text{support}(A\\rightarrow C) = \\text{support}(A \\cup C), \\;\\;\\; \\text{range: } [0, 1]$$\n",
    "\n",
    "Support is used to measure the abundance or frequency (often interpreted as significance or importance) of an itemset in a database. We refer to an itemset as a \"frequent itemset\" if you support is larger than a specified minimum-support threshold. Note that in general, due to the downward closure property, all subsets of a frequent itemset are also frequent.\n",
    "\n",
    "$$\\text{confidence}(A\\rightarrow C) = \\frac{\\text{support}(A\\rightarrow C)}{\\text{support}(A)}, \\;\\;\\; \\text{range: } [0, 1]$$\n",
    "\n",
    "The confidence of a rule A->C is the probability of seeing the consequent in a transaction given that it also contains the antecedent. Note that the metric is not symmetric or directed; for instance, the confidence for A->C is different than the confidence for C->A. The confidence is 1 (maximal) for a rule A->C if the consequent and antecedent always occur together.\n",
    "\n",
    "$$\\text{lift}(A\\rightarrow C) = \\frac{\\text{confidence}(A\\rightarrow C)}{\\text{support}(C)}, \\;\\;\\; \\text{range: } [0, \\infty]$$\n",
    "\n",
    "The lift metric is commonly used to measure how much more often the antecedent and consequent of a rule A->C occur together than we would expect if they were statistically independent. If A and C are independent, the Lift score will be exactly 1.\n",
    "\n",
    "$$\\text{levarage}(A\\rightarrow C) = \\text{support}(A\\rightarrow C) - \\text{support}(A) \\times \\text{support}(C), \\;\\;\\; \\text{range: } [-1, 1]$$\n",
    "\n",
    "Leverage computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent. A leverage value of 0 indicates independence.\n",
    "\n",
    "$$\\text{conviction}(A\\rightarrow C) = \\frac{1 - \\text{support}(C)}{1 - \\text{confidence}(A\\rightarrow C)}, \\;\\;\\; \\text{range: } [0, \\infty]$$\n",
    "\n",
    "A high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.\n",
    "\n",
    "\n",
    "A **closed** itemset is an itemset for which there exists no proper super-itemset with the same support count. A super-itemset is an itemset that contains all items of another itemset (the subset) and at least one additional item. A **closed frequent** itemset is an itemset that is both closed and frequent in a dataset.\n",
    "\n",
    "A **maximal frequent** itemset (or max-itemset) is a frequent itemset for which none of its immediate supersets are frequent. In other words, there exists no super-itemset of a maximal frequent itemset that is also frequent in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori and FP Growth algorithms\n",
    "\n",
    "| Algorithm | Description | Candidate Generation | Pattern Generation | Process | Memory Usage |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Apriori | Generates frequent patterns by making itemsets using pairing such as single, double, triple itemset. | Uses candidate generation. | Generates pattern by pairing the items. | Slower process with exponential increase in runtime as the number of itemsets increases. | Saves a converted version of the database in memory. |\n",
    "| FP Growth | Generates an FP-Tree for making frequent patterns. | No candidate generation. | Generates pattern by constructing an FP tree. | Faster process with linear increase in runtime as the number of itemsets increases. | Saves a compact version of the conditional FP-Tree for each item in memory. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>length</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>(1)</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>(2)</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>(3)</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(4)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(5)</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>(3, 1)</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(5, 1)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>(3, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(4, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(5, 2)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(3, 1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>(5, 1, 2)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support   itemsets  length  count\n",
       "0   0.666667        (1)       1      6\n",
       "1   0.777778        (2)       1      7\n",
       "2   0.666667        (3)       1      6\n",
       "3   0.222222        (4)       1      2\n",
       "4   0.222222        (5)       1      2\n",
       "5   0.444444     (1, 2)       2      4\n",
       "6   0.444444     (3, 1)       2      4\n",
       "7   0.222222     (5, 1)       2      2\n",
       "8   0.444444     (3, 2)       2      4\n",
       "9   0.222222     (4, 2)       2      2\n",
       "10  0.222222     (5, 2)       2      2\n",
       "11  0.222222  (3, 1, 2)       3      2\n",
       "12  0.222222  (5, 1, 2)       3      2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = [\n",
    "#     ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "#     ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "#     ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "#     ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "#     ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'],\n",
    "# ]\n",
    "dataset = [\n",
    "    ['1', '2', '5'],\n",
    "    ['2', '4'],\n",
    "    ['2', '3'],\n",
    "    ['1', '2', '4'],\n",
    "    ['1', '3'],\n",
    "    ['2', '3'],\n",
    "    ['1', '3'],\n",
    "    ['1', '2', '3', '5'],\n",
    "    ['1', '2', '3'],\n",
    "]\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.22222, use_colnames=True)\n",
    "# frequent_itemsets = fpgrowth(df, min_support=0.6, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets['count'] = (frequent_itemsets['support'] * len(dataset)).astype(int)\n",
    "frequent_itemsets\n",
    "# frequent_itemsets[frequent_itemsets['length'] == 3]\n",
    "# frequent_itemsets[frequent_itemsets['count'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1)</td>\n",
       "      <td>(2)</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(3)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1)</td>\n",
       "      <td>(3)</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(3)</td>\n",
       "      <td>(2)</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(2)</td>\n",
       "      <td>(3)</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(4)</td>\n",
       "      <td>(2)</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5)</td>\n",
       "      <td>(2)</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(3, 1)</td>\n",
       "      <td>(2)</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>-0.123457</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(3, 2)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(3)</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.074074</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(5, 1)</td>\n",
       "      <td>(2)</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(5, 2)</td>\n",
       "      <td>(1)</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>(5)</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>1.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(5)</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0          (1)         (2)            0.666667            0.777778  0.444444   \n",
       "1          (2)         (1)            0.777778            0.666667  0.444444   \n",
       "2          (3)         (1)            0.666667            0.666667  0.444444   \n",
       "3          (1)         (3)            0.666667            0.666667  0.444444   \n",
       "4          (5)         (1)            0.222222            0.666667  0.222222   \n",
       "5          (3)         (2)            0.666667            0.777778  0.444444   \n",
       "6          (2)         (3)            0.777778            0.666667  0.444444   \n",
       "7          (4)         (2)            0.222222            0.777778  0.222222   \n",
       "8          (5)         (2)            0.222222            0.777778  0.222222   \n",
       "9       (3, 1)         (2)            0.444444            0.777778  0.222222   \n",
       "10      (3, 2)         (1)            0.444444            0.666667  0.222222   \n",
       "11      (1, 2)         (3)            0.444444            0.666667  0.222222   \n",
       "12      (5, 1)         (2)            0.222222            0.777778  0.222222   \n",
       "13      (5, 2)         (1)            0.222222            0.666667  0.222222   \n",
       "14      (1, 2)         (5)            0.444444            0.222222  0.222222   \n",
       "15         (5)      (1, 2)            0.222222            0.444444  0.222222   \n",
       "\n",
       "    confidence      lift  leverage  conviction  \n",
       "0     0.666667  0.857143 -0.074074    0.666667  \n",
       "1     0.571429  0.857143 -0.074074    0.777778  \n",
       "2     0.666667  1.000000  0.000000    1.000000  \n",
       "3     0.666667  1.000000  0.000000    1.000000  \n",
       "4     1.000000  1.500000  0.074074         inf  \n",
       "5     0.666667  0.857143 -0.074074    0.666667  \n",
       "6     0.571429  0.857143 -0.074074    0.777778  \n",
       "7     1.000000  1.285714  0.049383         inf  \n",
       "8     1.000000  1.285714  0.049383         inf  \n",
       "9     0.500000  0.642857 -0.123457    0.444444  \n",
       "10    0.500000  0.750000 -0.074074    0.666667  \n",
       "11    0.500000  0.750000 -0.074074    0.666667  \n",
       "12    1.000000  1.285714  0.049383         inf  \n",
       "13    1.000000  1.500000  0.074074         inf  \n",
       "14    0.500000  2.250000  0.123457    1.555556  \n",
       "15    1.000000  2.250000  0.123457         inf  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points: [[1 1]\n",
      " [2 2]\n",
      " [2 3]\n",
      " [1 2]\n",
      " [5 6]\n",
      " [5 7]\n",
      " [6 7]\n",
      " [6 6]]\n",
      "Iteration 1:\n",
      "Centroids: [[1. 1.]\n",
      " [5. 6.]]\n",
      "Distances: [[0.   6.4 ]\n",
      " [1.41 5.  ]\n",
      " [2.24 4.24]\n",
      " [1.   5.66]\n",
      " [6.4  0.  ]\n",
      " [7.21 1.  ]\n",
      " [7.81 1.41]\n",
      " [7.07 1.  ]]\n",
      "Iteration 2:\n",
      "Centroids: [[1.5 2. ]\n",
      " [5.5 6.5]]\n",
      "Distances: [[1.12 7.11]\n",
      " [0.5  5.7 ]\n",
      " [1.12 4.95]\n",
      " [0.5  6.36]\n",
      " [5.32 0.71]\n",
      " [6.1  0.71]\n",
      " [6.73 0.71]\n",
      " [6.02 0.71]]\n",
      "Iteration 3:\n",
      "Centroids: [[1.5 2. ]\n",
      " [5.5 6.5]]\n",
      "Distances: [[1.12 7.11]\n",
      " [0.5  5.7 ]\n",
      " [1.12 4.95]\n",
      " [0.5  6.36]\n",
      " [5.32 0.71]\n",
      " [6.1  0.71]\n",
      " [6.73 0.71]\n",
      " [6.02 0.71]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of clusters and the number of iterations\n",
    "K = 2\n",
    "max_iterations = 3\n",
    "\n",
    "# Generate some sample data\n",
    "# data = np.random.randint(0, 10, size=(5, 2))\n",
    "data = np.array([[1, 1], [2, 2], [2, 3], [1, 2], [5,6], [5, 7], [6, 7], [6, 6]])\n",
    "print(f\"Points: {data}\")\n",
    "\n",
    "# Initialize the centroids by randomly selecting K data points\n",
    "# centroids = data[np.random.choice(data.shape[0], K, replace=False)]\n",
    "centroids = np.array([[1, 1], [5, 6]])\n",
    "centroids = centroids.astype(float)\n",
    "\n",
    "# Iterate the k-means algorithm\n",
    "for i in range(max_iterations):\n",
    "    # Assign each point to the nearest centroid\n",
    "    distances = np.sqrt(np.sum((data[:, np.newaxis, :] - centroids) ** 2, axis=2))\n",
    "    labels = np.argmin(distances, axis=1)\n",
    "    \n",
    "    # Print the centroids and the distances at each iteration\n",
    "    print(f\"Iteration {i+1}:\")\n",
    "    print(f\"Centroids: {centroids}\")\n",
    "    print(f\"Distances: {distances}\")\n",
    "    \n",
    "    # Update the centroids to the mean of the assigned points  \n",
    "    for k in range(K):\n",
    "        centroids[k] = np.mean(data[labels == k], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heirarchical Clustering\n",
    "\n",
    "In the context of cluster analysis, linkage measures are used to determine the distance between clusters. There are several common linkage measures used in hierarchical clustering:\n",
    "\n",
    "Single linkage: The distance between two clusters is defined as the shortest distance between any two points in the different clusters. Mathematically, for clusters $C_i$ and $C_j$, the single linkage distance is given by: $$d(C_i,C_j) = \\min_{x \\in C_i, y \\in C_j} d(x,y)$$\n",
    "\n",
    "Complete linkage: The distance between two clusters is defined as the longest distance between any two points in the different clusters. Mathematically, for clusters $C_i$ and $C_j$, the complete linkage distance is given by: $$d(C_i,C_j) = \\max_{x \\in C_i, y \\in C_j} d(x,y)$$\n",
    "\n",
    "Average linkage: The distance between two clusters is defined as the average distance between all pairs of points in the different clusters. Mathematically, for clusters $C_i$ and $C_j$, the average linkage distance is given by: $$d(C_i,C_j) = \\frac{1}{|C_i||C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} d(x,y)$$\n",
    "\n",
    "Centroid linkage: The distance between two clusters is defined as the distance between their centroids. Mathematically, for clusters $C_i$ and $C_j$, with centroids $\\mu_i$ and $\\mu_j$, respectively, the centroid linkage distance is given by: $$d(C_i,C_j) = d(\\mu_i,\\mu_j)$$\n",
    "\n",
    "Ward’s method: The distance between two clusters is defined as the increase in the total within-cluster variance that would result from merging them. Mathematically, for clusters $C_i$ and $C_j$, with centroids $\\mu_i$ and $\\mu_j$, respectively, and sizes $|C_i|$ and $|C_j|$, Ward’s method distance is given by: $$d(C_i,C_j) = \\frac{|C_i||C_j|}{|C_i|+|C_j|} d(\\mu_i,\\mu_j)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix: [[0.   1.   1.41 4.24 5.66 5.   4.  ]\n",
      " [1.   0.   1.   3.61 5.   4.24 4.12]\n",
      " [1.41 1.   0.   2.83 4.24 3.61 3.16]\n",
      " [4.24 3.61 2.83 0.   1.41 1.   3.16]\n",
      " [5.66 5.   4.24 1.41 0.   1.   4.  ]\n",
      " [5.   4.24 3.61 1.   1.   0.   4.12]\n",
      " [4.   4.12 3.16 3.16 4.   4.12 0.  ]]\n",
      "Initial Clusters: [[0], [1], [2], [3], [4], [5], [6]]\n",
      "Distance matrix: \n",
      "[[0.   1.   1.41 4.24 5.66 5.   4.  ]\n",
      " [0.   0.   1.   3.61 5.   4.24 4.12]\n",
      " [0.   0.   0.   2.83 4.24 3.61 3.16]\n",
      " [0.   0.   0.   0.   1.41 1.   3.16]\n",
      " [0.   0.   0.   0.   0.   1.   4.  ]\n",
      " [0.   0.   0.   0.   0.   0.   4.12]\n",
      " [0.   0.   0.   0.   0.   0.   0.  ]]\n",
      "Closest clusters: [0] and [1] (distance: 1.00)\n",
      "\n",
      "Clusters: [[0, 1], [2], [3], [4], [5], [6]]\n",
      "Distance matrix: \n",
      "[[0.   1.41 4.24 5.66 5.   4.12]\n",
      " [0.   0.   2.83 4.24 3.61 3.16]\n",
      " [0.   0.   0.   1.41 1.   3.16]\n",
      " [0.   0.   0.   0.   1.   4.  ]\n",
      " [0.   0.   0.   0.   0.   4.12]\n",
      " [0.   0.   0.   0.   0.   0.  ]]\n",
      "Closest clusters: [3] and [5] (distance: 1.00)\n",
      "\n",
      "Clusters: [[0, 1], [2], [3, 5], [4], [6]]\n",
      "Distance matrix: \n",
      "[[0.   1.41 5.   5.66 4.12]\n",
      " [0.   0.   3.61 4.24 3.16]\n",
      " [0.   0.   0.   1.41 4.12]\n",
      " [0.   0.   0.   0.   4.  ]\n",
      " [0.   0.   0.   0.   0.  ]]\n",
      "Closest clusters: [0, 1] and [2] (distance: 1.41)\n",
      "\n",
      "Clusters: [[0, 1, 2], [3, 5], [4], [6]]\n",
      "Distance matrix: \n",
      "[[0.   5.   5.66 4.12]\n",
      " [0.   0.   1.41 4.12]\n",
      " [0.   0.   0.   4.  ]\n",
      " [0.   0.   0.   0.  ]]\n",
      "Closest clusters: [3, 5] and [4] (distance: 1.41)\n",
      "\n",
      "Clusters: [[0, 1, 2], [3, 5, 4], [6]]\n",
      "Distance matrix: \n",
      "[[0.   5.66 4.12]\n",
      " [0.   0.   4.12]\n",
      " [0.   0.   0.  ]]\n",
      "Closest clusters: [0, 1, 2] and [6] (distance: 4.12)\n",
      "\n",
      "Clusters: [[0, 1, 2, 6], [3, 5, 4]]\n",
      "Distance matrix: \n",
      "[[0.   5.66]\n",
      " [0.   0.  ]]\n",
      "Closest clusters: [0, 1, 2, 6] and [3, 5, 4] (distance: 5.66)\n",
      "\n",
      "Clusters: [[0, 1, 2, 6, 3, 5, 4]]\n"
     ]
    }
   ],
   "source": [
    "### Linkage Clustering Code\n",
    "data = np.array([[2, 3], [2, 4], [3, 4], [5, 6], [6, 7], [5, 7], [6,3]])\n",
    "\n",
    "# Calculate the distance matrix\n",
    "# set print precision to 2 decimal places\n",
    "np.set_printoptions(precision=2)\n",
    "distances = np.sqrt(np.sum((data[:, np.newaxis, :] - data) ** 2, axis=2))\n",
    "print(f\"Distance Matrix: {distances}\")\n",
    "\n",
    "# Initialize the clusters\n",
    "clusters = [[i] for i in range(len(data))]\n",
    "print(f\"Initial Clusters: {clusters}\")\n",
    "\n",
    "# Iterate the clustering algorithm\n",
    "while len(clusters) > 1:\n",
    "\n",
    "    # distance matrix for current clusters\n",
    "    distance_mat = np.full((len(clusters), len(clusters)), fill_value=0.0, dtype=float)\n",
    "\n",
    "    # Find the closest clusters\n",
    "    min_distance = np.inf\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(i+1, len(clusters)):\n",
    "            # CHOOSE ONE OF THE LINKAGE METHODS\n",
    "            # manhattan distance\n",
    "            # distance = np.sum(np.abs(data[clusters[i]] - data[clusters[j]]))\n",
    "\n",
    "            distance = np.max(distances[clusters[i]][:, clusters[j]]) # complete linkage\n",
    "            # distance = np.min(distances[clusters[i]][:, clusters[j]]) # single linkage\n",
    "            # distance = np.mean(distances[clusters[i]][:, clusters[j]]) # average linkage\n",
    "            distance_mat[i, j] = distance\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                min_i, min_j = i, j\n",
    "    \n",
    "    print(f\"Distance matrix: \\n{distance_mat}\")\n",
    "    print(f\"Closest clusters: {clusters[min_i]} and {clusters[min_j]} (distance: {min_distance:.2f})\")\n",
    "\n",
    "    # Merge the closest clusters\n",
    "    clusters[min_i] += clusters[min_j]\n",
    "    del clusters[min_j]\n",
    "    \n",
    "    # Print the clusters at each iteration\n",
    "    print(f\"\\nClusters: {clusters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "* $\\epsilon$-neighborhood: The $\\epsilon$-neighborhood of a point $p$ is the set of points within a distance $\\epsilon$ of $p$.\n",
    "* MinPts: The minimum number of points required to form a dense region (including the point itself).\n",
    "* Core point: A point $p$ is a core point if at least MinPts points are within its $\\epsilon$-neighborhood.\n",
    "* Border point: A point $p$ is a border point if it is within the $\\epsilon$-neighborhood of a core point, but it is not a core point itself.\n",
    "* Noise point: A point $p$ is a noise point if it is neither a core point nor a border point.\n",
    "* For two objects $p$ and $q$, they can be directly density-reachable, density-reachable, or density-connected.\n",
    "* A density based cluster is a maximal set of density-connected points. A set of points $C \\subseteq D$ form a cluster if:\n",
    "    1. for any two points $p$ and $q$ in $C$, $p$ and $q$ are density-connected.\n",
    "    2. there does not exist a point $o \\in C$ and another object $o' \\in D \\setminus C$ such that $o$ and $o'$ are density-connected.\n",
    "* DBSCAN steps:\n",
    "    1. Find all the neighbor points within $\\epsilon$ and identify the core points or visited with more than MinPts neighbors.\n",
    "    2. For each core point if it is not already assigned to a cluster, create a new cluster.\n",
    "    3. Find recursively all its density connected points and assign them to the same cluster as the core point.\n",
    "    4. Iterate through the remaining unvisited points in the dataset. Those points that do not belong to any cluster are noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| k-means | - Simple and easy to implement. <br> - Scalable to large datasets. <br> - Can produce tight clusters. | - Assumes spherical clusters of similar size and density. <br> - Sensitive to initial centroid placement. <br> - Requires the number of clusters to be specified in advance. |\n",
    "| Single linkage | - Can handle non-convex clusters. <br> - Simple and easy to implement. | - Sensitive to noise and outliers. <br> - Can produce elongated or \"chained\" clusters. |\n",
    "| Complete linkage | - Less sensitive to noise and outliers than single linkage. <br> - Simple and easy to implement. | - Tends to produce compact clusters of similar size. <br> - Can break large clusters. |\n",
    "| Average linkage | - Less sensitive to noise and outliers than single linkage. <br> - Simple and easy to implement. | - Tends to produce compact clusters of similar size. |\n",
    "| DBSCAN | - Can handle clusters of arbitrary shape and size. <br> - Robust to noise and outliers. <br> - Does not require the number of clusters to be specified in advance. | - Sensitive to parameter selection. <br> - Assumes clusters have similar density. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopkins Statistic\n",
    "\n",
    "The Hopkins statistic is a measure of cluster tendency. \n",
    "- Sample $n$ points $p_1, p_2, \\dots, p_n$ from the dataset $D$., for each point $p_i$, find distance to nearest neighbors in $D$. $$ x_i = \\min_{v \\in D \\setminus \\{p_i\\}} d(p_i, v) $$\n",
    "- Sample $n$ points $q_1, q_2, \\dots, q_n$ from the dataset $D$ uniformly at random, for each point $q_i$, find distance to nearest neighbors in $D - \\{q_i\\}$. $$ y_i = \\min_{v \\in D \\setminus \\{q_i\\}} d(q_i, v) $$\n",
    "- Then the Hopkins statistic is defined as: $$ H = \\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i} $$\n",
    "- Interpretation:\n",
    "    1. $H = 0$ if D is highly clustered.\n",
    "    2. $H = 1$ if D cannot be clustered.\n",
    "    3. $H = 0.5$ if D is a uniform distribution.\n",
    "    4. If $H > 0.5$, then D may not have statistically significant clusters.\n",
    "\n",
    "### Silhouette Coefficient\n",
    "Calculate the silhouette coefficient for each point $o$ in the dataset $D$:\n",
    "$$ s(o) = \\frac{b(o) - a(o)}{\\max(a(o), b(o))} $$\n",
    "where $a(o)$ is the average distance between $o$ and all other points in the same cluster, and $b(o)$ is the minimum average distance between $o$ and all points in other clusters. \n",
    "\n",
    "Silhouette coefficient for cluster $C$ is defined as the average silhouette coefficient of all points in $C$.\n",
    "Silhouette coefficient for the dataset $D$ is defined as the average silhouette coefficient of all points in $D$.\n",
    "- Interpretation:\n",
    "    1. $s(o) = 1$ if $o$ is well clustered.\n",
    "    2. $s(o) = 0$ if $o$ is on the boundary of two clusters.\n",
    "    3. $s(o) = -1$ if $o$ is assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-variate Outliers using MLE\n",
    "\n",
    "* Learn the parameters of the normal distribution $µ$ and $σ$ using Maximum Likelihood Method (MLE).\n",
    "* Identify the points with low probability as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 26.92, Std: 5.55, 3*Std: 16.65, Mean - 3*Std: 10.27, Mean + 3*Std: 43.56\n",
      "Outliers: [10.]\n"
     ]
    }
   ],
   "source": [
    "# using MLE to get outliers\n",
    "data = np.array([10, 24.0, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4])\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "print(f\"Mean: {mean:.2f}, Std: {std:.2f}, 3*Std: {3*std:.2f}, Mean - 3*Std: {mean - 3*std:.2f}, Mean + 3*Std: {mean + 3*std:.2f}\")\n",
    "print(f\"Outliers: {data[(data < mean - 3*std) | (data > mean + 3*std)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDS Calculators\n",
    "\n",
    "1. Confusion matrix : https://onlineconfusionmatrix.com/\n",
    "2. Information Gain : https://planetcalc.com/8421/\n",
    "3. FP Tree: https://planktonfun.github.io/FPTreeSimulator/\n",
    "4. Single, Complete, and Average linkage with Dendrogram: https://people.revoledu.com/kardi/tutorial/Clustering/Online-Hierarchical-Clustering.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
