{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSS Formulas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "Relating performance and execution time for a computer $X$\n",
    "\n",
    "$$ Performance_X = \\frac{1}{\\text{Execution time}_X} $$\n",
    "\n",
    "If $X$ is $n$ times faster than $Y$,\n",
    "\n",
    "$$ \\frac{\\text{Performance}_X}{\\text{Performance}_Y} = \\frac{\\text{Execution time}_Y}{\\text{Execution time}_X} = n $$\n",
    "\n",
    "$$ \\text{Cycle time} = \\text{Clock rate} = \\frac{1}{\\text{Clock period}} $$\n",
    "\n",
    "A clock rate of 4 GHz is equaivalent to a clock cycle of 250 ps.\n",
    "\n",
    "- 1 microsecond (µs) = $10^{-6}$ seconds\n",
    "- 1 nanosecond (ns) = $10^{-9}$ seconds\n",
    "- 1 picosecond (ps) = $10^{-12}$ seconds\n",
    "- 1 femtosecond (fs) = $10^{-15}$ seconds\n",
    "\n",
    "$$ \\text{CPU time}_{program} = \\text{Clock cycles}_{program} \\times \\text{Clock period} = \\frac{\\text{Clock cycles}_{program}}{\\text{Clock rate}} $$\n",
    "\n",
    "$$ \\text{Clock cycles}_{program} = \\text{Instructions}_{program} \\times \\text{Average clock cycles per instruction (CPI)} $$\n",
    "\n",
    "Calculating CPI, from CPI for different instructions,\n",
    "\n",
    "$$ \\text{CPI} = \\frac{\\sum \\text{CPI}_i \\times \\text{I}_i}{\\text{I}_c} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amdahl’s Law\n",
    "\n",
    "$$ \\text{Execution time after improvement} = \\frac{\\text{Execution time affected by improvement}}{\\text{Amount of improvement}} + \\text{Execution time unaffected} $$\n",
    "\n",
    "A rule stating that the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used. \n",
    "\n",
    "Another version, \n",
    "\n",
    "$$ \\text{Speedup} = \\frac{\\text{Execution time single processor}}{\\text{Execution time on N parallel processors}} = \\frac{T(1-f) + Tf}{T(1-f) + \\frac{Tf}{N}} = \\frac{(1-f) + f}{(1-f) + \\frac{f}{N}} $$\n",
    "\n",
    "where $f$ is the fraction of the code that is infinitely parallelizable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIPS Rate\n",
    "\n",
    "$$\n",
    "MIPS = \\frac{Instruction \\ count}{Execution \\ time \\times 10^6} = \\frac{Clock \\ rate}{CPI \\times 10^6}\n",
    "$$\n",
    "(million instructions per second)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Connection Examples\n",
    "\n",
    "<img src=\"https://i.imgur.com/7qyNRKy.png\" alt=\"Untitled\" width=\"500\" height=\"250\"/>\n",
    "\n",
    "<img src=\"https://i.imgur.com/BqvUwF8.png\" alt=\"Untitled\" width=\"500\" height=\"250\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming detection\n",
    "\n",
    "- class of binary linear code\n",
    "- For each integer $r ≥ 2$ there is a code-word with block length $n = 2^r − 1$ and message length $k = 2^r − r − 1$\n",
    "- [article](https://www.simplilearn.com/tutorials/networking-tutorial/what-is-hamming-code-technique-to-detect-errors-correct-data)\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/BpaIxUv.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "| Parity bits | Total bits | Data bits | Name | Rate |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2 | 3 | 1 | Hamming $(3,1)$ | 1/3 ≈ 0.333 |\n",
    "| 3 | 7 | 4 | Hamming $(7,4)$ | 4/7 ≈ 0.571 |\n",
    "| 4 | 15 | 11 | Hamming $(15,11)$ | 11/15 ≈ 0.733 |\n",
    "| $r$ | $n = 2^r − 1$ | $k = 2^r − r − 1$ | Hamming $(2^r − 1, 2^r − r − 1)$ | $\\frac{2^r − r − 1}{2^r − 1}$ |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming correction\n",
    "- To check for errors, check all of the parity bits. If all parity bits are correct, there is no error.\n",
    "- Otherwise, the sum of the positions of the erroneous parity bits identifies the erroneous bit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disks\n",
    "\n",
    "$Capacity = \\left( \\frac{bytes}{sector} \\right) * \\left( \\frac{avg. sectors}{track} \\right) * \\left( \\frac{tracks}{surface} \\right) * \\left( \\frac{surfaces}{platter} \\right) * \\left( \\frac{platters}{disk} \\right)$\n",
    "\n",
    "$\\text{Data transfer rate} = \\text{Number of surfaces} \\times \\text{Capacity of one track} \\times \\text{Number of rotations per second}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache organization\n",
    "\n",
    "<img src=\"https://i.imgur.com/ecd5k1O.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "Word : Smallest addressable unit of memory\n",
    "\n",
    "Byte addressable memory : 1 word = 1 byte\n",
    "\n",
    "<img src=\"https://i.imgur.com/pFuxJVU.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "Memory here is 64 bytes, which is split into 16 blocks of 4 words each. Most significant 4 bits decide the block, and least significant 2 bits decide the word in that block."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericals related to Direct Mapped \n",
    "\n",
    "<img src=\"https://i.imgur.com/pKwfOnh.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "Say there are 16 blocks in main memory and 4 cache lines, then they get mapped in a round robin manner as shown above.\n",
    "\n",
    "<img src=\"https://i.imgur.com/uR4mCCm.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "So the block number bits are further split into tag bits and line number, and the tag bits tells us which block out of all the blocks that are actually mapped to that line is actually stored in that line. \n",
    "\n",
    "$\\text{\\# PA bits} = log_2(\\text{MM size in bytes})$\n",
    "\n",
    "$\\text{\\# Block/Line offset bits} = log_2(\\text{block / line size in bytes})$\n",
    "\n",
    "$\\text{\\# Block number bits} = log_2(\\text{\\# blocks in MM})$\n",
    "\n",
    "$\\text{\\# blocks in MM} = \\frac{\\text{main memory size}}{\\text{block size}}$\n",
    "\n",
    "$\\text{\\# Line number bits} = log_2(\\text{\\# lines in cache})$\n",
    "\n",
    "$\\text{\\# lines in cache} = \\frac{\\text{cache size}}{\\text{line size}}$\n",
    "\n",
    "Line number bits may also be known as index bits.\n",
    "\n",
    "$\\text{\\# Tag bits} = log_2(\\text{\\# number of blocks mapped to single line})$\n",
    "\n",
    "Tag directory : Keeps primarily the record of the tag bits, cache line wise, so number of entries = No. of cache lines\n",
    "\n",
    "$\\text{Tag directory size} = \\text{(\\#cache lines)} \\times \\text{(\\#tag bits)}$\n",
    "\n",
    "#### Memory access time\n",
    "\n",
    "$\\text{Effective Memory Access Time} = \\text{Cache access time} \\times \\text{hit rate} + (1 - \\text{hit rate}) \\times (\\text{cache access time} + \\text{main memory access time})$\n",
    "\n",
    "$= \\text{Cache access time} + (1 - \\text{hit rate}) \\times \\text{main memory access time}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericals related to Fully Associative\n",
    "\n",
    "Any block can be associated to any of the cache lines.\n",
    "\n",
    "<img src=\"https://i.imgur.com/wEFEUci.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "<img src=\"https://i.imgur.com/xZ89VCj.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "$\\text{\\# Tag bits} = log_2(\\text{\\# Number of blocks})$\n",
    "\n",
    "<img src=\"https://i.imgur.com/fGs6xmV.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "$$ Hit \\ latency = T_{\\text{n-bit comparator}} +  T_{\\text{OR gate}} $$\n",
    "\n",
    "Say comparator delay is given as 15n nanoseconds, then it is going to be $15 \\times \\#(\\text{tag bits})$ nanoseconds.\n",
    "\n",
    "For fully associative mapping, there are no line number bits / index bits."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Numericals related to Set Associative\n",
    "\n",
    "<img src=\"https://i.imgur.com/lBgjOI8.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "\n",
    "Each block can be mapped to any of the lines inside a set, thus there is a choice for each block for where it is to be mapped.\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-color:#ccc;border-spacing:0;}\n",
    ".tg td{background-color:#fff;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{background-color:#f0f0f0;border-color:#ccc;border-style:solid;border-width:1px;color:#333;\n",
    "  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-0vih{background-color:#f9f9f9;font-weight:bold;text-align:center;vertical-align:top}\n",
    ".tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-amwm\" colspan=\"3\"># PA Bits</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0vih\"># Tag bits</td>\n",
    "    <td class=\"tg-0vih\"># Set no. bits</td>\n",
    "    <td class=\"tg-0vih\"># Block / line offset bits</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "$\\text{\\# Set number bits} = log_2(\\text{\\# sets in cache})$\n",
    "\n",
    "$\\text{\\# sets} = \\frac{\\text{\\# (cache lines)}}{k}$\n",
    "\n",
    "We only need $k$  comparators for $k$-way set associative mapping.\n",
    "\n",
    "If the number of tag bits is $p$, then we need $p$-bit comparators.\n",
    "\n",
    "$\\text{Cache size} = \\text{\\#(sets)} \\times \\text{\\#(lines in set)} \\times \\text{Block size}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of cache misses\n",
    "\n",
    "There are several types of cache misses that can occur in a computer system:\n",
    "\n",
    "1. Compulsory misses: Also known as \"cold-start misses,\" these occur when a block of data is accessed for the first time and must be brought into the cache from main memory.\n",
    "2. Capacity misses: These occur when the cache is not large enough to hold all the actively used blocks of data, resulting in some blocks being evicted from the cache.\n",
    "3. Conflict misses: These occur when multiple blocks of data map to the same cache line, causing one or more of the blocks to be evicted from the cache.\n",
    "4. Coherence misses: These occur in a multi-core system when multiple cores attempt to access the same block of data and one core's copy of the data is out of date.\n",
    "5. Prefetching misses: These occur when a prefetch instruction, which is used to bring data into the cache before it is requested, fails to bring the correct data into the cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial locality\n",
    "\n",
    "Spatial locality is the tendency of a program to access memory addresses that are close to each other in memory. This is also known as \"locality of reference\". It is the reason why caches are effective as they store blocks of memory that are likely to be accessed together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory locality\n",
    "\n",
    "Temporal locality refers to the tendency of a program to access the same memory address multiple times within a short period of time. This is the reason why cache replacement policies such as LRU are effective as they keep the recently accessed memory blocks in cache and evicts the least recently used ones.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacement algorithms\n",
    "1. Least Recently Used (LRU): This algorithm evicts the block that was least recently accessed. This algorithm is based on the assumption that the block that was least recently accessed is also the block that is least likely to be accessed in the near future.\n",
    "2. Least Frequently Used (LFU): This algorithm evicts the block that has been accessed the fewest number of times. This algorithm is based on the assumption that the block that is accessed the least is also the block that is least likely to be accessed in the near future.\n",
    "3. First In First Out (FIFO): This algorithm evicts the block that has been in the cache the longest. This algorithm is based on the assumption that the block that has been in the cache the longest is also the block that is least likely to be accessed in the near future.\n",
    "4. Random: This algorithm evicts a random block from the cache. This algorithm does not make any assumptions about which block is the least likely to be accessed in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride calculation\n",
    "\n",
    "$Stride = \\frac{current\\ memory\\ address - previous\\ memory\\ address}{size\\ of\\ array\\ element}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining\n",
    "\n",
    "1. Time Taken\n",
    "    \n",
    "    The formula for the clock period in a pipeline processor is:\n",
    "    \n",
    "    $$ t = \\max{(t_i)} + t_L $$\n",
    "    \n",
    "    Where $t_i$ is the time delay of stage $S_i$ and $t_L$ is the time delay of the latch.\n",
    "    \n",
    "    The formula for the pipeline processor frequency is:\n",
    "    \n",
    "    $$ f = \\frac{1}{t}$$\n",
    "    \n",
    "    Where $t$ is the clock period.\n",
    "    \n",
    "    The formula for the time taken to complete $n$ tasks by a $k$-stage pipeline is:\n",
    "    \n",
    "    $$T_k = [k + (n-1)]t$$\n",
    "    \n",
    "    Where $k$ is the number of stages in the pipeline, $n$ is the number of tasks and $t$ is the time taken for each stage.\n",
    "    \n",
    "    The formula for the time taken by a non-pipelined processor to complete $n$ tasks is:\n",
    "    \n",
    "    $$T_1 = k * n * t$$\n",
    "    \n",
    "2. Speedup\n",
    "    \n",
    "    $$\\text{Speedup} = \\frac{kn}{[k + (n-1)]}$$\n",
    "    \n",
    "3. Efficiency\n",
    "    \n",
    "    $$ \\text{Efficiency}, \\eta = \\frac{kn}{k[k + (n-1)]} = \\frac{n}{[k + (n-1)]} $$\n",
    "    \n",
    "4. Time space Diagram\n",
    "\n",
    "    <img src=\"https://i.imgur.com/NV5qKYh.png\" alt=\"Untitled\" width=\"500\"/>\n",
    "        \n",
    "5. Throughput\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing modes\n",
    "\n",
    "<img src=\"https://i.imgur.com/qL5DqDI.png\" alt=\"Untitled\" width=\"500\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAID (Redundant Array of Independent Disks) \n",
    "\n",
    "A technology that combines multiple physical hard drives into a single logical drive. Different RAID levels offer different levels of data protection, performance, and capacity. Here is a summary of the most common RAID levels:\n",
    "\n",
    "RAID 0: Striping - data is split across multiple disks, offering improved performance and capacity, but no data protection.\n",
    "\n",
    "RAID 1: Mirroring - data is duplicated across multiple disks, offering complete data protection, but at the cost of halving the capacity.\n",
    "\n",
    "RAID 2: Hamming code - data is split across multiple disks with additional disk for error correction, it is not commonly used.\n",
    "\n",
    "RAID 3: Bit-level striping with dedicated parity - data is split across multiple disks with a dedicated disk for storing parity information, which can be used to reconstruct data in the event of a single disk failure.\n",
    "\n",
    "RAID 4: Block-level striping with dedicated parity - similar to RAID 3, but data is split into blocks rather than bits.\n",
    "\n",
    "RAID 5: Block-level striping with distributed parity - similar to RAID 4, but parity information is distributed across all disks rather than stored on a dedicated disk.\n",
    "\n",
    "RAID 6: Block-level striping with double distributed parity - like RAID 5, but uses an additional parity block for better fault tolerance and data protection.\n",
    "\n",
    "In summary, RAID 0 offers the best performance and capacity, but no data protection. RAID 1 offers complete data protection at the cost of halving capacity. RAID 3, 4, 5, and 6 offer varying levels of data protection and performance, with RAID 5 and 6 providing the best fault tolerance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timesharing vs Multiprocessing\n",
    "\n",
    "| Time Sharing                                                                                                                | Multiprogramming                                                                                            |\n",
    "| --------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
    "| Logical extension of multiprogramming. Many users/processes are allocated with computer resources in respective time slots. | Allows to execute multiple processes by monitoring their process states and switching in between processes. |\n",
    "| Processor time is shared with multiple users.                                                                               | Resolves processor and memory underutilization problem. Multiple programs run on CPU.                       |\n",
    "| Two or more users can use a processor in their terminal.                                                                    | The process can be executed by a single processor.                                                          |\n",
    "| Has fixed time slice.                                                                                                       | No fixed time slice.                                                                                        |\n",
    "| Execution power is taken off before finishing of execution.                                                                 | Execution power is not taken off before finishing a task.                                                   |\n",
    "| System works for the same or less time on each processes.                                                                   | System does not take same time to work on different processes.                                              |\n",
    "| System depends on time to switch between different processes.                                                               | System depends on devices to switch between tasks such as I/O interrupts etc.                               |\n",
    "| System model is multiple programs and multiple users.                                                                       | System model is multiple programs.                                                                          |\n",
    "| Minimizes response time.                                                                                                    | Maximizes processor use.                                                                                    |\n",
    "| Example: Windows NT.                                                                                                        | Example: Mac OS.                                                                                            |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 states of a process\n",
    "\n",
    "1. New: The process is being created and has not yet been admitted to the ready queue.\n",
    "2. Ready: The process is waiting to be assigned to a processor and is in the ready queue.\n",
    "3. Running: The process is currently being executed by a processor.\n",
    "4. Waiting: The process is waiting for some event to occur such as an I/O operation to complete.\n",
    "5. Terminated: The process has finished execution and is being removed from the system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Control Block\n",
    "\n",
    "| Field                         | Description                                                                             |\n",
    "| ----------------------------- | --------------------------------------------------------------------------------------- |\n",
    "| Process ID                    | A unique identifier for the process.                                                    |\n",
    "| Process State                 | The current state of the process (new, ready, running, waiting, or terminated).         |\n",
    "| Program Counter               | The address of the next instruction to be executed by the process.                      |\n",
    "| CPU Registers                 | The values of the CPU registers for the process.                                        |\n",
    "| CPU Scheduling Information    | Information used by the CPU scheduler such as priority and amount of CPU time used.     |\n",
    "| Memory Management Information | Information about the memory allocated to the process such as base and limit registers. |\n",
    "| Accounting Information        | Information about resource usage such as CPU time and time limits.                      |\n",
    "| I/O Status Information        | Information about I/O devices allocated to the process and a list of open files.        |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on a process\n",
    "\n",
    "**Fork**\n",
    "* `fork()` is a system call used in Unix-like operating systems to create a new process by duplicating the calling process. After the `fork()` system call, two processes (the parent and the child) will be identical but can execute independently. The child process can then use the `exec()` system call to replace its memory space with a new program.\n",
    "* takes no arguments and returns an integer value:\n",
    "    1. negative value: the `fork()` call failed.\n",
    "    2. zero: the `fork()` call succeeded and the process is the child process.\n",
    "    3. positive value: the `fork()` call succeeded and the process is the parent process. The value returned is the process ID of the newly created child process.\n",
    "\n",
    "**Spawn**\n",
    "* `spawn()`, on the other hand, is a system call used in some operating systems such as Windows to create a new process and load a new program into its memory space. Unlike `fork()`, `spawn()` does not duplicate the calling process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread\n",
    "\n",
    "* thread is a signle sequence stream within a process\n",
    "* threads are lightweight processes\n",
    "* a process can have multiple threads\n",
    "* share the same address space\n",
    "* share the same open files and signals\n",
    "* each thread has its own program counter, stack, and set of registers\n",
    "* threads are used to implement multiprocessing & multitasking\n",
    "\n",
    "| Process                                                  | Thread                                                                |\n",
    "| -------------------------------------------------------- | --------------------------------------------------------------------- |\n",
    "| Heavy weight and costly to create in terms of resources. | Light weight and economical to create.                                |\n",
    "| Relatively slow.                                         | Comparatively faster.                                                 |\n",
    "| Cannot access the memory area of another process.        | Can access the memory area of another thread within the same process. |\n",
    "| Process switching is time consuming.                     | Thread switching is faster.                                           |\n",
    "| One process can contain several threads.                 | One thread can belong to exactly one process.                         |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling Algorithms\n",
    "\n",
    "**Defintions**\n",
    "* Preemptive: A process can be interrupted by the scheduler at any time.\n",
    "* Non-preemptive: A process cannot be interrupted by the scheduler until it voluntarily gives up the CPU or terminates.\n",
    "* Throughput: Throughput refers to the number of processes that are completed per unit time. It is a measure of how much work is being accomplished by the system.\n",
    "* Arrival time: Arrival time is the time at which a process enters the system and is placed in the ready queue.\n",
    "* Completion time: Completion time is the time at which a process finishes execution and leaves the system.\n",
    "* Burst time: Burst time is the amount of time required by a process for CPU execution.\n",
    "* Response time: Response time is the time interval between the submission of a request by a user or process and the first response produced by the system. It is a measure of how quickly the system responds to requests.\n",
    "    * $\\text{response time} = \\text{time at which CPU gets process first} - \\text{time at which process arrives}$\n",
    "* Waiting time: Waiting time is the amount of time a process spends waiting in the ready queue before it is executed by the CPU. It is a measure of how long a process has to wait before it can start executing.\n",
    "    * $\\text{waiting time} = \\text{completion time} - \\text{arrival time} - \\text{burst time}$\n",
    "    * $\\text{waiting time} = \\text{turnaround time} - \\text{burst time}$\n",
    "* Turnaround time: Turnaround time is the total time taken for a process to complete, from the time it is submitted to the system until it finishes execution. It includes both the waiting time and the actual execution time of the process.\n",
    "    * $\\text{turnaround time} = \\text{completion time} - \\text{arrival time}$\n",
    "    * $\\text{turnaround time} = \\text{waiting time} + \\text{burst time}$\n",
    "* Context switch: A context switch is the process of saving the state of a process that is currently executing on a CPU so that it can be restored and executed again at a later time. It is also known as a process switch.\n",
    "* Problems with I/0 time: I/O time is the amount of time a process spends waiting for I/O operations to complete (outside of the CPU)\n",
    "    * another process can be scheduled to run while the I/O operation is being performed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "| Scheduling Algorithm                 | Description                                                                                                                     | Advantages                                                                                                                                                                                               | Disadvantages                                                                                                                                                                       |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| First Come First Served (FCFS)       | Jobs are executed in the order they arrive.                                                                                     | Simple and easy to implement.                                                                                                                                                                            | Poor performance in terms of average waiting time, turnaround time and response time. Long jobs can hold up the CPU, resulting in poor throughput.                                  |\n",
    "| Shortest Job First (SJF)             | Jobs with the shortest expected processing time are executed first.                                                             | Minimizes average waiting time and turnaround time.                                                                                                                                                      | Requires knowledge of the expected processing time of each job, which may not be available. Can lead to starvation of long jobs.                                                    |\n",
    "| Shortest Remaining Time First (SRTF) | The job with the shortest expected processing time remaining is executed first.                                                 | Reduces average waiting time and response time compared to SJF.                                                                                                                                          | Requires preemption of currently executing jobs, which can result in high overhead. Can lead to starvation of long jobs.                                                            |\n",
    "| Priority Scheduling                  | Jobs are executed based on their priority level, with higher priority jobs being executed first.                                | Allows for more important jobs to be executed first.                                                                                                                                                     | Can lead to starvation of low priority jobs. Requires a way to determine the priority of each job.                                                                                  |\n",
    "| Round Robin Scheduling               | Jobs are executed for a fixed time slice and then moved to the back of the queue.                                               | Provides fairness by allowing each job to have a chance at the CPU.                                                                                                                                      | Poor performance if the time slice is too long, as short jobs will have to wait for the CPU. If the time slice is too short, a large portion of time is spent on context switching. |\n",
    "| Multi level Feedback Queue           | Jobs are assigned to one of several queues based on their priority level. Jobs can move between queues based on their behavior. | Provides flexibility by allowing for different scheduling algorithms to be used for different queues. Can prevent starvation by allowing lower priority jobs to eventually reach the front of the queue. | Requires careful tuning of the number of queues, time slice, and criteria for moving jobs between queues. Can be complex to implement and understand.                               |\n",
    "\n",
    "Online calculators : \n",
    "1. https://boonsuen.com/process-scheduling-solver\n",
    "2. https://vasu-gondaliya.github.io/cpu-scheduling-algorithms/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Syncronization\n",
    "\n",
    "* sharing system resources by processes in such a way that, concurrent access to shared data is handled thereby minimizing the chances of inconsistent data\n",
    "* race condition : a situation where the outcome of a process depends on the sequence or timing of other uncontrollable events\n",
    "* critical section : a section of code that accesses shared data\n",
    "* mutual exclusion : only one process can be in a critical section at a time\n",
    "* successfull use of concurrent processes requires:\n",
    "    1. mutual exclusion\n",
    "    2. progress: when no process is in its critical section, a process that wants to enter its critical section will eventually be able to do so\n",
    "    3. bounded waiting: processes requesting the critical section should not be delayed indefinitely (no deadlock, starvation)\n",
    "    4. no assumptions related to order, timing, or number of processes\n",
    "* synchronization : processes must wait for each other to finish using a shared resource before they can use it\n",
    "\n",
    "#### Peterson's Solution (Strict Alternation)\n",
    "* two processes, P0 and P1, are competing for a shared resource\n",
    "* each process has a boolean variable that indicates whether it is in its critical section\n",
    "* each process has a variable that indicates the other process\n",
    "* each process loops forever\n",
    "    * while the other process is in its critical section, do nothing\n",
    "    * enter its critical section\n",
    "    * exit its critical section\n",
    "    * set its boolean variable to false\n",
    "    * do something else\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/PIfeJ.png\" width=\"800\">\n",
    "\n",
    "#### Semaphores\n",
    "* a semaphore is a counter variable or abstract data type used to control access to a common resource by multiple processes in a concurrent system \n",
    "* two operations:\n",
    "    1. wait() / p() / down(): decrements the semaphore by 1\n",
    "    2. signal() / v() / up(): increments the semaphore by 1\n",
    "* two types:\n",
    "    1. counting semaphores: $S$ varies from $(-\\infty, \\infty)$\n",
    "    2. binary semaphores: $S$ varies from $0$ or $1$\n",
    "        * mutex locks, provides mutual exclusion\n",
    "* semaphore operations are atomic and blocking\n",
    "* main disadvantage: busy waiting, also called spinlock\n",
    "    * solution : on finding that the semaphore is not available, the process can block itself and be put on a waiting list\n",
    "\n",
    "```c\n",
    "typedef struct {\n",
    "    int value;\n",
    "    pthread_mutex_t mutex;\n",
    "    pthread_cond_t cond;\n",
    "} semaphore;\n",
    "\n",
    "void wait(semaphore *s) {\n",
    "    pthread_mutex_lock(&s->mutex);\n",
    "    // atomically decrement the value of the semaphore\n",
    "    s->value--;\n",
    "    // if the value is negative, block the calling process until it becomes positive again\n",
    "    if (s->value < 0) {\n",
    "        pthread_cond_wait(&s->cond, &s->mutex);\n",
    "    }\n",
    "    pthread_mutex_unlock(&s->mutex);\n",
    "}\n",
    "\n",
    "void signal(semaphore *s) {\n",
    "    pthread_mutex_lock(&s->mutex);\n",
    "    // atomically increment the value of the semaphore\n",
    "    s->value++;\n",
    "    // if there are any processes blocked on the semaphore, unblock one of them\n",
    "    if (s->value <= 0) {\n",
    "        pthread_cond_signal(&s->cond);\n",
    "    }\n",
    "    pthread_mutex_unlock(&s->mutex);\n",
    "}\n",
    "```\n",
    "\n",
    "#### Consumer-Producer Problem\n",
    "* a classic synchronization problem where two or more threads (the producers and the consumers) share a common buffer of fixed size\n",
    "* producers generate data and add it to the buffer\n",
    "* consumers remove data from the buffer and consume it\n",
    "* challenge is to synchronize the access to the buffer so that the producers do not add data to a full buffer and the consumers do not remove data from an empty buffer\n",
    "\n",
    "```c\n",
    "int buffer[BUFFER_SIZE];\n",
    "int in = 0;\n",
    "int out = 0;\n",
    "\n",
    "int mutex = 1;\n",
    "int empty = BUFFER_SIZE;\n",
    "int full = 0;\n",
    "\n",
    "void *producer(void *arg) {\n",
    "    for (int i = 0; i < 20; i++) {\n",
    "        // wait on the empty semaphore before adding data to ensure that there is an empty slot available\n",
    "        wait(&empty);\n",
    "        // wait on the mutex semaphore before accessing the shared buffer to ensure mutual exclusion\n",
    "        wait(&mutex);\n",
    "        // add data to the circular buffer\n",
    "        buffer[in] = i;\n",
    "        printf(\"Produced: %d\\n\", i);\n",
    "        in = (in + 1) % BUFFER_SIZE;\n",
    "        // signal the mutex semaphore to release the lock on the shared buffer\n",
    "        signal(&mutex);\n",
    "        // signal the full semaphore to indicate that there is a new full slot available\n",
    "        signal(&full);\n",
    "    }\n",
    "}\n",
    "\n",
    "void *consumer(void *arg) {\n",
    "    for (int i = 0; i < 20; i++) {\n",
    "        // wait on the full semaphore before removing data to ensure that there is a full slot available\n",
    "        wait(&full);\n",
    "        // wait on the mutex semaphore before accessing the shared buffer to ensure mutual exclusion\n",
    "        wait(&mutex);\n",
    "        // remove data from the circular buffer  \n",
    "        int item = buffer[out];\n",
    "        printf(\"Consumed: %d\\n\", item);\n",
    "        out = (out + 1) % BUFFER_SIZE;\n",
    "        // signal the mutex semaphore to release the lock on the shared buffer\n",
    "        signal(&mutex);\n",
    "        // signal the empty semaphore to indicate that there is a new empty slot available\n",
    "        signal(&empty);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deadlocks\n",
    "\n",
    "* a deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process\n",
    "* 4 necessary conditions for deadlock:\n",
    "    1. mutual exclusion: at least one resource must be held in non-shareable mode\n",
    "    2. hold and wait: a process must be holding at least one resource and waiting for another resource that is held by another process\n",
    "    3. no preemption: a resource cannot be taken from a process unless the process releases the resource\n",
    "    4. circular wait: there must be a circular chain of waiting for resources\n",
    "* resource allocation graph (RAG) : a directed graph where each vertex represents a process and each edge represents a resource\n",
    "    * if there is a cycle in the RAG and:\n",
    "        1. only one instance of each resource is available, then a deadlock will occur\n",
    "        2. multiple instances of each resource are available, then a deadlock may or may not occur\n",
    "\n",
    "\n",
    "#### Deadlock Prevention and Avoidance\n",
    " \n",
    "* deadlock prevention : a system is designed so that the four conditions for deadlock are never satisfied at the same time\n",
    "    1. mutual exclusion : a system is designed so that no process ever holds a resource in non-shareable mode\n",
    "    2. hold and wait : a system is designed so that a process never holds more than one resource at a time\n",
    "        1. protocol 1 : a process must request all of the resources it needs before it begins execution\n",
    "        2. protocol 2 : a process must request resources only when it is not holding any resources\n",
    "    3. no preemption : a system is designed so that a process never holds a resource for longer than necessary\n",
    "        1. protocol 1 : if a process requests a resource and is denied, it releases all of the resources it is holding\n",
    "        2. protocol 2 : if a process requests a resource and is denied, check whether they are allocated to another process that is waiting for a resource held by the first process. if so, resources are released from the second process and allocated to the first process\n",
    "    4. circular wait : a system is designed so that the order in which resources are requested is the same for all processes\n",
    "* deadlock avoidance : a system is designed so that the system never enters an unsafe state\n",
    "    * unsafe state : a state where a process requests a resource that is currently held by another process and the request cannot be granted without causing a deadlock\n",
    "    * banker's algorithm : a deadlock avoidance algorithm that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes a \"safety check\" to test for possible activities, before deciding whether allocation should be allowed to continue\n",
    "\n",
    "#### Deadlock Detection and Recovery\n",
    "* deadlock detection : a system periodically checks to see if a deadlock has occurred\n",
    "    * deadlock detection algorithm : a system periodically checks to see if a deadlock has occurred \n",
    "* deadlock recovery : a system recovers from a deadlock by aborting one or more of the processes involved\n",
    "    * deadlock recovery algorithm : a system recovers from a deadlock by aborting one or more of the processes involved\n",
    "        1. abort one or more processes\n",
    "        2. rollback the system to a previous state\n",
    "        3. ignore the deadlock\n",
    "\n",
    "##### Banker's Algorithm\n",
    "Banker’s Algorithm is a resource allocation and deadlock avoidance algorithm that tests for safety by simulating the allocation for predetermined maximum possible amounts of all resources.\n",
    "\n",
    "Calculator : https://jangidbhanu.github.io/BankersAlgorithm/\n",
    "\n",
    "Steps:\n",
    "1. Calculate the need matrix\n",
    "2. Mark all the processes as not finished\n",
    "3. Create an array to store the safe sequence\n",
    "4. While all processes are not finished or system is not in safe state\n",
    "    1. Find a process that is not finished and whose needs can be satisfied with the current available resources\n",
    "    2. If no such process exists, system is not in safe state\n",
    "    3. Else, allocate the resources to the process\n",
    "    4. Mark the process as finished\n",
    "    5. Add the allocated resources to the available resources\n",
    "6. If all processes are finished, the system is in safe state\n",
    "7. Else, the system is not in safe state\n",
    "\n",
    "##### Resource-Request Algorithm\n",
    "\n",
    "It is used to determine if a resource request can be granted safely. The algorithm checks if granting the request will leave the system in a safe state before granting it. If the request can be granted, the algorithm updates the data structures and grants the request. If the request cannot be granted, the process must wait until another process releases enough resources.\n",
    "\n",
    "Steps:\n",
    "1. If $request_i$ <= $need_i$, go to step 2. Else, request is denied\n",
    "2. If $request_i$ <= $available$, go to step 3. Else, request is blocked\n",
    "3. Allocate the requested resources to process $P_i$\n",
    "4. If the system is now in a safe state, go to step 5. Else, go to step 6\n",
    "5. Request is granted\n",
    "6. Deallocate the requested resources from process $P_i$, request is denied\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "\n",
    "Five requirements\n",
    "* Relocation : the ability to move a program from one location in memory to another\n",
    "    * memory references must be translated into physical addresses\n",
    "    * for example, mapping from virtual memory to physical memory\n",
    "    * address binding happens at 3 stages:\n",
    "        1. compile time : the compiler translates symbolic addresses into absolute addresses\n",
    "        2. load time : the loader translates absolute addresses into relative addresses\n",
    "        3. execution time : the operating system translates relative addresses into absolute addresses\n",
    "* Protection : the ability to prevent a process from accessing memory that belongs to another process\n",
    "    * each process has its own address space\n",
    "    * provide security by using 2 registers:\n",
    "        1. base register : the lowest valid address for the process\n",
    "        2. limit register : the size of the address space\n",
    "* Sharing : the ability to allow multiple processes to access the same memory\n",
    "    * allow several processes to access the same location in memory\n",
    "* Logical organization\n",
    "    * logical address space : the address space of a process generated by the CPU, virtual address \n",
    "    * programs are written in modules\n",
    "* Physical organization\n",
    "    * physical address space : the address space of the main memory, physical address as seen by memory unit\n",
    "    * two level organization\n",
    "        1. primary memory : the main memory\n",
    "        2. secondary memory : the disk\n",
    "    * MMU : memory management unit \n",
    "        * translates virtual addresses to physical addresses\n",
    "        * maintains the page table\n",
    "        * maintains the TLB (translation lookaside buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision vector\n",
    "Find Forbidden Latency,Collision Vector,Greedy Cycle : https://www.youtube.com/watch?v=2PIj3YekQlk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
